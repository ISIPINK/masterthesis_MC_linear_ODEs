{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table of contents\n",
    "TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMC for exponential function\n",
    "### Basic construction\n",
    "We ran some tests for \n",
    "$y'=y, y(0)=1$ with a basic Monte Carlo recursive scheme to build understanding. <br>\n",
    "\n",
    "To do Monte Carlo you need a random variable $Y(t)$ which you can simulate such that $y(t) = E[Y(t)]$. To construct such a random variable you first need to turn the ODE into an integral equation which is easier to manipulate in this context. There are multiple techniques to do that (we will write it down later) in this case we want a recursive integral equation for theoretical reasons. This is easily obtained by integrating both sides.\n",
    "\n",
    "$$\n",
    "y(t)= 1 + \\int_0^t y(s)ds.\n",
    "$$\n",
    "Do the following substitution in the integral $ds = \\frac{ds}{dU} dU$ with $U = \\text{Uniform}(0,t)$. (https://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem $U$ dominates $1_{[0,t]}$ you could interpret constants as random variables) Now you can write the integral as an expectation using reverse LOTUS (https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician).\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y(t)&= 1 + E[y(U) \\frac{ds}{dU}]\\\\\n",
    "&= 1 + tE[y(x) \\mid U = x]\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Suppose we can construct a family of random variables $Y(x)>0, 0<x<t$ (induction like) such that $y(x) = E[Y(x)]$. We will try to construct $Y(t)$ out of those. Sub this property in the previous equation.\n",
    "\n",
    "$$\n",
    "y(t)= 1 + tE[E[Y(x)] \\mid U = x]\n",
    "$$\n",
    "\n",
    "Now merge the expectations (in this case it's ok because everything is positive).\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y(t)&= 1 + tE[Y(x) \\mid U = x]\\\\\n",
    "    &= 1 + tE[Y(U)]\\\\\n",
    "    &= E[1 + tY(U)]\n",
    "\\end{align*}\n",
    "$$\n",
    "$1 + tY(U)$ is has the property of the random variable we wanted to construct. To complete the construction we would need\n",
    "to construct $Y(x)>0, 0<x<\\varepsilon$ for an arbitrary $\\varepsilon$. We haven't figured out how to do that but we approximate such random variable for small $\\varepsilon$ with $1$.<br>\n",
    "Following code demonstrates how to construct $Y(t)$ approximately using recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp(1) is approx = 2.7599096369837333\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "def Y(t,eps):\n",
    "    return 1 + t* Y(random()*t,eps) if t>eps else 1 \n",
    "\n",
    "sol = 0\n",
    "nsim = 10**3\n",
    "eps = 0.01\n",
    "t = 1\n",
    "\n",
    "for _ in range(nsim):\n",
    "    sol += Y(t,eps)/nsim\n",
    "\n",
    "print(f\"exp({t}) is approx = {sol}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Monte Carlo simulation to convergence we need $\\text{Var}(Y(t)) < \\infty$. (which we don't have results on yet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifications\n",
    "Once we have a random variable where we can use Monte Carlo on there are tricks to manipulate that random variable such that the expectance is preserved but the behavior of the random variable changes. \n",
    "\n",
    "#### Russian Roulette\n",
    "Russian Roulette is a popular technique in rendering (see Veach). The big idea is accelerating simulation by \"killing\" low contribution simulations early but this has as side effect increased variance. This can be achieved mathematically by adding bernoulli processes without changing the expectance. <br> \n",
    "In previous example we can change the recursive definition slightly by replacing $t$ with bernoulli process $B(t)$ for $t<1$ (this doesn't affect the expectance). Then the code becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp(5) is approx = 145.87086638244105\n",
      "%error = -0.017129833604434153\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from math import exp\n",
    "def Y(t):\n",
    "    return (1 + Y(random()*t) if 1-t<random() else 1) if t<1 else 1 + t* Y(random()*t)\n",
    "\n",
    "sol = 0\n",
    "nsim = 10**4\n",
    "t = 5\n",
    "\n",
    "for _ in range(nsim):\n",
    "    sol += Y(t)/nsim\n",
    "\n",
    "percentage_error = (sol - exp(t))/exp(t)\n",
    "\n",
    "print(f\"exp({t}) is approx = {sol}\")\n",
    "print(f\"%error = {percentage_error}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to approximate our random variable for $\\varepsilon$ because the simulation gets stopped by killing. Russian Roulette in this case got rid of a parameter, made our random variable unbiased and accelerated the program! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61dcd53c54624714075f66cf77d2f4f7b806bcd73e530a683be31ad82b480a0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
