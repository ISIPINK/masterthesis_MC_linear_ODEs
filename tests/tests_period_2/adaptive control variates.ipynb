{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check out Neural Control Variates and Regression-based Monte Carlo Integration in bibliography and other works on sequential Monte carlo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on control variates for ODEs/recursive integral equations that are build using approximate solutions <br>\n",
    "\n",
    "We already went through some basic examples in period1 but we haven't explained why those examples work. <br>\n",
    "\n",
    "Ignore for the sake of simplicity the Russian Roulette:\n",
    "$$\n",
    "\\begin{align*}\n",
    "Y(t)&= 1+tY(U) \\Rightarrow \\\\\n",
    "\\text{Var}(Y(t))&= t^{2} \\text{Var}(Y(U))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Reducing $\\text{Var}(Y(U))$ reduces $\\text{Var}(Y(t))$ there aren't a lot first moves you can do by trial and error you can find that using the law of total variance (https://en.wikipedia.org/wiki/Law_of_total_variance) helps:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Var}(Y(U))&= E[\\text{Var}(Y(s)) \\mid U = s] + \\text{Var}(E[Y(s)] \\mid U =s) \\\\\n",
    "                &= E[\\text{Var}(Y(s)) \\mid U = s] + \\text{Var}(y(U)) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Notice that both of those terms are positive because of the variances that are positive. If you  study this expression by doing control variate type transformations there stands $1$ case out. Let $q$ be a function.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Var}(Y(U)-q(U) + E[q(U)]) &=  \\text{Var}(Y(U)-q(U)) \\\\\n",
    "                            &=  E[\\text{Var}(Y(s) -q(s)) \\mid U = s] + \\text{Var}(E[Y(s)-q(s)] \\mid U =s) \\\\ \n",
    "                            &=  E[\\text{Var}(Y(s)) \\mid U = s] + \\text{Var}(y(U)-q(U))  \n",
    "\\end{align*}\n",
    "$$\n",
    "So $q$ only influences the second term and the second term has a nice interpretation:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Var}(y(U)-q(U))   &= \\frac{1}{t} \\int_{0}^{t} (y(s)-q(s))^{2} ds \\\\ \n",
    "                        &\\sim ||y-q||^{2}_{2}   \n",
    "\\end{align*}\n",
    "$$\n",
    "This error of $y$ and $q$ is for every $t$ different a but you can bound those with the biggest $t$. <br>\n",
    "\n",
    "A simpler analysis of variance can be done for regular Monte Carlo. We will show a derivation where don't use a clean control variate that get compensated with independent random variable $G$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Var}(f(U)-g(U)+G)&=\\text{Var}(f(U)-g(U)) + \\text{Var}(G) \\\\ \n",
    "    &=\\text{Var}(f(U)-g(U)) + \\text{Var}(G) \\\\ \n",
    "    &= ||f-g||_{2}^{2} + \\text{Var}(G)\n",
    "\\end{align*}\n",
    "$$\n",
    "if $G$ is obtained by Monte Carlo we then call it a MC control variate. Reducing the variance can be achieved by minimizing the error on the approximate function and reducing the variance of the Monte Carlo estimator of G. <br>\n",
    "\n",
    "You also could use a control variate on $G$ ... if you repeat this infinite often and turn everything backwards you get multilevel Monte Carlo. So multilevel Monte Carlo is just MC control variate on MC control variate infinite times. <br>\n",
    "\n",
    "Something similar can be done with recursive Monte Carlo a big benefit is that there is no recursion on the MC control variates. <br>\n",
    "\n",
    "Minimizing the norm in this context could be done by stochastic gradient descent in general the approach we chose for is we think equivalent with the right set up and some work we'll side step with orthogonal theory. <br>\n",
    "\n",
    "For simplicity we introduce orthogonal adaptive control variates for regular Monte Carlo which also van be used to do function approximation given a point estimator over a domain. <br>\n",
    "\n",
    "Let's say we want to MC integrate $f=e^{x}$ over $[-1,1]$ to construct control variate we keep an approximation in an cutoff orthonormal decomposition $\\sum_{n=0}^{k} a_n p_{n}(x)$ in this case of Legendre polynomials (https://en.wikipedia.org/wiki/Legendre_polynomials). To approximate the coefficients in this decomposition we again use Monte Carlo integration:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_{n}   &= \\frac{2n+1}{2}\\int_{-1}^{1}f(x)p_{n}(x)dx \\\\\n",
    "        &= (2n+1)E[f(U)p_n(U)] \n",
    "\\end{align*}\n",
    "$$\n",
    "where that $U = \\text{Uniform(-1,1)}$ and we can control variate that:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_{n}   &= \\frac{2n+1}{2}\\int_{-1}^{1}f(x)p_{n}(x)dx \\\\\n",
    "        &= (2n+1)E\\left[\\left(f(U)-\\sum \\bar{a}_j p_{j}(U) \\right)p_n(U) \\right] + \\bar{a}_{n} \n",
    "\\end{align*}\n",
    "$$\n",
    "Where $\\bar{a}_j$ are the current estimates of the coefficients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61dcd53c54624714075f66cf77d2f4f7b806bcd73e530a683be31ad82b480a0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
