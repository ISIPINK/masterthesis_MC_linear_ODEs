{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let say we want to say something about following problem:\n",
    "\n",
    "$$\n",
    "y' = Ay, y(0) \\sim N(0, \\sigma) , A \\sim N(0,\\sigma)\n",
    "$$\n",
    "\n",
    "The solution of this problem is a family of random variables $Y(t)$ the general way to obtain this numerically is sampling $y(0),A$ and solving a differential equation. In general there is no way around that $Y(t)$ just contains to much information in some cases you can use some tricks to make the problem possible and even analytically possible. <br>\n",
    "\n",
    "But instead wanting to know the general solution we find it easier to make a Monte Carlo techniques that calculate things of the following form $E[f(Y(t))]$ where $f$ analytic. Great examples are $E[Y(t)]$ and $E[Y^{2}(t)]$ which contain some information about $Y(t)$. <br>\n",
    "\n",
    "How is actually very simple:\n",
    "$$\n",
    "\\begin{align*}\n",
    "E[f(Y(t))] &= E[E[f(Y(t)) \\mid y(0), A]] \\\\    \n",
    "           &= E[E[f(y(t,y(0),A))]] \\\\    \n",
    "            &= E[E[f(E[U(t,y(0),A)])]] \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "$y(t,y(0),A)$ is the solution of the ODE given $y(0)$ and $A$ and $U$ an unbiased estimator of it.\n",
    "We already explained how to deal with $f$, see period1 analytic functions. <br> \n",
    "\n",
    "Ofcourse if $f$ isn't nice you can settle for approximations with analytical functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61dcd53c54624714075f66cf77d2f4f7b806bcd73e530a683be31ad82b480a0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
