\documentclass[a4paper,12pt]{article}

\setlength{\textwidth}{15.0cm}
\setlength{\textheight}{24.0cm}
\setlength{\topmargin}{0cm}
\setlength{\headsep}{0cm}
\setlength{\headheight}{0cm}
\pagestyle{plain}

\usepackage[dvips]{epsfig}
\usepackage{tikz}

\usepackage[
backend=biber,
style=alphabetic,
]{biblatex}

\usepackage{amsmath,amssymb,amsthm}
\input{environments.tex}

\usepackage{comment}
\usepackage{listings}
\input{custom commands.tex}

\addbibresource{bibliography.bib} 
\setlength{\parindent}{0pt}

\begin{document}

\begin{comment}
\title{Unbiased Monte Carlo for Recursive Integrals}
\author{Isidoor Pinillo Esquivel}
\maketitle
\end{comment}

\input{titelblad_MP.tex}

\newpage
\tableofcontents
\newpage

\begin{abstract}
    We will write this at the end.
\end{abstract}

\section{Introduction}

\subsection{Introductory Example}
To get familiar with Monte Carlo for estimating recursive integrals
we demonstrate it on following problem:

\begin{equation} \label{ydy}
    y'=y, y(0)=1.
\end{equation}

By integrating both sides of (\ref{ydy}) following integral equation can be derived:

\begin{equation} \label{Integral ydy}
    y(t) = 1 + \int_{0}^{t} y(s) ds.
\end{equation}

Equation (\ref{Integral ydy}) is a recursive integral equation or to be more specific
a linear Volterra integral equation of the second type. By naively using Monte
Carlo on the recursive integral of equation (\ref{Integral ydy}) one derives following estimator:

\[
    Y(t) = 1 + t y(Ut)
    .\]

where $U=\text{Uniform}(0,1)$. If $y$ is well behaved then $E[Y(t)]=y(t)$ but we can't
calculate $Y(t)$ without accesses to $y(s),s<t$. Notice that we can replace $y$ by a
unbiased estimator of it without changing $E[Y(t)]=y(t)$ by the law of total expectance
($E[X] = E[E[X|Z]]$). By replacing $y$ by $Y$ itself we obtain a recursive expression for $Y$:

\begin{equation}\label{recursive RV}
    Y(t) = 1 + tY(Ut).
\end{equation}

Equation (\ref{recursive RV}) is a recursive random variable equation (RRVE). If you would implement equation
(\ref{recursive RV}) with recursion it will run indefinitely. A biased way of around this is by approximating
$Y(t) \approx 1$ near $t = 0$. Later we discuss Russian roulette (\ref{Russian roulette}) which
can be used as an unbiased stopping mechanism.

\vspace*{0.2cm}

\begin{pythonn}[implementation of (\ref{recursive RV})] \label{python eps ydy}
    \pythoncode{python code/eps_ydy.py}
\end{pythonn}

An issue with (\ref{python eps ydy}) is that the variance increases rapidly when $t$ increases. Which we
later solve in the section on ODEs. Note that (\ref{python eps ydy}) keeps desirable properties
from unbiased Monte Carlo methods such as: being embarrassingly parallel,
robustness and having simple error estimates.


\subsection{Contributions}
We write this at the end. Probably a lot of conjectures.

\subsection{Related Work}
work on
\begin{itemize}
    \item alternative methods for recursive integrals
    \item MC work on ODEs
    \item MC work on PDEs
    \item WoS
\end{itemize}
This is just to give a general overview we probably reference specific ideas when we first introduce them.

\section{Background}
\subsection{Modifying Monte Carlo}
Once we have a RRVE it is possible to modify it
to have more desirable properties. \\

Russian roulette is a Monte Carlo technique widely used in rendering. The idea behind
Russian roulette is replacing a random variable with a cheaper approximation sometimes.

\begin{definition}[Russian roulette] \label{Russian roulette}
    Define Russian roulette on $X$ with free parameters
    $Y_{1},Y_{2}: E[Y_{1}] = E[Y_{2}]$, $p \in [0,1]$ and $U$
    independent of $Y_{1},Y_{2},X$ the following way:
    \[
        X \rightarrow \begin{cases}
            \frac{1}{p}(X- (1-p)Y_{1}) & \text{ if } U<p \\
            Y_{2}                      & \text{ else }
        \end{cases}
        .\]
\end{definition}

\begin{example}
    Say that we are interested in estimating $E[Z]$ with $Z$
    defined in the following way:
    \[
        Z = U + \frac{f(U)}{1000}
        .\]
    where $f:\mathbb{R} \rightarrow [0,1]$ expensive to compute.
    Estimating $Z$ directly would require calling $f$ each
    simulation. We can modify $Z$ to
    \[
        \tilde{Z} = U + B\left(\frac{1}{100}\right)\frac{f(U)}{10}
        .\]
    where $B(\frac{1}{100}) \sim \text{Bernouli}(\frac{1}{100})$. Now $\tilde{Z}$
    just requires calling $f$ on average once every $100$ simulations with the variance
    only increasing slightly compared to $Z$.
\end{example}

Maybe this wasn't the best example because you could also estimate the expectance of the
$2$ terms of $Z$ separately.

\begin{example}[Russian roulette on (\ref{recursive RV})]
    Russian roulette can fix the indefinite recursion issue of
    equation (\ref{recursive RV}) by approximating $Y$ near $t = 0$ with $1$. Concretely
    we replace the $t$ in front of the recursive term with $B(t) \sim \text{Bernouli(t)}$
    when $t<1$.
    \[
        Y(t) =
        \begin{cases}
            1 + B(t)Y(Ut) & \text{ if } t<1 \\
            1 + tY(Ut)    & \text{ else}
        \end{cases}
        .\]
\end{example}

\vspace{0.2cm}

\begin{pythonn}[Russian roulette on (\ref{recursive RV})]
    \pythoncode{python code/RR_ydy.py}
\end{pythonn}

Splitting is a technique that has almost the reverse effect as Russian roulette.
The idea behind splitting is to reduce variance in certain places by using more
samples.

\begin{definition}[splitting]
    Splitting $X$ means using multiple $X_{j} \sim X$ not independent per se to
    lower variance by averaging them:
    \[
        \bar{X}= \frac{1}{N} \sum_{j=1}^{N} X_{j}
        .\]
\end{definition}

Splitting the recursive term in a RRVE can lead to (additive) branching recursion.
Extra care should be taken that all branches get terminated with probability $1$. This can be
achieved by termination strategies already discussed and later we discuss coupled recursion for
alleviating additive branching recursion in RRVEs.

\begin{example}[splitting on (\ref{recursive RV})]
    We can "split" the recursive term of  (\ref{recursive RV}) in $2$:
    \[
        Y(t) = 1 + \frac{t}{2}(Y_{1}(Ut)+Y_{2}(Ut))
        .\]
    with $Y_{1}(t),Y_{2}(t)$ i.i.d. $Y(t)$.
\end{example}

\vspace{0.2cm}

\begin{pythonn}[splitting on (\ref{recursive RV})]
    \pythoncode{python code/SRR_ydy.py}
\end{pythonn}

\begin{definition}[$2$-level MC] \label{2 level}
    $2$-level MC on $X$ with parameters $\tilde{X}, Y: E[\tilde{X}]=E[Y]$:
    \[
        X \rightarrow X-\tilde{X} + Y
        .\]
\end{definition}

\begin{definition}[control variates] \label{CV}
    Control variate on $f(X)$ is
    \[
        f(X) \rightarrow f(X)-\tilde{f}(X) + E[\tilde{f}(X)]
        .\]
\end{definition}
Control variates are a special case of $2$-level MC. Usually $\tilde{f}$ is an approximation
of $f$ to reduce variance.

\begin{example}[control variate on (\ref{recursive RV})]
    To make a control variate for (\ref{recursive RV}) that reduces variance
    we use following approximation of $y(t) \approx 1+t$:
    \[
        Y(t)= 1+t+\frac{t^{2}}{2} + t(Y(Ut)-1-Ut)
        .\]
    Notice that we can cancel the constant term of the control variate
    but that would affect the Russian roulette negatively.
\end{example}

\vspace*{0.2cm}
\begin{pythonn}[control variate on (\ref{recursive RV})]
    \pythoncode{python code/CVRR_ydy.py}
\end{pythonn}

\begin{comment}
Introduces Russian roulette, splitting, control variates, importance sampling and maybe quasi Monte Carlo with the
$y'=y$ example. We are missing importance sampling and quasi MC
\end{comment}

\begin{related}
    Our favorite work that discusses these techniques is \cite{veach_robust_nodate}.
    More interesting works can be found on Monte Carlo techniques in rendering.
    $2$-level gets discussed in \cite{giles_multilevel_2013}.
\end{related}
% need to fix veach refrence see bibliography

\subsection{Monte Carlo Trapezoidal Rule}
We present here a Monte Carlo trapezoidal rule with similar convergence behavior to
methods discussed later. The Monte Carlo trapezoidal rule will just be
regular Monte Carlo control variated with the normal trapezoidal rule.

\begin{definition}[MC trapezoidal rule]
    Define the MC trapezoidal rule for $f$ on $[x,x+dx]$ the following
    way:
    \begin{equation}
        \int_{x}^{x+dx} f(s)ds \approx
        \frac{f(x)+f(x+dx)}{2} + f(S_{x})-f(x)-\frac{S_{x}-x}{dx}(f(x+dx)-f(x))
    \end{equation}
    with $S_{x} = \text{Uniform}(x,x+dx)$.
\end{definition}


Defining the composite MC trapezoidal rule as
the sum of MC trapezoidal rules on equally divided intervals
is possible but expensive. Every interval would add a function call
compared to the normal composite MC trapezoidal rule. Instead
you can aggressively Russian roulette into the normal trapezoidal rule
such that the increase in functions calls is arbitrarily small.

\begin{definition}[composite MC trapezoidal rule] \label{MCtrap}
    Define the composite MC trapezoidal rule for $f$ on $[a,b]$ with
    $n$ intervals and a Russian roulette rate $l$ the following way:
    \begin{align}
         & \int_{a}^{b} f(s)ds \approx        \\
         & \sum_{x}  \frac{f(x)+f(x+dx)}{2} +
        l B \left(\frac{1}{l} \right)
        \left(f(S_{x})-f(x)-\frac{S_{x}-x}{dx}(f(x+dx)-f(x)) \right)
    \end{align}

    with $S_{x} = \text{Uniform}(x,x+dx)$.

\end{definition}

\begin{pythonn}[implementation of (\ref{MCtrap})]
    We implement (\ref{MCtrap}) for $\int_{0}^{1}e^{s}ds$.
    \vspace*{0.5cm}
    \pythoncode{python code/trap1.py}
\end{pythonn}

What is surprising about this MC composite rule is that
under the right smoothness conditions it adds $0.5$
for every dimension order of convergence over the normal
composite rule.

\begin{lemma}[half variance phenomenon]
    Maybe a lemma about MC integrating a polynomial
    with proof and this becomes a theorem
\end{lemma}

\begin{proof}
    Also a maybe, maybe just a numerical example.
\end{proof}

\begin{related}
    Optimal theoritcal bounds on randomized algorithms can be found in:
    (see literature randomized trapezoidal rule)
    \cite{wu_randomised_2020}.
\end{related}

comparing normal vs Monte Carlo trapezoidal rule and highlighting the "half variance phenomenon".
+ maybe integrating polynomials for intuition

\subsection{Unbiased Non-Linearity}
At first sight it looks only possible to deal with linear problems in an unbiased way but by using
independent samples it possible to deal with polynomial non-linearity's which practically extend
to any continuos functions by the Weierstrass approximation theorem.  It is not always easy to
transform non-linearity into polynomials but it is not difficult to come up with
biased alternative approaches based on linearization or approximate polynomial non-linearity.


\begin{example}[$y'=y^{2}$]
    Let's do following example:
    \begin{equation} \label{dyy2}
        y'= y^2.
    \end{equation}
    with $y(1)=-1$. This has solution $-\frac{1}{t}$. Integrate both sides of
    equation (\ref{dyy2}) to arrive at following integral equation:
    \begin{equation} \label{Integral dyy2}
        y(t) = -1 + \int_{1}^{t} y(s) y(s) ds .
    \end{equation}
    To estimate the recursive integral in equation (\ref{Integral ydy}) we use $2$
    independent $Y_{1},Y_{2}\sim Y$ :
    \[
        Y(t) = -1 + (t-1) Y_{1}(S) Y_{2}(S)
        .\]
    With $S \sim \text{Uniform}(1,t)$. This is a branching RRVE this is
    typical when dealing with non-linearity.
\end{example}

\vspace*{0.2cm}
\begin{pythonn}[$y'=y^{2}$]
    \pythoncode{python code/dyy2.py}
\end{pythonn}

\begin{example}[$e^{E[X]}$]
    $e^{\int x(s)ds}$ is common expression encountered when studying ODEs.
    In this example we demonstrate how you can generate unbiased estimates of
    $e^{E[X]}$ with simulations of $X$. The taylor series of $e^{x}$ is:
    \begin{align}
        e^{E[X]} & = \sum_{n=0}^{\infty} \frac{E^{n}[X]}{n!}     \\
                 & = 1 + \frac{1}{1}E[X]\left(1+ \frac{1}{2}E[X]
        \left(1+\frac{1}{3}E[X]\left(1+ ...\right)\right)\right). \label{taylor e}
    \end{align}
    Change the fractions of equation (\ref{taylor e}) to Bernoulli processes
    and replace all $X$ with independent $X_j$ with $E[X]=E[X_{i}]$.
    \begin{align*}
        e^{E[X]} & = E
        \left[1 + B\left(\frac{1}{1}\right)E[X_1]
        \left(1+ B\left(\frac{1}{2}\right)E[X_2]
        \left(1+B\left(\frac{1}{3}\right)E[X_3]
        \left(1+ ...\right)
        \right)
        \right)
        \right]              \\
                 & = E\left[
            1 + B\left(\frac{1}{1}\right)X_1
            \left(1+ B\left(\frac{1}{2}\right)X_2
            \left(1+B\left(\frac{1}{3}\right)X_3
            \left(1+ ...\right)
            \right)
            \right)
        \right]              \\
    \end{align*}
    What is inside the expectation is something that we can simulate with simulations of $X_{j}$.
\end{example}

\vspace{0.2cm}
\begin{pythonn}[$e^{E[X]}$]
    The following python code estimates $e^{\int_{0}^{t} s^{2}ds}$:
    \vspace*{0.4cm}
    \pythoncode{python code/expX.py}
    %}
\end{pythonn}

\begin{related}
    A similar approach to non-linearity can be found in \cite{ermakov_monte_2019}.
    We have more papers on how to deal with non-linearity stashed, no idea if they are
    worth mentioning.
\end{related}

\subsection{Recursion}
In this section we discuss recursion related techniques.

\begin{technique}[coupled recursion]
    The idea behind coupled recursion is sharing recursion calls of
    multiple RVVEs for simulation. This does make them dependent.
    This is like assuming $2$ induction hypotheses at the same
    time and proving both inductions steps at the same time vs
    doing separate induction proofs. Which should be easier
    because you have accesses to more assumptions at the same time.
\end{technique}

\begin{example}[coupled recursion]
    Lets say you are interested in calculating the
    sensitivity of the solution of an ODE to a
    parameter $a$:
    \begin{align}
        y'             & =ay,y(0)=1 \Rightarrow \label{couple recu ex1} \\
        \partial_{a}y' & = y + a \partial_{a}y' \label{couple recu ex2}
    \end{align}
    Turn (\ref{couple recu ex1}) and (\ref{couple recu ex2}) into RRVEs.
    To emphasize that they are coupled, that they should
    recurse together we write them in a matrix equation:
    \begin{equation} \label{coupled mat}
        \begin{bmatrix}
            Y(t) \\
            \partial_{a}Y(t)
        \end{bmatrix}=
        \begin{bmatrix}
            1 \\
            0
        \end{bmatrix}+
        t \begin{bmatrix}
            a & 0 \\
            1 & a
        \end{bmatrix}
        \begin{bmatrix}
            Y(Ut) \\
            \partial_{a}Y(Ut)
        \end{bmatrix}.
    \end{equation}

\end{example}

\begin{pythonn} [implementation of (\ref{coupled mat})]
    \pythoncode{python code/coupled_mat.py}
\end{pythonn}

\begin{technique}[recursion in recursion]
    Recursion in recursion is what is sound like. This is like proving a induction
    step of an induction proof with induction. The next flight variant of WoS
    is a beautiful example of recursion in recursion described in
    \cite{sawhney_grid-free_2022}.
\end{technique}

\begin{example}[recursion in recursion]
    maybe induction in induction proof example or a reference to ODE solvers later.
\end{example}

Most programming languages support recursion but this comes with restrictions
like maximum recursion depth and performance issues. Tail recursion solve those
issues when possible.

\begin{technique}[non-branching tail recursion]
    Tail recursion is reordering all operations in a way that
    almost no operation needs to happen after the recursion call such that when reaching
    the last recursion call we can return the answer without retracing all steps.
\end{technique}

All non-branching recursion
in this paper can be implemented straight forwardly. This can easily be seen because
all of the operations are associative ($(xy)z = x(yz)$). \\
Tail recursion is not always desirable because we lose the intermediate values of the
recursion calls. It is also possible to combine tail recursion with normal recursion.

\begin{pythonn}[tail recursion on (\ref{coupled mat})]
    We implement (\ref{coupled mat}) but this time with tail recurion.
    We collect addition operations in a vector sol and multiplication
    in a matrix $W$.
    \vspace{0.3cm}
    \pythoncode{python code/tailrecu.py}
\end{pythonn}

Branching tail recursion is hard. There  are multiple ways to
do branching tail recursion with each their advantages and disadvantages. \\
In the context of recursive Monte Carlo there are $2$ techniques that
stand out:

\begin{technique}[tree regrowing]
    The structure of branching recursion can be captured by a tree. Storing that tree
    in memory can be expensive. In recursion you only need to retrace steps
    $1$ by $1$ therefore you only need local parts of the recursion tree. Tree
    regrowing tries to alleviate memory issues by instead storing the whole
    tree only storing seeds (of the random generator) of parts of the
    tree and growing them when needed.
\end{technique}

\begin{technique}[backward tail recursion]
    One way of doing branching tail recursion is by using operation buffers for
    all leafs which is not memory friendly. In backward tail recursion you retrace
    steps and do all operations in reverse to recover the buffer needed.
\end{technique}


\begin{related}
    This blog discusses branching tail recursion:
    \url{https://jeroenvanwijgerden.me/post/recursion-1/}.
    The techniques for tail recursion  gets discussed in \cite{vicini_path_2021}.
    They applied it on a non-branching estimator, which we think is overkill.


\end{related}
\subsection{Green Functions}
green function stuff that we will be needing, we aren't sure in how much detail we're going to go.

\begin{example}[numerical green functions]
    There will be probably some green functions that we need
    that don't have an analytic expression yet.
\end{example}

\section{$1$-Dimensional Recursive Integrals}

\subsection{Linear Recursive Integrals}
We have algo in mind for this case based on coupled recursion on disjunct sets.


\subsection{IVPs ODEs}
An IVP example probably using DRRMC maybe compare it to parareal. Maybe also non-linear algo

\subsection{BVPs ODEs}
A BVP example using yet another algo that hopefully has the half variance phenomenon.

\section{Higher Dimensional Recursive Integrals}
\subsection{Complicated Geometry}
\begin{example}[nasty $2$D integral]
    $2$D integral that is difficult because of its geometry
\end{example}

\subsection{Recursive Brownian Motion}
WoS like way to simulate Brownian motion which is related to the green function
of the heat equation

\begin{example}[recursive Brownian motion]
    see period5
\end{example}

\subsection{Heat Equation}
a geometric robust way to solve the heat equation and maybe a higher order method to solve
the heat equation

\subsection{Wave Equation}
probably won't get to it

\newpage
\printbibliography
\newpage

\section{Appendix}
Derivation of the green functions and some expressions.
\end{document}
