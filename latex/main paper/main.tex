\documentclass[a4paper,12pt]{article}

\setlength{\textwidth}{15.0cm}
\setlength{\textheight}{24.0cm}
\setlength{\topmargin}{0cm}
\setlength{\headsep}{0cm}
\setlength{\headheight}{0cm}
\pagestyle{plain}

\usepackage[dvips]{epsfig}
\usepackage{tikz}

\usepackage[
backend=biber,
style=alphabetic,
]{biblatex}

\usepackage{amsmath,amssymb,amsthm}
\input{environments.tex}

\usepackage{comment}
\usepackage{listings}
\input{custom commands.tex}

\addbibresource{bibliography.bib} 
\setlength{\parindent}{0pt}

\begin{document}

\begin{comment}
\title{Unbiased Monte Carlo for Recursive Integrals}
\author{Isidoor Pinillo Esquivel}
\maketitle
\end{comment}

\input{titelblad_MP.tex}

\newpage
\tableofcontents
\newpage

\begin{abstract}
    We will write this at the end.
\end{abstract}

\section{Introduction}

\subsection{Introductory Example}
To get familiar with Monte Carlo for estimating recursive integrals
we demonstrate it on following problem:

\begin{equation} \label{ydy}
    y'=y, y(0)=1.
\end{equation}

By integrating both sides of (\ref{ydy}) following integral equation can be derived:

\begin{equation} \label{Integral ydy}
    y(t) = 1 + \int_{0}^{t} y(s) ds.
\end{equation}

Equation (\ref{Integral ydy}) is a recursive integral equation or to be more specific
a linear Volterra integral equation of the second type. By naively using Monte
Carlo on the recursive integral of equation (\ref{Integral ydy}) one derives following estimator:

\[
    Y(t) = 1 + t y(Ut)
    .\]

where $U=\text{Uniform}(0,1)$. If $y$ is well behaved then $E[Y(t)]=y(t)$ but we can't
calculate $Y(t)$ without accesses to $y(s),s<t$. Notice that we can replace $y$ by a
unbiased estimator of it without changing $E[Y(t)]=y(t)$ by the law of total expectance
($E[X] = E[E[X|Z]]$). By replacing $y$ by $Y$ itself we obtain a recursive expression for $Y$:

\begin{equation}\label{recursive RV}
    Y(t) = 1 + tY(Ut).
\end{equation}

Equation (\ref{recursive RV}) is a recursive random variable equation (RRVE). If you would implement equation
(\ref{recursive RV}) with recursion it will run indefinitely. A biased way of around this is by approximating
$Y(t) \approx 1$ near $t = 0$. Later we discuss Russian roulette (\ref{Russian roulette}) which
can be used as an unbiased stopping mechanism. \\

\begin{python}[implementation of (\ref{recursive RV})] \label{python eps ydy}
    \pythoncode{python code/eps_ydy.py}
\end{python}

An issue with (\ref{python eps ydy}) is that the variance increases rapidly when $t$ increases. Which we
later solve in the section on ODEs. Note that (\ref{python eps ydy}) keeps desirable properties
from unbiased Monte Carlo methods such as: being embarrassingly parallel,
robustness and having simple error estimates.


\subsection{Contributions}
We write this at the end.

\subsection{Related Work}
work on
\begin{itemize}
    \item alternative methods for recursive integrals
    \item MC work on ODEs
    \item MC work on PDEs
    \item WoS
\end{itemize}
This is just to give a general overview we probably reference specific ideas when we first introduce them.

\section{Background}
\subsection{Modifying Monte Carlo}
Once we have a RRVE it is possible to modify it
to have more desirable properties.
Our favorite work that discusses these techniques is \cite{veach_robust_nodate}.
% need to fix veach refrence see bibliography


\begin{definition}[Russian roulette] \label{Russian roulette}
    Define Russian roulette on $X$ with free parameters
    $Y_{1},Y_{2}: E[Y_{1}] = E[Y_{2}]$, $p \in [0,1]$ and $U$
    independent of $Y_{1},Y_{2},X$ the following way:
    \[
        X \rightarrow \begin{cases}
            \frac{1}{p}(X- (1-p)Y_{1}) & \text{ if } U<p \\
            Y_{2}                      & \text{ else }
        \end{cases}
        .\]
\end{definition}
The idea behind Russian roulette is replacing $X$ with a cheaper approximation $Y_{2}$
sometimes.

\begin{example}
    Say that we are interested in estimating $E[Z]$ with $Z$
    defined in the following way:
    \[
        Z = U + \frac{f(U)}{1000}
        .\]
    where $f:\mathbb{R} \rightarrow [0,1]$ expensive to compute.
    Estimating $Z$ directly would require calling $f$ each
    simulation. We can modify $Z$ to
    \[
        \tilde{Z} = U + B\left(\frac{1}{100}\right)\frac{f(U)}{10}
        .\]
    where $B(\frac{1}{100}) \sim \text{Bernouli}(\frac{1}{100})$. Now $\tilde{Z}$
    just requires calling $f$ on average once every $100$ simulations with the variance
    only increasing slightly compared to $Z$.
\end{example}

Maybe this wasn't the best example because you could also estimate the expectance of the
$2$ terms of $Z$ separately.

\begin{example}[Russian roulette on (\ref{recursive RV})]
    Russian roulette can fix the indefinite recursion issue of
    equation (\ref{recursive RV}) by approximating $Y$ near $t = 0$ with $1$. Concretely
    we replace the $t$ in front of the recursive term with $B(t) = \text{Bernouli(t)}$
    when $t<1$.
\end{example}

\vspace{0.2cm}

\begin{python}[Russian roulette on (\ref{recursive RV})]
    \pythoncode{python code/RR_ydy.py}
\end{python}

\begin{definition}[splitting]
    Splitting $X$ means using multiple $X_{j} \sim X$ not independent per se to
    lower variance by averaging them:
    \[
        \bar{X}= \frac{1}{N} \sum_{j=1}^{N} X_{j}
        .\]
\end{definition}

Splitting the recursive term in a RRVE can lead to (additive) branching recursion.
Extra care should be taken that all branches get terminated with probability $1$. This can be
achieved by termination strategies already discussed and later we discuss coupled recursion for
alleviating additive branching recursion in RRVEs.

\begin{example}[splitting on (\ref{recursive RV})]
    We can "split" the recursive term of  (\ref{recursive RV}) in $2$:
    \[
        Y(t) = 1 + \frac{t}{2}(Y_{1}(Ut)+Y_{2}(Ut))
        .\]
    with $Y_{1}(t),Y_{2}(t)$ i.i.d. $Y(t)$.
\end{example}

\vspace{0.2cm}

\begin{python}[splitting on (\ref{recursive RV})]
    \pythoncode{python code/SRR_ydy.py}
\end{python}

\begin{definition}[$2$-level MC] \label{2 level}
    $2$-level MC on $X$ with parameters $\tilde{X}, Y: E[\tilde{X}]=E[Y]$:
    \[
        X \rightarrow X-\tilde{X} + Y
        .\]
\end{definition}

This gets discussed in \cite{giles_multilevel_2013}.

\begin{definition}[control variates] \label{CV}
    Control variate on $f(X)$ is
    \[
        f(X) \rightarrow f(X)-\tilde{f}(X) + E[\tilde{f}(X)]
        .\]
\end{definition}
Control variates are a special case of $2$-level MC. Usually $\tilde{f}$ is an approximation
of $f$ to reduce variance.

\begin{example}[control variate on (\ref{recursive RV})]
    To make a control variate for (\ref{recursive RV}) that reduces variance
    we use following approximation of $y(t) \approx 1+t$:
    \[
        Y(t)= 1+t+\frac{t^{2}}{2} + t(Y(Ut)-1-Ut)
        .\]
    Notice that we can cancel the constant term of the control variate
    but that would affect the Russian roulette negatively.
\end{example}

\begin{python}
    \pythoncode{python code/CVRR_ydy.py}
\end{python}

\begin{comment}
Introduces Russian roulette, splitting, control variates, importance sampling and maybe quasi Monte Carlo with the
$y'=y$ example. We are missing importance sampling and quasi MC
\end{comment}


\subsection{Monte Carlo Trapezoidal Rule}
The Monte Carlo trapezoidal rule has some interesting convergence
behavior \cite{wu_randomised_2020}.

We aren't yet comfortable with our understanding of the randomized trapezoidal rule. \\
comparing normal vs Monte Carlo trapezoidal rule and highlighting the "half variance phenomenon".
+ maybe integrating polynomials for intuition

\subsection{Unbiased Non-Linearity}
\begin{example}[$y'=y^{2}$]
    see python note book
\end{example}

\begin{example}[$e^{E[X]}$]
    see python note book
\end{example}

\subsection{Recursion}

\begin{example}[coupled recursion]
    example with $y'=y$ (I need to redo this example)
\end{example}

\begin{example}[recursion in recursion]
    maybe induction in induction proof example
\end{example}

\begin{example}[tail recursion]
    discuss problems with implementing recursion and solutions.  \\
    inverse problem example
\end{example}

\subsection{Green Functions}
green function stuff that we will be needing, we aren't sure in how much detail we're going to go.

\begin{example}[numerical green functions]
    There will be probably some green functions that we need
    that don't have an analytic expression yet.
\end{example}

\section{$1$-Dimensional Recursive Integrals}

\subsection{Linear Recursive Integrals}
We have algo in mind for this case based on coupled recursion on disjunct sets.


\subsection{IVPs ODEs}
An IVP example probably using DRRMC maybe compare it to parareal. Maybe also non-linear algo

\subsection{BVPs ODEs}
A BVP example using yet another algo that hopefully has the half variance phenomenon.

\section{Higher Dimensional Recursive Integrals}
\subsection{Complicated Geometry}
\begin{example}[nasty $2$D integral]
    $2$D integral that is difficult because of its geometry
\end{example}

\subsection{Recursive Brownian Motion}
WoS like way to simulate Brownian motion which is related to the green function
of the heat equation

\begin{example}[recursive Brownian motion]
    see period5
\end{example}

\subsection{Heat Equation}
a geometric robust way to solve the heat equation and maybe a higher order method to solve
the heat equation

\subsection{Wave Equation}
probably won't get to it

\newpage
\printbibliography
\newpage

\section{Appendix}
Derivation of the green functions and some expressions.
\end{document}
