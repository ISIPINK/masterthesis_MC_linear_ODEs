
@techreport{settles_active_2009,
	type = {Technical {Report}},
	title = {Active {Learning} {Literature} {Survey}},
	url = {https://minds.wisconsin.edu/handle/1793/60660},
	abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the training data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difficult, time-consuming, or expensive to obtain. 
 
This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.},
	language = {en},
	urldate = {2022-09-17},
	institution = {University of Wisconsin-Madison Department of Computer Sciences},
	author = {Settles, Burr},
	year = {2009},
	note = {Accepted: 2012-03-15T17:23:56Z},
	keywords = {active learning, Computer Science - Machine Learning},
	file = {Settles - 2009 - Active Learning Literature Survey.pdf:C\:\\Users\\isido\\Zotero\\storage\\XZAZ8M6G\\Settles - 2009 - Active Learning Literature Survey.pdf:application/pdf;Snapshot:C\:\\Users\\isido\\Zotero\\storage\\A9QXPGLH\\60660.html:text/html},
}

@misc{baier_switching_2020,
	title = {Switching {Scheme}: {A} {Novel} {Approach} for {Handling} {Incremental} {Concept} {Drift} in {Real}-{World} {Data} {Sets}},
	shorttitle = {Switching {Scheme}},
	url = {http://arxiv.org/abs/2011.02738},
	abstract = {Machine learning models nowadays play a crucial role for many applications in business and industry. However, models only start adding value as soon as they are deployed into production. One challenge of deployed models is the effect of changing data over time, which is often described with the term concept drift. Due to their nature, concept drifts can severely affect the prediction performance of a machine learning system. In this work, we analyze the effects of concept drift in the context of a real-world data set. For efﬁcient concept drift handling, we introduce the switching scheme which combines the two principles of retraining and updating of a machine learning model. Furthermore, we systematically analyze existing regular adaptation as well as triggered adaptation strategies. The switching scheme is instantiated on New York City taxi data, which is heavily inﬂuenced by changing demand patterns over time. We can show that the switching scheme outperforms all other baselines and delivers promising prediction results.},
	language = {en},
	urldate = {2022-09-17},
	publisher = {arXiv},
	author = {Baier, Lucas and Kellner, Vincent and Kühl, Niklas and Satzger, Gerhard},
	month = nov,
	year = {2020},
	note = {arXiv:2011.02738 [cs]},
	keywords = {active learning, Computer Science - Machine Learning},
	annote = {Comment: 54th Annual Hawaii International Conference on System Sciences (HICSS-54)},
	file = {Baier e.a. - 2020 - Switching Scheme A Novel Approach for Handling In.pdf:C\:\\Users\\isido\\Zotero\\storage\\4SZGPPN7\\Baier e.a. - 2020 - Switching Scheme A Novel Approach for Handling In.pdf:application/pdf},
}

@misc{mayaki_autoregressive_2022,
	title = {Autoregressive based {Drift} {Detection} {Method}},
	url = {http://arxiv.org/abs/2203.04769},
	abstract = {In the classic machine learning framework, models are trained on historical data and used to predict future values. It is assumed that the data distribution does not change over time (stationarity). However, in real-world scenarios, the data generation process changes over time and the model has to adapt to the new incoming data. This phenomenon is known as concept drift and leads to a decrease in the predictive model’s performance. In this study, we propose a new concept drift detection method based on autoregressive models called ADDM. This method can be integrated into any machine learning algorithm from deep neural networks to simple linear regression model. Our results show that this new concept drift detection method outperforms the state-of-the-art drift detection methods, both on synthetic data sets and real-world data sets. Our approach is theoretically guaranteed as well as empirical and effective for the detection of various concept drifts. In addition to the drift detector, we proposed a new method of concept drift adaptation based on the severity of the drift.},
	language = {en},
	urldate = {2022-09-17},
	publisher = {arXiv},
	author = {Mayaki, Mansour Zoubeirou A. and Riveill, Michel},
	month = mar,
	year = {2022},
	note = {arXiv:2203.04769 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Mayaki en Riveill - 2022 - Autoregressive based Drift Detection Method.pdf:C\:\\Users\\isido\\Zotero\\storage\\54YTNIWQ\\Mayaki en Riveill - 2022 - Autoregressive based Drift Detection Method.pdf:application/pdf},
}

@article{sabelfeld_sparsified_2009,
	title = {Sparsified {Randomization} {Algorithms} for large systems of linear equations and a new version of the {Random} {Walk} on {Boundary} method},
	volume = {15},
	issn = {0929-9629, 1569-3961},
	url = {https://www.degruyter.com/document/doi/10.1515/MCMA.2009.015/html},
	doi = {10.1515/MCMA.2009.015},
	abstract = {Sparsiﬁed Randomization Monte Carlo (SRMC) algorithms for solving large systems of linear algebraic equations are presented. We construct efﬁcient stochastic algorithms based on a probabilistic sampling of small size sub-matrices, or a randomized evaluation of a matrix-vector product and matrix iterations via a random sparsiﬁcation of the matrix. This approach is beyond the standard Markov chain based Neumann–Ulam method which has no universal instrument to decrease the variance. Instead, in the new method, ﬁrst, the variance can be decreased by increasing the number of the sampled columns of the matrix in play, and second, it is free of the restricted assumption of the Neumann–Ulam scheme that the Neumann series converges. We apply the developed methods to different stochastic iterative procedures. Application to boundary integral equation of the electrostatic potential theory is given where we develop a SRMC algorithm for solving the approximated system of linear algebraic equations, and compare it with the standard Random Walk on Boundary method.},
	language = {en},
	number = {3},
	urldate = {2022-09-17},
	journal = {Monte Carlo Methods and Applications},
	author = {Sabelfeld, K. and Mozartova, N.},
	month = jan,
	year = {2009},
	keywords = {monte carlo, linear systems},
	file = {Sabelfeld en Mozartova - 2009 - Sparsified Randomization Algorithms for large syst.pdf:C\:\\Users\\isido\\Zotero\\storage\\YBMLVR26\\Sabelfeld en Mozartova - 2009 - Sparsified Randomization Algorithms for large syst.pdf:application/pdf},
}

@article{benzi_analysis_2017,
	title = {Analysis of {Monte} {Carlo} accelerated iterative methods for sparse linear systems},
	volume = {24},
	issn = {1070-5325, 1099-1506},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/nla.2088},
	doi = {10.1002/nla.2088},
	abstract = {We consider hybrid deterministic-stochastic iterative algorithms for the solution of large, sparse linear systems. Starting from a convergent splitting of the coeﬃcient matrix, we analyze various types of Monte Carlo acceleration schemes applied to the original preconditioned Richardson (stationary) iteration. These methods are expected to have considerable potential for resiliency to faults when implemented on massively parallel machines. We establish suﬃcient conditions for the convergence of the hybrid schemes, and we investigate diﬀerent types of preconditioners including sparse approximate inverses. Numerical experiments on linear systems arising from the discretization of partial diﬀerential equations are presented.},
	language = {en},
	number = {3},
	urldate = {2022-09-17},
	journal = {Numerical Linear Algebra with Applications},
	author = {Benzi, Michele and Evans, Thomas M. and Hamilton, Steven P. and Lupo Pasini, Massimiliano and Slattery, Stuart R.},
	month = may,
	year = {2017},
	keywords = {monte carlo, linear systems},
	file = {Benzi e.a. - 2017 - Analysis of Monte Carlo accelerated iterative meth.pdf:C\:\\Users\\isido\\Zotero\\storage\\ZL2JBC57\\Benzi e.a. - 2017 - Analysis of Monte Carlo accelerated iterative meth.pdf:application/pdf},
}

@article{wu_multiway_2019,
	title = {Multiway {Monte} {Carlo} {Method} for {Linear} {Systems}},
	volume = {41},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/18M121527X},
	doi = {10.1137/18M121527X},
	abstract = {We study a novel variation on the Ulam–von Neumann Monte Carlo method for solving a linear system. This is an old randomized procedure that results from using a random walk to stochastically evaluate terms in the Neumann series. In order to apply this procedure, the variance of the stochastic estimator needs to be bounded. The best known suﬃcient condition for bounding the variance is that the inﬁnity norm of the matrix in the Neumann series is smaller than one, which greatly limits the usability of this method. We improve this condition by proposing a new stochastic estimator based on a diﬀerent type of random walk. Our multiway walk and estimator is based on a time-inhomogeneous Markov process that iterates through a sequence of transition matrices built from the original linear system. For our new method, we prove that a necessary and suﬃcient condition for convergence is that the spectral radius of the elementwise absolute value of the matrix underlying the Neumann series is smaller than one. This is a strictly weaker condition than currently exists. In addition, our new method is often faster than the standard algorithm. Through experiments, we demonstrate the potential for our method to reduce the time needed to solve linear equations by incorporating it into an outer iterative method.},
	language = {en},
	number = {6},
	urldate = {2022-09-17},
	journal = {SIAM Journal on Scientific Computing},
	author = {Wu, Tao and Gleich, David F.},
	month = jan,
	year = {2019},
	keywords = {monte carlo, linear systems},
	pages = {A3449--A3475},
	file = {Wu en Gleich - 2019 - Multiway Monte Carlo Method for Linear Systems.pdf:C\:\\Users\\isido\\Zotero\\storage\\UABEC4GQ\\Wu en Gleich - 2019 - Multiway Monte Carlo Method for Linear Systems.pdf:application/pdf},
}

@article{dick_higher_2019,
	title = {Higher order {Quasi}-{Monte} {Carlo} integration for {Bayesian} {PDE} {Inversion}},
	volume = {77},
	issn = {08981221},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0898122118305315},
	doi = {10.1016/j.camwa.2018.09.019},
	abstract = {We analyze combined Quasi-Monte Carlo quadrature and Finite Element approximations in Bayesian estimation of solutions to countably-parametric operator equations with holomorphic dependence on the parameters as considered in Schillings and Schwab (2014). Such problems arise in numerical uncertainty quantification and in Bayesian inversion of operator equations with distributed uncertain inputs, such as uncertain coefficients, uncertain domains or uncertain source terms and boundary data. We show that the parametric Bayesian posterior densities belong to a class of weighted Bochner spaces of functions of countably many variables, with a particular structure of the QMC quadrature weights: up to a (problem-dependent, and possibly large) finite dimension S product weights can be used, and beyond this dimension, weighted spaces with so-called SPOD weights, recently introduced in Dick et al. (2014), are used to describe the solution regularity. We establish error bounds for higher order Quasi-Monte Carlo quadrature for the Bayesian estimation based on Dick et al. (2016). It implies, in particular, regularity of the parametric solution and of the countably-parametric Bayesian posterior density in SPOD (‘‘Smoothness driven, Product and Order Dependent’’) weighted spaces of integrand functions. This, in turn, implies that the Quasi-Monte Carlo quadrature methods in Dick et al. (2014) are applicable to these problem classes, with dimension-independent convergence rates O(N−1/p) of N-point HoQMC approximated Bayesian estimates, where 0 {\textless} p {\textless} 1 depends only on the sparsity class of the uncertain input in the Bayesian estimation. Fast componentby-component (CBC for short) construction Gantner and Schwab (2016) allow efficient deterministic Bayesian estimation with up to 104 parameters.},
	language = {en},
	number = {1},
	urldate = {2022-09-17},
	journal = {Computers \& Mathematics with Applications},
	author = {Dick, Josef and Gantner, Robert N. and Le Gia, Quoc T. and Schwab, Christoph},
	month = jan,
	year = {2019},
	keywords = {monte carlo},
	pages = {144--172},
	file = {Dick e.a. - 2019 - Higher order Quasi-Monte Carlo integration for Bay.pdf:C\:\\Users\\isido\\Zotero\\storage\\JNIFMW27\\Dick e.a. - 2019 - Higher order Quasi-Monte Carlo integration for Bay.pdf:application/pdf},
}

@article{ji_reusing_2012,
	title = {Reusing {Random} {Walks} in {Monte} {Carlo} {Methods} for {Linear} {Systems}},
	volume = {9},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050912001627},
	doi = {10.1016/j.procs.2012.04.041},
	abstract = {In this paper, we present an approach of reusing random walks in Monte Carlo methods for linear systems. The fundamental idea is, during the Monte Carlo sampling process, the random walks generated to estimate one unknown element can also be effectively reused to estimate the other unknowns in the solution vector. As a result, when the random walks are reused, a single random walk can contribute samples for estimations of multiple unknowns in the solution simultaneously while ensuring that the samples for the same unknown element are statistically independent. Consequently, the total number of random walk transition steps needed for estimating the overall solution vector is reduced, which improves the performance of the Monte Carlo algorithm. We apply this approach to the Monte Carlo algorithm in two linear algebra applications, including solving a system of linear equations and approximating the inversion of a matrix. Our computational results show that compared to the conventional implementations of Monte Carlo algorithms for linear systems without random walk reusing, our approach can significantly improve the performance of Monte Carlo sampling process by reducing the overall number of transition steps in random walks to obtain the entire solution within desired precision.},
	language = {en},
	urldate = {2022-09-17},
	journal = {Procedia Computer Science},
	author = {Ji, Hao and Li, Yaohang},
	year = {2012},
	keywords = {monte carlo, linear systems},
	pages = {383--392},
	file = {Ji en Li - 2012 - Reusing Random Walks in Monte Carlo Methods for Li.pdf:C\:\\Users\\isido\\Zotero\\storage\\G5YWGZPN\\Ji en Li - 2012 - Reusing Random Walks in Monte Carlo Methods for Li.pdf:application/pdf},
}

@article{sabelfeld_vector_2016,
	title = {Vector {Monte} {Carlo} stochastic matrix-based algorithms for large linear systems},
	volume = {22},
	issn = {0929-9629, 1569-3961},
	url = {https://www.degruyter.com/document/doi/10.1515/mcma-2016-0112/html},
	doi = {10.1515/mcma-2016-0112},
	abstract = {In this short article we suggest randomized scalable stochastic matrix-based algorithms for large linear systems. The idea behind these stochastic methods is a randomized vector representation of matrix iterations. In addition, to minimize the variance, it is suggested to use stochastic and double stochastic matrices for efficient randomized calculation of matrix iterations and a random gradient based search strategy. The iterations are performed by sampling random rows and columns only, thus avoiding not only matrix matrix but also matrix vector multiplications. Further improvements of the methods can be obtained through projections by a random gaussian matrix.},
	language = {en},
	number = {3},
	urldate = {2022-09-17},
	journal = {Monte Carlo Methods and Applications},
	author = {Sabelfeld, Karl K.},
	month = jan,
	year = {2016},
	keywords = {monte carlo, linear systems},
	file = {Sabelfeld - 2016 - Vector Monte Carlo stochastic matrix-based algorit.pdf:C\:\\Users\\isido\\Zotero\\storage\\EX64VKZ2\\Sabelfeld - 2016 - Vector Monte Carlo stochastic matrix-based algorit.pdf:application/pdf},
}

@incollection{carmona_least-squares_2012,
	address = {Berlin, Heidelberg},
	title = {Least-{Squares} {Monte} {Carlo} for {Backward} {SDEs}},
	volume = {12},
	isbn = {978-3-642-25745-2 978-3-642-25746-9},
	url = {http://link.springer.com/10.1007/978-3-642-25746-9_8},
	abstract = {In this paper we ﬁrst give a review of the least-squares Monte Carlo approach for approximating the solution of backward stochastic diﬀerential equations (BSDEs) ﬁrst suggested by Gobet, Lemor, and Warin (Ann. Appl. Probab., 15, 2005, 2172–2202). We then propose the use of basis functions, which form a system of martingales, and explain how the least-squares Monte Carlo scheme can be simpliﬁed by exploiting the martingale property of the basis functions. We partially compare the convergence behavior of the original scheme and the scheme based on martingale basis functions, and provide several numerical examples related to option pricing problems under diﬀerent interest rates for borrowing and investing.},
	language = {en},
	urldate = {2022-09-17},
	booktitle = {Numerical {Methods} in {Finance}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bender, Christian and Steiner, Jessica},
	editor = {Carmona, René A. and Del Moral, Pierre and Hu, Peng and Oudjane, Nadia},
	year = {2012},
	doi = {10.1007/978-3-642-25746-9_8},
	note = {Series Title: Springer Proceedings in Mathematics},
	keywords = {monte carlo, SDE},
	pages = {257--289},
	file = {Bender en Steiner - 2012 - Least-Squares Monte Carlo for Backward SDEs.pdf:C\:\\Users\\isido\\Zotero\\storage\\4LPW4G2Q\\Bender en Steiner - 2012 - Least-Squares Monte Carlo for Backward SDEs.pdf:application/pdf},
}

@misc{deaconu_walk_2015,
	title = {The walk on moving spheres: a new tool for simulating {Brownian} motion's exit time from a domain},
	shorttitle = {The walk on moving spheres},
	url = {http://arxiv.org/abs/1401.3695},
	abstract = {In this paper we introduce a new method for the simulation of the exit time and exit position of a δ-dimensional Brownian motion from a domain. The main interest of our method is that it avoids splitting time schemes as well as inversion of complicated series. The method, called walk on moving spheres algorithm, was ﬁrst introduced for hitting times of Bessel processes. In this study this method is adapted and developed for the ﬁrst time for the Brownian motion hitting times. The idea is to use the connexion between the δdimensional Bessel process and the δ-dimensional Brownian motion thanks to an explicit Bessel hitting time distribution associated with a particular curved boundary. This allows to build a fast and accurate numerical scheme for approximating the hitting time. We introduce also an overview of existing methods for the simulation of the Brownian hitting time and perform numerical comparisons with existing methods.},
	language = {en},
	urldate = {2022-09-17},
	publisher = {arXiv},
	author = {Deaconu, Madalina and Herrmann, Samuel and Maire, Sylvain},
	month = oct,
	year = {2015},
	note = {arXiv:1401.3695 [math]},
	keywords = {monte carlo, Mathematics - Probability, PDE, walk on spheres},
	file = {Deaconu e.a. - 2015 - The walk on moving spheres a new tool for simulat.pdf:C\:\\Users\\isido\\Zotero\\storage\\EIYVRXS7\\Deaconu e.a. - 2015 - The walk on moving spheres a new tool for simulat.pdf:application/pdf},
}

@article{sawhney_grid-free_2022,
	title = {Grid-free {Monte} {Carlo} for {PDEs} with spatially varying coefficients},
	volume = {41},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3528223.3530134},
	doi = {10.1145/3528223.3530134},
	abstract = {Partial differential equations (PDEs) with spatially varying coefficients arise throughout science and engineering, modeling rich heterogeneous material behavior. Yet conventional PDE solvers struggle with the immense complexity found in nature, since they must first discretize the problem---leading to spatial aliasing, and global meshing/sampling that is costly and error-prone. We describe a method that approximates neither the domain geometry, the problem data, nor the solution space, providing the exact solution (in expectation) even for problems with extremely detailed geometry and intricate coefficients. Our main contribution is to extend the
              walk on spheres (WoS)
              algorithm from constant- to variable-coefficient problems, by drawing on techniques from volumetric rendering. In particular, an approach inspired by
              null-scattering
              yields unbiased Monte Carlo estimators for a large class of 2nd order elliptic PDEs, which share many attractive features with Monte Carlo rendering: no meshing, trivial parallelism, and the ability to evaluate the solution at any point without solving a global system of equations.},
	language = {en},
	number = {4},
	urldate = {2022-09-17},
	journal = {ACM Transactions on Graphics},
	author = {Sawhney, Rohan and Seyb, Dario and Jarosz, Wojciech and Crane, Keenan},
	month = jul,
	year = {2022},
	keywords = {monte carlo, PDE, walk on spheres, rendering},
	pages = {1--17},
	file = {Sawhney e.a. - 2022 - Grid-free Monte Carlo for PDEs with spatially vary.pdf:C\:\\Users\\isido\\Zotero\\storage\\52QDH9YB\\Sawhney e.a. - 2022 - Grid-free Monte Carlo for PDEs with spatially vary.pdf:application/pdf},
}

@article{sawhney_monte_nodate,
	title = {Monte {Carlo} {Geometry} {Processing}:{A} {Grid}-{Free} {Approach} to {PDE}-{Based} {Methods} on {Volumetric} {Domains}},
	volume = {38},
	abstract = {This paper explores how core problems in PDE-based geometry processing can be efficiently and reliably solved via grid-free Monte Carlo methods. Modern geometric algorithms often need to solve Poisson-like equations on geometrically intricate domains. Conventional methods most often mesh the domain, which is both challenging and expensive for geometry with fine details or imperfections (holes, self-intersections, etc.). In contrast, gridfree Monte Carlo methods avoid mesh generation entirely, and instead just evaluate closest point queries. They hence do not discretize space, time, nor even function spaces, and provide the exact solution (in expectation) even on extremely challenging models. More broadly, they share many benefits with Monte Carlo methods from photorealistic rendering: excellent scaling, trivial parallel implementation, view-dependent evaluation, and the ability to work with any kind of geometry (including implicit or procedural descriptions). We develop a complete “black box” solver that encompasses integration, variance reduction, and visualization, and explore how it can be used for various geometry processing tasks. In particular, we consider several fundamental linear elliptic PDEs with constant coefficients on solid regions of Rn . Overall we find that Monte Carlo methods significantly broaden the horizons of geometry processing, since they easily handle problems of size and complexity that are essentially hopeless for conventional methods.},
	language = {en},
	number = {4},
	author = {Sawhney, Rohan and Crane, Keenan},
	keywords = {monte carlo, PDE, walk on spheres, rendering},
	pages = {18},
	file = {Sawhney en Crane - Monte Carlo Geometry ProcessingA Grid-Free Approa.pdf:C\:\\Users\\isido\\Zotero\\storage\\J5MXL5PE\\Sawhney en Crane - Monte Carlo Geometry ProcessingA Grid-Free Approa.pdf:application/pdf},
}

@misc{ferguson_deeply_2018,
	title = {Deeply {Learning} {Derivatives}},
	url = {http://arxiv.org/abs/1809.02233},
	abstract = {This paper uses deep learning to value derivatives. The approach is broadly applicable, and we use a call option on a basket of stocks as an example. We show that the deep learning model is accurate and very fast, capable of producing valuations a million times faster than traditional models. We develop a methodology to randomly generate appropriate training data and explore the impact of several parameters including layer width and depth, training data quality and quantity on model speed and accuracy.},
	language = {en},
	urldate = {2022-09-19},
	publisher = {arXiv},
	author = {Ferguson, Ryan and Green, Andrew},
	month = oct,
	year = {2018},
	note = {arXiv:1809.02233 [cs, q-fin]},
	keywords = {Computer Science - Machine Learning, Quantitative Finance - Computational Finance, finance, machine learning},
	file = {Ferguson en Green - 2018 - Deeply Learning Derivatives.pdf:C\:\\Users\\isido\\Zotero\\storage\\5KLHN2WM\\Ferguson en Green - 2018 - Deeply Learning Derivatives.pdf:application/pdf},
}

@misc{lehtinen_noise2noise_2018,
	title = {{Noise2Noise}: {Learning} {Image} {Restoration} without {Clean} {Data}},
	shorttitle = {{Noise2Noise}},
	url = {http://arxiv.org/abs/1803.04189},
	abstract = {We apply basic statistical reasoning to signal reconstruction by machine learning – learning to map corrupted observations to clean signals – with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans – all corrupted by different processes – based on noisy data only.},
	language = {en},
	urldate = {2022-09-19},
	publisher = {arXiv},
	author = {Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
	month = oct,
	year = {2018},
	note = {arXiv:1803.04189 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, machine learning},
	annote = {Comment: Added link to official implementation and updated MRI results to match it},
	file = {Lehtinen e.a. - 2018 - Noise2Noise Learning Image Restoration without Cl.pdf:C\:\\Users\\isido\\Zotero\\storage\\NAGQ9NY7\\Lehtinen e.a. - 2018 - Noise2Noise Learning Image Restoration without Cl.pdf:application/pdf},
}

@misc{noauthor_accelerating_2020,
	title = {Accelerating {Python} for {Exotic} {Option} {Pricing}},
	url = {https://developer.nvidia.com/blog/accelerating-python-for-exotic-option-pricing/},
	abstract = {In finance, computation efficiency can be directly converted to trading profits sometimes. Quants are facing the challenges of trading off research efficiency with computation efficiency.},
	language = {en-US},
	urldate = {2022-09-19},
	journal = {NVIDIA Technical Blog},
	month = mar,
	year = {2020},
	keywords = {cupy, numba},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\LNJMET6I\\accelerating-python-for-exotic-option-pricing.html:text/html},
}

@misc{noauthor_github_nodate,
	title = {{GitHub} - {NVIDIA}/fsi-samples: {A} collection of open-source {GPU} accelerated {Python} tools and examples for quantitative analyst tasks and leverages {RAPIDS} {AI} project, {Numba}, {cuDF}, and {Dask}.},
	url = {https://github.com/NVIDIA/fsi-samples},
	urldate = {2022-09-19},
	file = {GitHub - NVIDIA/fsi-samples\: A collection of open-source GPU accelerated Python tools and examples for quantitative analyst tasks and leverages RAPIDS AI project, Numba, cuDF, and Dask.:C\:\\Users\\isido\\Zotero\\storage\\FTUN9SNE\\fsi-samples.html:text/html},
}

@misc{noauthor_nevilles_2022,
	title = {Neville's algorithm},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Neville%27s_algorithm&oldid=1100728778},
	abstract = {In mathematics, Neville's algorithm is an algorithm used for polynomial interpolation that was derived by the mathematician Eric Harold Neville in 1934. Given n + 1 points, there is a unique polynomial of degree ≤ n which goes through the given points. Neville's algorithm evaluates this polynomial.
Neville's algorithm is based on the Newton form of the interpolating polynomial and the recursion relation for the divided differences. It is similar to Aitken's algorithm (named after Alexander Aitken), which is nowadays not used.},
	language = {en},
	urldate = {2022-09-26},
	journal = {Wikipedia},
	month = jul,
	year = {2022},
	note = {Page Version ID: 1100728778},
	keywords = {finite differnces, interpolation},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\J69UYIG8\\Neville's_algorithm.html:text/html},
}

@misc{vanroose_krylov-simplex_2021,
	title = {Krylov-{Simplex} method that minimizes the residual in \${\textbackslash}ell\_1\$-norm or \${\textbackslash}ell\_{\textbackslash}infty\$-norm},
	url = {http://arxiv.org/abs/2101.11416},
	abstract = {The paper presents two variants of a Krylov-Simplex iterative method that combines Krylov and simplex iterations to minimize the residual r = b−Ax. The ﬁrst method minimizes r ∞, i.e. maximum of the absolute residuals. The second minimizes r 1, and ﬁnds the solution with the least absolute residuals. Both methods search for an optimal solution xk in a Krylov subspace which results in a small linear programming problem. A specialized simplex algorithm solves this projected problem and ﬁnds the optimal linear combination of Krylov basis vectors to approximate the solution. The resulting simplex algorithm requires the solution of a series of small dense linear systems that only diﬀer by rank-one updates. The QR factorization of these matrices is updated each iteration. We demonstrate the eﬀectiveness of the methods with numerical experiments.},
	language = {en},
	urldate = {2022-09-26},
	publisher = {arXiv},
	author = {Vanroose, Wim and Cornelis, Jeffrey},
	month = jan,
	year = {2021},
	note = {arXiv:2101.11416 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, Mathematics - Optimization and Control, optimization-constrained, optimization},
	annote = {Comment: 22 pages},
	file = {Vanroose en Cornelis - 2021 - Krylov-Simplex method that minimizes the residual .pdf:C\:\\Users\\isido\\Zotero\\storage\\M2G9WBDA\\Vanroose en Cornelis - 2021 - Krylov-Simplex method that minimizes the residual .pdf:application/pdf},
}

@article{caglar_solution_2007,
	title = {Solution of fifth order boundary value problems by using local polynomial regression},
	volume = {186},
	issn = {00963003},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0096300306010204},
	doi = {10.1016/j.amc.2006.08.046},
	abstract = {In this paper, we present a novel method based on the local polynomial regression for solving of ﬁfth order boundary value problems. The method is tested on numerical example to demonstrate its usefulness. The method presented in this paper is also compared with those developed by Siddiqi and Akram [Solution of ﬁfth order boundary value problems using nonpolynomial spline technique, Appl. Math. Comput. 175 (2006) 1575–1581], as well and is observed to be better.},
	language = {en},
	number = {2},
	urldate = {2022-09-26},
	journal = {Applied Mathematics and Computation},
	author = {Caglar, Hikmet and Caglar, Nazan},
	month = mar,
	year = {2007},
	keywords = {ODE},
	pages = {952--956},
	file = {Caglar en Caglar - 2007 - Solution of fifth order boundary value problems by.pdf:C\:\\Users\\isido\\Zotero\\storage\\Z8EJ5CDR\\Caglar en Caglar - 2007 - Solution of fifth order boundary value problems by.pdf:application/pdf},
}

@article{su_numerical_2012,
	title = {Numerical {Solution} of {Integro}-{Differential} {Equations} with {Local} {Polynomial} {Regression}},
	volume = {02},
	issn = {2161-718X, 2161-7198},
	url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/ojs.2012.23043},
	doi = {10.4236/ojs.2012.23043},
	abstract = {In recent years, there has been a growing interest in the Integro-Differential Equations (IDEs) which are a combination of differential and Fredholm-Volterra integral equations. IDEs play an important role in many branches of linear and nonlinear functional analysis and their applications in the theory of engineering, mechanics, physics, chemistry, astronomy, biology, economics, potential theory and electrostatics. The mentioned integro-differential equations are usually difficult to solve analytically, so a numerical method is required. Many different methods are used to obtain the solution of the linear and nonlinear IDEs such as the successive approximations, A domain decomposition, Homotopy perturbation method, Chebyshev and Taylor collocation, Haar Wavelet, Tau and Walsh series methods [1-8]. Recently, the authors [9], have used local polynomial regression (LPR) method for the numerical solution of linear and non-linear Fredholm and Volterra integral equations. In this paper, we consider the linear IDEs,     x a yx pxyx gx K     (1) where the upper limit of the integral is constant or variable,    are constants, g xpx  and the kernel     K xt y  are given functions, whereas x 0 needs to be determined. The subject of this paper is to try to find numerical solutions of integro-differential equations by means of local polynomial regression method which is presented firstly by Hikmat Caglar [9]. Finally, we show the method to achieve the desired accuracy. Details of the structure of the present method are explained in sections. We apply LPR method for IDEs. In Section 3, it’s proved the efficiency of numerical method. Finally, Section 4 contains some conclusions and directions for future expectations and researches.},
	language = {en},
	number = {03},
	urldate = {2022-09-26},
	journal = {Open Journal of Statistics},
	author = {Su, Liyun and Yan, Tianshun and Zhao, Yanyong and Li, Fenglan and Liu, Ruihua},
	year = {2012},
	keywords = {ODE, integral equations},
	pages = {352--355},
	file = {Su e.a. - 2012 - Numerical Solution of Integro-Differential Equatio.pdf:C\:\\Users\\isido\\Zotero\\storage\\MDM22BGE\\Su e.a. - 2012 - Numerical Solution of Integro-Differential Equatio.pdf:application/pdf},
}

@article{haji-sheikh_floating_1966,
	title = {The {Floating} {Random} {Walk} and {Its} {Application} to {Monte} {Carlo} {Solutions} of {Heat} {Equations}},
	volume = {14},
	url = {http://www.jstor.org/stable/2946271},
	abstract = {Introduction. As longago as the turnof thecentury, it hadbeen recognizedthatprobabilitysamplingtechniquescould,in principle, be employed in solvingpartialdifferential equationsarisingin physicsand engineering. In 1899,LordRayleigh[1]demonstratet dherelationshipbetweenstochastic processesand parabolicdifferential equations,whilea similarrelationship was establishedin 1928by Courantand hiscoworkers[2]forellipticdifferentialequations.The earliestknowncomputationalexperiments involving samplingtechniquesare due to Todd[3],whoworkedwithLaplace's equationin asquaregeometryhavingprescribedboundarytemperatures. Somewhat more recently,Ehrlich [4] used samplingtechniquesto solve the equation a2u +au kau \_+ - + -- = 0 ax2 ay2 y ay in simplepolygonswithprescribedboundaryvalues ofu. In general,there have been relativelyfew suchcomputationalexperiments. The name "Monte Carlo" is characteristically used to describeprobabilitysamplingtechniquesthat approximatethe solutionofmathematical or physicalproblems. A valuable surveyofMonte Carlomethodsas applied to the solutionof differential and difference equationshas been provided by Curtiss[5]. A primereferencesourceforthe randomwalk technique, which is basic to Monte Carlo methods,is Feller's text on probability theory[6]},
	language = {en},
	number = {2},
	journal = {SIAM Journal on Applied Mathematics},
	author = {Haji-Sheikh, A. and Sparrow, E. M.},
	year = {1966},
	keywords = {monte carlo, PDE, walk on spheres},
	pages = {370--389},
	annote = {
Kram1032
1 month ago
Love to see this progress! I wonder whether this could be done in the time domain as well? I know, it tends to be the case with Monte-Carlo methods, that they are timeless. They try to sample some steady state, right? But if you make the entire spacio-temporal volume your "steady state", it ought to be possible regardless. Normally you couldn't properly save, and keep access to all that data so you could completely skip out on frames. But perhaps there is a way around that somehow? Maybe something inspired by NeRFs. A sort of data structure that's iteratively updated with gradient descend anyways, so you could probably reformulate that as a PDE that can be handled with a method like this I'd think? Then you could effectively get rid of all explicit grids (globally spatial, locally surface- or volume-spatial, and temporal) and go gridless continuous end to end perhaps. It seems to me the application of refining subsurface scattering is a nice example of how that might be helpful.

·


Rohan Sawhney
1 month ago
Hi! It's possible to handle temporal problems such as the heat equation with walk on spheres, I recommend looking at this paper: https://epubs.siam.org/doi/10.1137/0114031 Since the heat equation is an initial value problem, the high-level idea is to also sample an "exit time" at each step of the random walk from a known distribution (in addition to a uniform exit location on the sphere), and to keep a counter of the total time elapsed during the walk. If the counter value exceeds the predetermined amount/time for which one wants to flow the heat, then the initial value at the current position of the random walk inside the domain is added to the Monte Carlo estimate.
 1 

Kram1032
1 month ago (edited)
 @Rohan Sawhney  thanks I'll check it out! sounds interesting
},
	file = {Haji-Sheikh en Sparrow - 1966 - The Floating Random Walk and Its Application to Mo.pdf:C\:\\Users\\isido\\Zotero\\storage\\M4YQIH62\\Haji-Sheikh en Sparrow - 1966 - The Floating Random Walk and Its Application to Mo.pdf:application/pdf},
}

@book{hastie_elements_2001,
	title = {The {Elements} of {Statistical} {Learning}  {Data} {Mining}, {Inference}, and {Prediction}},
	abstract = {The many topics include neural networks, support vector machines, classification trees and boosting - the first comprehensive treatment of this topic in any book},
	publisher = {Springer Berlin Heidelberg},
	author = {Hastie, Trevor and Friedman, Jerome and Tibshirani, Robert},
	year = {2001},
	keywords = {machine learning, statistics},
	file = {ESLII_print12_toc-compressed.pdf:C\:\\Users\\isido\\Zotero\\storage\\Q8DWLFW6\\ESLII_print12_toc-compressed.pdf:application/pdf},
}

@article{cockayne_bayesian_2019,
	title = {Bayesian {Probabilistic} {Numerical} {Methods}},
	volume = {61},
	issn = {0036-1445, 1095-7200},
	url = {http://arxiv.org/abs/1702.03673},
	doi = {10.1137/17M1139357},
	abstract = {The emergent field of probabilistic numerics has thus far lacked clear statistical principals. This paper establishes Bayesian probabilistic numerical methods as those which can be cast as solutions to certain inverse problems within the Bayesian framework. This allows us to establish general conditions under which Bayesian probabilistic numerical methods are well-defined, encompassing both non-linear and non-Gaussian models. For general computation, a numerical approximation scheme is proposed and its asymptotic convergence established. The theoretical development is then extended to pipelines of computation, wherein probabilistic numerical methods are composed to solve more challenging numerical tasks. The contribution highlights an important research frontier at the interface of numerical analysis and uncertainty quantification, with a challenging industrial application presented.},
	language = {en},
	number = {3},
	urldate = {2022-09-30},
	journal = {SIAM Review},
	author = {Cockayne, Jon and Oates, Chris and Sullivan, Tim and Girolami, Mark},
	month = jan,
	year = {2019},
	note = {arXiv:1702.03673 [cs, math, stat]},
	keywords = {Mathematics - Numerical Analysis, Mathematics - Statistics Theory, Statistics - Computation, Statistics - Methodology, statistics},
	pages = {756--789},
	annote = {Received from professor In’t Hout on 30/09/2022.
},
	file = {Cockayne e.a. - 2019 - Bayesian Probabilistic Numerical Methods.pdf:C\:\\Users\\isido\\Zotero\\storage\\9ST6SIRW\\Cockayne e.a. - 2019 - Bayesian Probabilistic Numerical Methods.pdf:application/pdf},
}

@article{gondzio_interior_2012,
	title = {Interior point methods 25 years later},
	volume = {218},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221711008204},
	doi = {10.1016/j.ejor.2011.09.017},
	abstract = {Interior point methods for optimization have been around for more than 25 years now. Their presence has shaken up the ﬁeld of optimization. Interior point methods for linear and (convex) quadratic programming display several features which make them particularly attractive for very large scale optimization. Among the most impressive of them are their low-degree polynomial worst-case complexity and an unrivalled ability to deliver optimal solutions in an almost constant number of iterations which depends very little, if at all, on the problem dimension. Interior point methods are competitive when dealing with small problems of dimensions below one million constraints and variables and are beyond competition when applied to large problems of dimensions going into millions of constraints and variables.},
	language = {en},
	number = {3},
	urldate = {2022-10-27},
	journal = {European Journal of Operational Research},
	author = {Gondzio, Jacek},
	month = may,
	year = {2012},
	keywords = {optimization-constrained, interior point methods, optimization},
	pages = {587--601},
	file = {Gondzio - 2012 - Interior point methods 25 years later.pdf:C\:\\Users\\isido\\Zotero\\storage\\NNLGF4AU\\Gondzio - 2012 - Interior point methods 25 years later.pdf:application/pdf},
}

@misc{kitapbayev_closed_2021,
	title = {Closed form optimal exercise boundary of the {American} put option},
	url = {http://arxiv.org/abs/1912.05438},
	abstract = {We present three models of stock price with time-dependent interest rate, dividend yield, and volatility, respectively, that allow for explicit forms of the optimal exercise boundary of the finite maturity American put option. The optimal exercise boundary satisfies the nonlinear integral equation of Volterra type. We choose time-dependent parameters of the model so that the integral equation for the exercise boundary can be solved in the closed form. We also define the contracts of put type with time-dependent strike price that support the explicit optimal exercise boundary.},
	language = {en},
	urldate = {2022-12-03},
	publisher = {arXiv},
	author = {Kitapbayev, Yerkin},
	month = jan,
	year = {2021},
	note = {arXiv:1912.05438 [q-fin]},
	keywords = {Primary Primary 91G20, 60G40. Secondary 60J60, 35R35, 45G10, Quantitative Finance - Mathematical Finance, Quantitative Finance - Pricing of Securities, finance},
	file = {Kitapbayev - 2021 - Closed form optimal exercise boundary of the Ameri.pdf:C\:\\Users\\isido\\Zotero\\storage\\99M3PKZV\\Kitapbayev - 2021 - Closed form optimal exercise boundary of the Ameri.pdf:application/pdf},
}

@article{healy_pricing_2021,
	title = {Pricing {American} options under negative rates},
	issn = {14601559, 17552850},
	url = {http://arxiv.org/abs/2109.15157},
	doi = {10.21314/JCF.2021.004},
	abstract = {This paper starts by deﬁning the criteria where the early-exercise of an American option is never optimal, under positive, or negative rates. It follows with a short analysis of the various shapes of the exercise region under negative interest rates. It then presents a new integral equation, which establishes the option price, and the two early exercise boundaries, under negative rates. It shows how to solve this new equation, through modiﬁcations of the modern and efﬁcient algorithm of Andersen and Lake, from the initial guess of the two boundaries to more subtle changes required in their ﬁxed point method for stability. Finally, the performance and accuracy of the resulting algorithm is assessed against a cutting edge ﬁnite difference method implementation.},
	language = {en},
	urldate = {2022-12-03},
	journal = {The Journal of Computational Finance},
	author = {Healy, Jherek},
	year = {2021},
	note = {arXiv:2109.15157 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance, Quantitative Finance - Pricing of Securities, finance},
	file = {Healy - 2021 - Pricing American options under negative rates.pdf:C\:\\Users\\isido\\Zotero\\storage\\8RBI4CZR\\Healy - 2021 - Pricing American options under negative rates.pdf:application/pdf},
}

@article{bossy_parallel_2010,
	title = {Parallel {Pricing} {Algorithms} for {Multi}--{Dimensional} {Bermudan}/{American} {Options} using {Monte} {Carlo} methods},
	volume = {81},
	issn = {03784754},
	url = {http://arxiv.org/abs/0805.1827},
	doi = {10.1016/j.matcom.2010.08.005},
	abstract = {In this paper we present two parallel Monte Carlo based algorithms for pricing multi–dimensional Bermudan/American options. First approach relies on computation of the optimal exercise boundary while the second relies on classiﬁcation of continuation and exercise values. We also evaluate the performance of both the algorithms in a desktop grid environment. We show the eﬀectiveness of the proposed approaches in a heterogeneous computing environment, and identify scalability constraints due to the algorithmic structure.},
	language = {en},
	number = {3},
	urldate = {2022-12-03},
	journal = {Mathematics and Computers in Simulation},
	author = {Bossy, Mireille and Baude, Françoise and Doan, Viet Dung and Gaikwad, Abhijeet and Stokes-Rees, Ian},
	month = nov,
	year = {2010},
	note = {arXiv:0805.1827 [cs]},
	keywords = {monte carlo, Computer Science - Computational Engineering, Finance, and Science, Computer Science - Distributed, Parallel, and Cluster Computing, finance},
	pages = {568--577},
	file = {Bossy e.a. - 2010 - Parallel Pricing Algorithms for Multi--Dimensional.pdf:C\:\\Users\\isido\\Zotero\\storage\\R7AAXVAF\\Bossy e.a. - 2010 - Parallel Pricing Algorithms for Multi--Dimensional.pdf:application/pdf},
}

@misc{sevcovic_iterative_2007,
	title = {An iterative algorithm for evaluating approximations to the optimal exercise boundary for a nonlinear {Black}-{Scholes} equation},
	url = {http://arxiv.org/abs/0710.5301},
	abstract = {The purpose of this paper is to analyze and compute the early exercise boundary for a class of nonlinear Black–Scholes equations with a nonlinear volatility which can be a function of the second derivative of the option price itself. A motivation for studying the nonlinear Black–Scholes equation with a nonlinear volatility arises from option pricing models taking into account e.g. nontrivial transaction costs, investor’s preferences, feedback and illiquid markets effects and risk from a volatile (unprotected) portfolio. We present a new method how to transform the free boundary problem for the early exercise boundary position into a solution of a time depending nonlinear parabolic equation deﬁned on a ﬁxed domain. We furthermore propose an iterative numerical scheme that can be used to ﬁnd an approximation of the free boundary. We present results of numerical approximation of the early exercise boundary for various types of nonlinear Black–Scholes equations and we discuss dependence of the free boundary on various model parameters.},
	language = {en},
	urldate = {2022-12-03},
	publisher = {arXiv},
	author = {Sevcovic, Daniel},
	month = oct,
	year = {2007},
	note = {arXiv:0710.5301 [math, q-fin]},
	keywords = {Quantitative Finance - Computational Finance, Mathematics - Numerical Analysis, finance},
	annote = {Comment: 17 pages},
	file = {Sevcovic - 2007 - An iterative algorithm for evaluating approximatio.pdf:C\:\\Users\\isido\\Zotero\\storage\\QEPCMV5C\\Sevcovic - 2007 - An iterative algorithm for evaluating approximatio.pdf:application/pdf},
}

@misc{sorge_valuation_1998,
	title = {Valuation of path-dependent {American} options using a {Monte} {Carlo} approach},
	url = {http://arxiv.org/abs/math/9801057},
	abstract = {It is shown how to obtain accurate values for American options using Monte Carlo simulation. The main feature of the novel algorithm consists of tracking the boundary between exercise and hold regions via optimization of a certain payoﬀ function. We compare estimates from simulation for some types of claims with results from binomial tree calculations and ﬁnd very good agreement. The novel method allows to calculate so far untractable path-dependent option values.},
	language = {en},
	urldate = {2022-12-03},
	publisher = {arXiv},
	author = {Sorge, H.},
	month = jan,
	year = {1998},
	note = {arXiv:math/9801057},
	keywords = {Quantitative Finance - Computational Finance, Mathematics - Numerical Analysis, finance},
	annote = {Comment: 32 pages LaTeX including 4 postscript figures},
	file = {Sorge - 1998 - Valuation of path-dependent American options using.pdf:C\:\\Users\\isido\\Zotero\\storage\\TEBUQCAP\\Sorge - 1998 - Valuation of path-dependent American options using.pdf:application/pdf},
}

@misc{hout_numerical_2021,
	title = {Numerical valuation of {American} basket options via partial differential complementarity problems},
	url = {http://arxiv.org/abs/2106.01200},
	abstract = {We study the principal component analysis based approach introduced by Reisinger \& Wittum [1] and the comonotonic approach considered by Hanbali \& Linders [2] for the approximation of American basket option values via multidimensional partial diﬀerential complementarity problems (PDCPs). Both approximation approaches require the solution of just a limited number of low-dimensional PDCPs. It is demonstrated by ample numerical experiments that they deﬁne approximations that lie close to each other. Next, an eﬃcient discretisation of the pertinent PDCPs is presented that leads to a favourable convergence behaviour.},
	language = {en},
	urldate = {2022-12-03},
	publisher = {arXiv},
	author = {Hout, Karel in 't and Snoeijer, Jacob},
	month = jun,
	year = {2021},
	note = {arXiv:2106.01200 [cs, math, q-fin]},
	keywords = {Quantitative Finance - Computational Finance, Mathematics - Numerical Analysis, Computer Science - Computational Engineering, Finance, and Science, finance},
	file = {Hout en Snoeijer - 2021 - Numerical valuation of American basket options via.pdf:C\:\\Users\\isido\\Zotero\\storage\\VYEP4C9L\\Hout en Snoeijer - 2021 - Numerical valuation of American basket options via.pdf:application/pdf},
}

@misc{noauthor_free_2022,
	title = {Free boundary problem},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Free_boundary_problem&oldid=1117659232},
	abstract = {In mathematics, a free boundary problem (FB problem) is a partial differential equation to be solved for both an unknown function 
  
    
      
        u
      
    
    \{{\textbackslash}displaystyle u\}
   and an unknown domain 
  
    
      
        Ω
      
    
    \{{\textbackslash}displaystyle {\textbackslash}Omega \}
  . The segment 
  
    
      
        Γ
      
    
    \{{\textbackslash}displaystyle {\textbackslash}Gamma \}
   of the boundary of 
  
    
      
        Ω
      
    
    \{{\textbackslash}displaystyle {\textbackslash}Omega \}
   which is not known at the outset of the problem is the free boundary.
FBs arise in various mathematical models encompassing applications that ranges from physical to economical, financial and biological phenomena, where there is an extra effect of the medium. This effect is in general a qualitative change of the medium and hence an appearance of a phase transition: ice to water, liquid to crystal, buying to selling (assets), active to inactive (biology), blue to red (coloring games), disorganized to organized (self-organizing criticality). An interesting aspect of such a criticality is the so-called sandpile dynamic (or Internal DLA).
The most classical example is the melting of ice: Given a block of ice, one can solve the heat equation given appropriate initial and boundary conditions to determine its temperature. But, if in any region the temperature is greater than the melting point of ice, this domain will be occupied by liquid water instead. The boundary formed from the ice/liquid interface is controlled dynamically by the solution of the PDE.},
	language = {en},
	urldate = {2022-12-03},
	journal = {Wikipedia},
	month = oct,
	year = {2022},
	note = {Page Version ID: 1117659232},
	keywords = {finance},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\T8LYY8CI\\Free_boundary_problem.html:text/html},
}

@misc{herrera_parallel_2014,
	title = {Parallel {American} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1404.1180},
	abstract = {In this paper we introduce a new algorithm for American Monte Carlo that can be used either for American-style options, callable structured products or for computing counterparty credit risk (e.g. CVA or PFE computation). Leveraging least squares regressions, the main novel feature of our algorithm is that it can be fully parallelized. Moreover, there is no need to store the paths and the payoﬀ computation can be done forwards: this allows to price structured products with complex path and exercise dependencies. The key idea of our algorithm is to split the set of paths in several subsets which are used iteratively. We give the convergence rate of the algorithm. We illustrate our method on an American put option and compare the results with the Longstaﬀ-Schwartz algorithm.},
	language = {en},
	urldate = {2022-12-03},
	publisher = {arXiv},
	author = {Herrera, Calypso and Paulot, Louis},
	month = apr,
	year = {2014},
	note = {arXiv:1404.1180 [q-fin]},
	keywords = {monte carlo, Quantitative Finance - Computational Finance, Quantitative Finance - Pricing of Securities, finance},
	annote = {Comment: 36 pages},
	file = {Herrera en Paulot - 2014 - Parallel American Monte Carlo.pdf:C\:\\Users\\isido\\Zotero\\storage\\MKELBXNC\\Herrera en Paulot - 2014 - Parallel American Monte Carlo.pdf:application/pdf},
}

@incollection{garcke_sparse_2012,
	address = {Berlin, Heidelberg},
	title = {Sparse {Grids} in a {Nutshell}},
	volume = {88},
	isbn = {978-3-642-31702-6 978-3-642-31703-3},
	url = {http://link.springer.com/10.1007/978-3-642-31703-3_3},
	abstract = {The technique of sparse grids allows to overcome the curse of dimensionality, which prevents the use of classical numerical discretization schemes in more than three or four dimensions, under suitable regularity assumptions. The approach is obtained from a multi-scale basis by a tensor product construction and subsequent truncation of the resulting multiresolution series expansion. This entry level article gives an introduction to sparse grids and the sparse grid combination technique.},
	language = {en},
	urldate = {2022-12-04},
	booktitle = {Sparse {Grids} and {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Garcke, Jochen},
	editor = {Garcke, Jochen and Griebel, Michael},
	year = {2012},
	doi = {10.1007/978-3-642-31703-3_3},
	note = {Series Title: Lecture Notes in Computational Science and Engineering},
	pages = {57--80},
	file = {Garcke - 2012 - Sparse Grids in a Nutshell.pdf:C\:\\Users\\isido\\Zotero\\storage\\UN7T7FRH\\Garcke - 2012 - Sparse Grids in a Nutshell.pdf:application/pdf},
}

@misc{noauthor_parareal_2022,
	title = {Parareal},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Parareal&oldid=1117710630},
	abstract = {Parareal is a parallel algorithm from numerical analysis and used for the solution of initial value problems.
It was introduced in 2001 by Lions, Maday and Turinici. Since then, it has become one of the most widely studied parallel-in-time integration methods.},
	language = {en},
	urldate = {2022-12-04},
	journal = {Wikipedia},
	month = oct,
	year = {2022},
	note = {Page Version ID: 1117710630},
	keywords = {ODE},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\RCGZQ9J3\\Parareal.html:text/html},
}

@misc{noauthor_multilevel_2022,
	title = {Multilevel {Monte} {Carlo} method (wikipedia)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Multilevel_Monte_Carlo_method&oldid=1070124752},
	abstract = {Multilevel Monte Carlo (MLMC) methods in numerical analysis are algorithms for computing expectations that arise in stochastic simulations. Just as Monte Carlo methods, they rely on repeated random sampling, but these samples are taken on different levels of accuracy. MLMC methods can greatly reduce the computational cost of standard Monte Carlo methods by taking most samples with a low accuracy and corresponding low cost, and only very few samples are taken at high accuracy and corresponding high cost.},
	language = {en},
	urldate = {2022-12-04},
	journal = {Wikipedia},
	month = feb,
	year = {2022},
	note = {Page Version ID: 1070124752},
	keywords = {monte carlo},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\NSTZZI8D\\Multilevel_Monte_Carlo_method.html:text/html},
}

@article{cliffe_multilevel_2011,
	title = {Multilevel {Monte} {Carlo} methods and applications to elliptic {PDEs} with random coefficients},
	volume = {14},
	issn = {1432-9360, 1433-0369},
	url = {http://link.springer.com/10.1007/s00791-011-0160-x},
	doi = {10.1007/s00791-011-0160-x},
	abstract = {We consider the numerical solution of elliptic partial differential equations with random coefﬁcients. Such problems arise, for example, in uncertainty quantiﬁcation for groundwater ﬂow. We describe a novel variance reduction technique for the standard Monte Carlo method, called the multilevel Monte Carlo method, and demonstrate numerically its superiority. The asymptotic cost of solving the stochastic problem with the multilevel method is always significantly lower than that of the standard method and grows only proportionally to the cost of solving the deterministic problem in certain circumstances. Numerical calculations demonstrating the effectiveness of the method for one- and two-dimensional model problems arising in groundwater ﬂow are presented.},
	language = {en},
	number = {1},
	urldate = {2022-12-04},
	journal = {Computing and Visualization in Science},
	author = {Cliffe, K. A. and Giles, M. B. and Scheichl, R. and Teckentrup, A. L.},
	month = jan,
	year = {2011},
	keywords = {monte carlo},
	pages = {3--15},
	file = {Cliffe e.a. - 2011 - Multilevel Monte Carlo methods and applications to.pdf:C\:\\Users\\isido\\Zotero\\storage\\EM2WHUW8\\Cliffe e.a. - 2011 - Multilevel Monte Carlo methods and applications to.pdf:application/pdf},
}

@misc{giles_multilevel_2013,
	title = {Multilevel {Monte} {Carlo} methods},
	url = {http://arxiv.org/abs/1304.5472},
	abstract = {The author’s presentation of multilevel Monte Carlo path simulation at the MCQMC 2006 conference stimulated a lot of research into multilevel Monte Carlo methods. This paper reviews the progress since then, emphasising the simplicity, ﬂexibility and generality of the multilevel Monte Carlo approach. It also offers a few original ideas and suggests areas for future research.},
	language = {en},
	urldate = {2022-12-04},
	publisher = {arXiv},
	author = {Giles, Michael B.},
	month = apr,
	year = {2013},
	note = {arXiv:1304.5472 [math]},
	keywords = {monte carlo, Mathematics - Numerical Analysis},
	file = {Giles - 2013 - Multilevel Monte Carlo methods.pdf:C\:\\Users\\isido\\Zotero\\storage\\YRKQ5XCV\\Giles - 2013 - Multilevel Monte Carlo methods.pdf:application/pdf},
}

@book{oksendal_stochastic_1995,
	address = {Berlin, Heidelberg},
	series = {Universitext},
	title = {Stochastic {Differential} {Equations}},
	isbn = {978-3-540-60243-9 978-3-662-03185-8},
	url = {http://link.springer.com/10.1007/978-3-662-03185-8},
	language = {en},
	urldate = {2022-12-04},
	publisher = {Springer Berlin Heidelberg},
	author = {Øksendal, Bernt},
	year = {1995},
	doi = {10.1007/978-3-662-03185-8},
	file = {Øksendal - 1995 - Stochastic Differential Equations.pdf:C\:\\Users\\isido\\Zotero\\storage\\NN2LBDZP\\Øksendal - 1995 - Stochastic Differential Equations.pdf:application/pdf},
}

@article{zhu_calculating_2007,
	title = {{CALCULATING} {THE} {EARLY} {EXERCISE} {BOUNDARY} {OF} {AMERICAN} {PUT} {OPTIONS} {WITH} {AN} {APPROXIMATION} {FORMULA}},
	volume = {10},
	issn = {0219-0249, 1793-6322},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0219024907004615},
	doi = {10.1142/S0219024907004615},
	abstract = {In this paper, an algorithm to improve the computational accuracy of the analytical approximation to the value of American put options and their optimal exercise boundary proposed by Zhu (2004) is presented. In the current approach, Zhu’s simple approximation formula is used as an initial guess for the optimal exercise boundary of American put options. The determination of an improved optimal exercise boundary is then achieved by setting a null value of the Theta of option value on the optimal exercise boundary. Test example results show that the improvement is indeed signiﬁcant.},
	language = {en},
	number = {07},
	urldate = {2022-12-04},
	journal = {International Journal of Theoretical and Applied Finance},
	author = {Zhu, Song-Ping and He, Zhi-Wei},
	month = nov,
	year = {2007},
	keywords = {finance},
	pages = {1203--1227},
	file = {Zhu en He - 2007 - CALCULATING THE EARLY EXERCISE BOUNDARY OF AMERICA.pdf:C\:\\Users\\isido\\Zotero\\storage\\2ZSHAVUG\\Zhu en He - 2007 - CALCULATING THE EARLY EXERCISE BOUNDARY OF AMERICA.pdf:application/pdf},
}

@article{bormetti_pricing_2006,
	title = {Pricing exotic options in a path integral approach},
	volume = {6},
	issn = {1469-7688, 1469-7696},
	url = {http://www.tandfonline.com/doi/abs/10.1080/14697680500510878},
	doi = {10.1080/14697680500510878},
	abstract = {In the framework of Black-Scholes-Merton model of ﬁnancial derivatives, a path integral approach to option pricing is presented. A general formula to price European path dependent options on multidimensional assets is obtained and implemented by means of various ﬂexible and eﬃcient algorithms. As an example, we detail the cases of Asian, barrier knock out, reverse cliquet and basket call options, evaluating prices and Greeks. The numerical results are compared with those obtained with other procedures used in quantitative ﬁnance and found to be in good agreement. In particular, when pricing at-the-money and out-of-the-money options, the path integral approach exhibits competitive performances.},
	language = {en},
	number = {1},
	urldate = {2022-12-04},
	journal = {Quantitative Finance},
	author = {Bormetti, G. and Montagna, G. and Moreni, N. and Nicrosini, O.},
	month = feb,
	year = {2006},
	keywords = {finance},
	pages = {55--66},
	file = {Bormetti e.a. - 2006 - Pricing exotic options in a path integral approach.pdf:C\:\\Users\\isido\\Zotero\\storage\\5RXL3P32\\Bormetti e.a. - 2006 - Pricing exotic options in a path integral approach.pdf:application/pdf},
}

@article{capuozzo_path_2021,
	title = {Path integral {Monte} {Carlo} method for option pricing},
	volume = {581},
	issn = {03784371},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378437121005045},
	doi = {10.1016/j.physa.2021.126231},
	abstract = {The Markov chain Monte Carlo (MCMC) method, in conjunction with the Metropolis–Hastings algorithm, is used to simulate the path integral for the Black–Scholes–Merton model of option pricing. After a brief derivation of the path integral solution of this model, we develop the MCMC method by discretizing the path integral on a time lattice and evaluating this discretized form for various scenarios. Particular attention is paid to the existence of autocorrelations, their decay with the number of sweeps, and the resulting estimate of the corresponding errors. After testing our approach against closed-form solutions, we demonstrate the utility and flexibility of our method with applications to non-Gaussian models.},
	language = {en},
	urldate = {2022-12-05},
	journal = {Physica A: Statistical Mechanics and its Applications},
	author = {Capuozzo, Pietro and Panella, Emanuele and Schettini Gherardini, Tancredi and Vvedensky, Dimitri D.},
	month = nov,
	year = {2021},
	keywords = {monte carlo, finance},
	pages = {126231},
	file = {Capuozzo e.a. - 2021 - Path integral Monte Carlo method for option pricin.pdf:C\:\\Users\\isido\\Zotero\\storage\\4543G9SW\\Capuozzo e.a. - 2021 - Path integral Monte Carlo method for option pricin.pdf:application/pdf},
}

@article{devreese_path_2010,
	title = {Path integral approach to {Asian} options in the {Black}-{Scholes} model},
	volume = {389},
	issn = {03784371},
	url = {http://arxiv.org/abs/0906.4456},
	doi = {10.1016/j.physa.2009.10.020},
	abstract = {We derive a closed-form solution for the price of an average price as well as an average strike geometric Asian option, by making use of the path integral formulation. Our results are compared to a numerical Monte Carlo simulation. We also develop a pricing formula for an Asian option with a barrier on a control process, combining the method of images with a partitioning of the set of paths according to the average along the path. This formula is exact when the correlation is zero, and is approximate when the correlation increases.},
	language = {en},
	number = {4},
	urldate = {2022-12-05},
	journal = {Physica A: Statistical Mechanics and its Applications},
	author = {Devreese, Jeroen P. A. and Lemmens, Damiaan and Tempere, Jacques},
	month = feb,
	year = {2010},
	note = {arXiv:0906.4456 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance, Quantitative Finance - Pricing of Securities, finance},
	pages = {780--788},
	annote = {Comment: 13 pages, 3 figures, updated version has added references to path integral literature},
	file = {Devreese e.a. - 2010 - Path integral approach to Asian options in the Bla.pdf:C\:\\Users\\isido\\Zotero\\storage\\TRU3Y9RR\\Devreese e.a. - 2010 - Path integral approach to Asian options in the Bla.pdf:application/pdf},
}

@misc{xia_multilevel_2011,
	title = {Multilevel {Monte} {Carlo} method for jump-diffusion {SDEs}},
	url = {http://arxiv.org/abs/1106.4730},
	abstract = {We investigate the extension of the multilevel Monte Carlo path simulation method to jump-diﬀusion SDEs. We consider models with ﬁnite rate activity , using a jump-adapted discretisation in which the jump times are computed and added to the standard uniform discretisation times. The key component in multilevel analysis is the calculation of an expected payoﬀ diﬀerence between a coarse path simulation and a ﬁne path simulation with twice as many timesteps. If the Poisson jump rate is constant, the jump times are the same on both paths and the multilevel extension is relatively straightforward, but the implementation is more complex in the case of state-dependent jump rates for which the jump times naturally diﬀer.},
	language = {en},
	urldate = {2022-12-05},
	publisher = {arXiv},
	author = {Xia, Yuan},
	month = jun,
	year = {2011},
	note = {arXiv:1106.4730 [q-fin]},
	keywords = {monte carlo, Quantitative Finance - Computational Finance},
	annote = {Comment: 36 pages, 10 figures},
	file = {Xia - 2011 - Multilevel Monte Carlo method for jump-diffusion S.pdf:C\:\\Users\\isido\\Zotero\\storage\\YGXG767H\\Xia - 2011 - Multilevel Monte Carlo method for jump-diffusion S.pdf:application/pdf},
}

@article{casella_exact_2011,
	title = {Exact {Simulation} of {Jump}-{Diffusion} {Processes} with {Monte} {Carlo} {Applications}},
	volume = {13},
	issn = {1387-5841, 1573-7713},
	url = {http://link.springer.com/10.1007/s11009-009-9163-1},
	doi = {10.1007/s11009-009-9163-1},
	abstract = {We introduce a novel algorithm (JEA) to simulate exactly from a class of one-dimensional jump-diffusion processes with state-dependent intensity. The simulation of the continuous component builds on the recent Exact Algorithm ((1)). The simulation of the jump component instead employes a thinning algorithm with stochastic acceptance probabilities in the spirit of (14). In turn JEA allows unbiased Monte Carlo simulation of a wide class of functionals of the process’ trajectory, including discrete averages, max/min, crossing events, hitting times. Our numerical experiments show that the method outperforms Monte Carlo methods based on the Euler discretization.},
	language = {en},
	number = {3},
	urldate = {2022-12-05},
	journal = {Methodology and Computing in Applied Probability},
	author = {Casella, Bruno and Roberts, Gareth O.},
	month = sep,
	year = {2011},
	keywords = {first passage},
	pages = {449--473},
	file = {Casella en Roberts - 2011 - Exact Simulation of Jump-Diffusion Processes with .pdf:C\:\\Users\\isido\\Zotero\\storage\\6PDHDU6J\\Casella en Roberts - 2011 - Exact Simulation of Jump-Diffusion Processes with .pdf:application/pdf},
}

@inproceedings{binder_complexity_2009,
	title = {The complexity of simulating {Brownian} {Motion}},
	isbn = {978-0-89871-680-1 978-1-61197-306-8},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611973068.7},
	doi = {10.1137/1.9781611973068.7},
	abstract = {We analyze the complexity of the Walk on Spheres algorithm for simulating Brownian Motion in a domain Ω ⊂ Rd. The algorithm, which was ﬁrst proposed in the 1950s, produces samples from the hitting probability distribution of the Brownian Motion process on ∂Ω within an error of ε. The algorithm is used as a building block for solving a variety of diﬀerential equations, including the Dirichlet Problem.},
	language = {en},
	urldate = {2022-12-07},
	booktitle = {Proceedings of the {Twentieth} {Annual} {ACM}-{SIAM} {Symposium} on {Discrete} {Algorithms}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Binder, Ilia and Braverman, Mark},
	month = jan,
	year = {2009},
	keywords = {first passage},
	pages = {58--67},
	file = {Binder en Braverman - 2009 - The complexity of simulating Brownian Motion.pdf:C\:\\Users\\isido\\Zotero\\storage\\9LATAZYY\\Binder en Braverman - 2009 - The complexity of simulating Brownian Motion.pdf:application/pdf},
}

@misc{azzone_fast_2022,
	title = {A fast {Monte} {Carlo} scheme for additive processes and option pricing},
	url = {http://arxiv.org/abs/2112.08291},
	abstract = {In this paper, we present a very fast Monte Carlo scheme for additive processes: the computational time is of the same order of magnitude of standard algorithms for Brownian motions. We analyze in detail numerical error sources and propose a technique that reduces the two major sources of error. We also compare our results with a benchmark method: the jump simulation with Gaussian approximation.},
	language = {en},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Azzone, Michele and Baviera, Roberto},
	month = nov,
	year = {2022},
	note = {arXiv:2112.08291 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance},
	file = {Azzone en Baviera - 2022 - A fast Monte Carlo scheme for additive processes a.pdf:C\:\\Users\\isido\\Zotero\\storage\\NKJIF6K7\\Azzone en Baviera - 2022 - A fast Monte Carlo scheme for additive processes a.pdf:application/pdf},
}

@misc{herrmann_exact_2021,
	title = {Exact simulation of the first passage time through a given level for jump diffusions},
	url = {http://arxiv.org/abs/2106.05560},
	abstract = {Continuous-time stochastic processes play an important role in the description of random phenomena, it is therefore of prime interest to study particular variables depending on their paths, like stopping time for example. One approach consists in pointing out explicit expressions of the probability distributions, an other approach is rather based on the numerical generation of the random variables. We propose an algorithm in order to generate the ﬁrst passage time through a given level of a one-dimensional jump diﬀusion. This process satisﬁes a stochastic diﬀerential equation driven by a Brownian motion and subject to random shocks characterized by an independent Poisson process. Our algorithm belongs to the family of rejection sampling procedures, also called exact simulation in this context: the outcome of the algorithm and the stopping time under consideration are identically distributed. It is based on both the exact simulation of the diﬀusion at a given time and on the exact simulation of ﬁrst passage time for continuous diﬀusions. It is therefore based on an extension of the algorithm introduced by Herrmann and Zucca [16] in the continuous framework. The challenge here is to generate the exact position of a continuous diﬀusion conditionally to the fact that the given level has not been reached before. We present the construction of the algorithm and give numerical illustrations, conditions on the recurrence of jump diﬀusions are also discussed.},
	language = {en},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Herrmann, Samuel and Massin, Nicolas},
	month = jun,
	year = {2021},
	note = {arXiv:2106.05560 [math]},
	keywords = {Mathematics - Probability, first passage},
	file = {Herrmann en Massin - 2021 - Exact simulation of the first passage time through.pdf:C\:\\Users\\isido\\Zotero\\storage\\DBB4B6I2\\Herrmann en Massin - 2021 - Exact simulation of the first passage time through.pdf:application/pdf},
}

@misc{llorente_survey_2021,
	title = {A survey of {Monte} {Carlo} methods for noisy and costly densities with application to reinforcement learning},
	url = {http://arxiv.org/abs/2108.00490},
	abstract = {This survey gives an overview of Monte Carlo methodologies using surrogate models, for dealing with densities which are intractable, costly, and/or noisy. This type of problem can be found in numerous real-world scenarios, including stochastic optimization and reinforcement learning, where each evaluation of a density function may incur some computationally-expensive or even physical (real-world activity) cost, likely to give diﬀerent results each time. The surrogate model does not incur this cost, but there are important trade-oﬀs and considerations involved in the choice and design of such methodologies. We classify the diﬀerent methodologies into three main classes and describe speciﬁc instances of algorithms under a uniﬁed notation. A modular scheme which encompasses the considered methods is also presented. A range of application scenarios is discussed, with special attention to the likelihood-free setting and reinforcement learning. Several numerical comparisons are also provided.},
	language = {en},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Llorente, F. and Martino, L. and Read, J. and Delgado, D.},
	month = sep,
	year = {2021},
	note = {arXiv:2108.00490 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	file = {Llorente e.a. - 2021 - A survey of Monte Carlo methods for noisy and cost.pdf:C\:\\Users\\isido\\Zotero\\storage\\ZXRFKLSV\\Llorente e.a. - 2021 - A survey of Monte Carlo methods for noisy and cost.pdf:application/pdf},
}

@article{browne_survey_2012,
	title = {A {Survey} of {Monte} {Carlo} {Tree} {Search} {Methods}},
	volume = {4},
	issn = {1943-068X, 1943-0698},
	url = {http://ieeexplore.ieee.org/document/6145622/},
	doi = {10.1109/TCIAIG.2012.2186810},
	abstract = {Monte Carlo Tree Search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difﬁcult problem of computer Go, but has also proved beneﬁcial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the ﬁrst ﬁve years of MCTS research. We outline the core algorithm’s derivation, impart some structure on the many variations and enhancements that have been proposed, and summarise the results from the key game and non-game domains to which MCTS methods have been applied. A number of open research questions indicate that the ﬁeld is ripe for future work.},
	language = {en},
	number = {1},
	urldate = {2022-12-07},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
	month = mar,
	year = {2012},
	pages = {1--43},
	file = {Browne e.a. - 2012 - A Survey of Monte Carlo Tree Search Methods.pdf:C\:\\Users\\isido\\Zotero\\storage\\E9YVZ4L9\\Browne e.a. - 2012 - A Survey of Monte Carlo Tree Search Methods.pdf:application/pdf},
}

@article{luengo_survey_2020,
	title = {A {Survey} of {Monte} {Carlo} {Methods} for {Parameter} {Estimation}},
	volume = {2020},
	issn = {1687-6180},
	url = {http://arxiv.org/abs/2107.11820},
	doi = {10.1186/s13634-020-00675-6},
	abstract = {Statistical signal processing applications usually require the estimation of some parameters of interest given a set of observed data. These estimates are typically obtained either by solving a multi-variate optimization problem, as in the maximum likelihood (ML) or maximum a posteriori (MAP) estimators, or by performing a multi-dimensional integration, as in the minimum mean squared error (MMSE) estimators. Unfortunately, analytical expressions for these estimators cannot be found in most real-world applications, and the Monte Carlo (MC) methodology is one feasible approach. MC methods proceed by drawing random samples, either from the desired distribution or from a simpler one, and using them to compute consistent estimators. The most important families of MC algorithms are Markov chain MC (MCMC) and importance sampling (IS). On the one hand, MCMC methods draw samples from a proposal density, building then an ergodic Markov chain whose stationary distribution is the desired distribution by accepting or rejecting those candidate samples as the new state of the chain. On the other hand, IS techniques draw samples from a simple proposal density, and then assign them suitable weights that measure their quality in some appropriate way. In this paper, we perform a thorough review of MC methods for the estimation of static parameters in signal processing applications. A historical note on the development of MC schemes is also provided, followed by the basic MC method and a brief description of the rejection sampling (RS) algorithm, as well as three sections describing many of the most relevant MCMC and IS algorithms, and their combined use. Finally, ﬁve numerical examples (including the estimation of the parameters of a chaotic system, a localization problem in wireless sensor networks and a spectral analysis application) are provided in order to demonstrate the performance of the described approaches.},
	language = {en},
	number = {1},
	urldate = {2022-12-07},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Luengo, D. and Martino, L. and Bugallo, M. and Elvira, V. and S ärkkä, S.},
	month = dec,
	year = {2020},
	note = {arXiv:2107.11820 [cs, eess, math, stat]},
	keywords = {Mathematics - Numerical Analysis, Statistics - Computation, Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Signal Processing},
	pages = {25},
	file = {Luengo e.a. - 2020 - A Survey of Monte Carlo Methods for Parameter Esti.pdf:C\:\\Users\\isido\\Zotero\\storage\\AW7RXXX8\\Luengo e.a. - 2020 - A Survey of Monte Carlo Methods for Parameter Esti.pdf:application/pdf},
}

@incollection{devroye_exact_2010,
	address = {Heidelberg},
	title = {On {Exact} {Simulation} {Algorithms} for {Some} {Distributions} {Related} to {Brownian} {Motion} and {Brownian} {Meanders}},
	isbn = {978-3-7908-2597-8 978-3-7908-2598-5},
	url = {http://link.springer.com/10.1007/978-3-7908-2598-5_1},
	abstract = {We survey and develop exact random variate generators for several distributions related to Brownian motion, Brownian bridge, Brownian excursion, Brownian meander, and related restricted Brownian motion processes. Various parameters such as maxima and ﬁrst passage times are dealt with at length. We are particularly interested in simulating process variables in expected time uniformly bounded over all parameters.},
	language = {en},
	urldate = {2022-12-08},
	booktitle = {Recent {Developments} in {Applied} {Probability} and {Statistics}},
	publisher = {Physica-Verlag HD},
	author = {Devroye, Luc},
	editor = {Devroye, Luc and Karasözen, Bülent and Kohler, Michael and Korn, Ralf},
	year = {2010},
	doi = {10.1007/978-3-7908-2598-5_1},
	keywords = {first passage},
	pages = {1--35},
	file = {Devroye - 2010 - On Exact Simulation Algorithms for Some Distributi.pdf:C\:\\Users\\isido\\Zotero\\storage\\IKVR3JRG\\Devroye - 2010 - On Exact Simulation Algorithms for Some Distributi.pdf:application/pdf},
}

@misc{dieker_euler_2017,
	title = {On the {Euler} discretization error of {Brownian} motion about random times},
	url = {http://arxiv.org/abs/1708.04356},
	abstract = {In this paper we derive weak limits for the discretization errors of sampling barrierhitting and extreme events of Brownian motion by using the Euler discretization simulation method. Speciﬁcally, we consider the Euler discretization approximation of Brownian motion to sample barrier-hitting events, i.e. hitting for the ﬁrst time a deterministic “barrier” function; and to sample extreme events, i.e. attaining a minimum on a given compact time interval or unbounded closed time interval. For each case we study the discretization error between the actual time the event occurs versus the time the event occurs for the discretized path, and also the discretization error on the position of the Brownian motion at these times. We show limits in distribution for the discretization errors normalized by their convergence rate, and give closed-form analytic expressions for the limiting random variables. Additionally, we use these limits to study the asymptotic behaviour of Gaussian random walks in the following situations: (1.) the overshoot of a Gaussian walk above a barrier that goes to inﬁnity; (2.) the minimum of a Gaussian walk compared to the minimum of the Brownian motion obtained when interpolating the Gaussian walk with Brownian bridges, both up to the same time horizon that goes to inﬁnity; and (3.) the global minimum of a Gaussian walk compared to the global minimum of the Brownian motion obtained when interpolating the Gaussian walk with Brownian bridges, when both have the same positive drift decreasing to zero. In deriving these limits in distribution we provide a uniﬁed framew√ork to understand the relation between several papers where the constant −ζ(1/2)/ 2π has appeared, where ζ is the Riemann zeta function. In particular, we show that this constant is the mean of some of the limiting distributions we derive.},
	language = {en},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Dieker, A. B. and Lagos, Guido},
	month = aug,
	year = {2017},
	note = {arXiv:1708.04356 [math]},
	keywords = {Mathematics - Probability, first passage},
	file = {Dieker en Lagos - 2017 - On the Euler discretization error of Brownian moti.pdf:C\:\\Users\\isido\\Zotero\\storage\\BPSGSE6D\\Dieker en Lagos - 2017 - On the Euler discretization error of Brownian moti.pdf:application/pdf},
}

@article{xu_distribution_2013,
	title = {On the {Distribution} of {First} {Exit} {Time} for {Brownian} {Motion} with {Double} {Linear} {Time}-{Dependent} {Barriers}},
	volume = {2013},
	issn = {2090-5572},
	url = {https://www.hindawi.com/journals/isrn/2013/865347/},
	doi = {10.1155/2013/865347},
	abstract = {This paper focuses on the first exit time for a Brownian motion with a double linear time-dependent barrier specified by. We are concerned in this paper with the distribution of the Brownian motion hitting the upper barrier before hitting the lower linear barrier. The main method we applied here is the Girsanov transform formula. As a result, we expressed the density of such exit time in terms of a finite series. This result principally provides us an analytical expression for the distribution of the aforementioned exit time and  an easy way to compute the distribution of first exit time numerically.},
	language = {en},
	urldate = {2022-12-08},
	journal = {ISRN Applied Mathematics},
	author = {Xu, Lin and Zhu, Dongjin},
	month = sep,
	year = {2013},
	keywords = {first passage},
	pages = {1--5},
	file = {Xu en Zhu - 2013 - On the Distribution of First Exit Time for Brownia.pdf:C\:\\Users\\isido\\Zotero\\storage\\R3QCYSHV\\Xu en Zhu - 2013 - On the Distribution of First Exit Time for Brownia.pdf:application/pdf},
}

@article{beskos_varepsilon-strong_2012,
	title = {\${\textbackslash}varepsilon\$-{Strong} simulation of the {Brownian} path},
	volume = {18},
	issn = {1350-7265},
	url = {https://projecteuclid.org/journals/bernoulli/volume-18/issue-4/varepsilon-Strong-simulation-of-the-Brownian-path/10.3150/11-BEJ383.full},
	doi = {10.3150/11-BEJ383},
	abstract = {We present an iterative sampling method which delivers upper and lower bounding processes for the Brownian path. We develop such processes with particular emphasis on being able to unbiasedly simulate them on a personal computer. The dominating processes converge almost surely in the supremum and L1 norms. In particular, the rate of converge in L1 is of the order O(K−1/2), K denoting the computing cost. The a.s. enfolding of the Brownian path can be exploited in Monte Carlo applications involving Brownian paths whence our algorithm (termed the ε-strong algorithm) can deliver unbiased Monte Carlo estimators over path expectations, overcoming discretisation errors characterising standard approaches. We will show analytical results from applications of the ε-strong algorithm for estimating expectations arising in option pricing. We will also illustrate that individual steps of the algorithm can be of separate interest, giving new simulation methods for interesting Brownian distributions.},
	language = {en},
	number = {4},
	urldate = {2022-12-08},
	journal = {Bernoulli},
	author = {Beskos, Alexandros and Peluchetti, Stefano and Roberts, Gareth},
	month = nov,
	year = {2012},
	keywords = {first passage},
	file = {Beskos e.a. - 2012 - \$varepsilon\$-Strong simulation of the Brownian pa.pdf:C\:\\Users\\isido\\Zotero\\storage\\G8BZTKXZ\\Beskos e.a. - 2012 - \$varepsilon\$-Strong simulation of the Brownian pa.pdf:application/pdf},
}

@article{metwally_using_2002,
	title = {Using {Brownian} {Bridge} for {Fast} {Simulation} of {Jump}-{Diffusion} {Processes} and {Barrier} {Options}},
	volume = {10},
	issn = {1074-1240, 2168-8524},
	url = {http://jod.pm-research.com/lookup/doi/10.3905/jod.2002.319189},
	doi = {10.3905/jod.2002.319189},
	abstract = {Barrier options are one of the most popular derivatives in the financial markets. The authors present a fast and unbiased Monte Carlo approach to pricing barrier options when the underlying security follows a simple jump-diffusion process with constant parameters and a continuously monitored barrier. Two algorithms are based on the Brownian bridge concept. The first one is based on a sampling approach to evaluate an integral that results from application of the Brownian bridge. The second approach approximates that integral using a Taylor series expansion. Both methods significantly reduce bias and speed convergence compared to the standard Monte Carlo simulation approach. For example, the first method achieves zero bias. In addition, it is about 100 times faster than the conventional Monte Carlo method that achieves acceptable bias. In developing the second algorithm, the authors derive a novel approach for obtaining a first-passage time density integral using a Taylor series expansion. This approach is potentially useful in other applications, where the expectation of some function over the first-passage time distribution needs to be derived.},
	language = {en},
	number = {1},
	urldate = {2022-12-08},
	journal = {The Journal of Derivatives},
	author = {Metwally, Steve A.K. and Atiya, Amir F.},
	month = aug,
	year = {2002},
	keywords = {first passage},
	pages = {43--54},
	file = {Metwally en Atiya - 2002 - Using Brownian Bridge for Fast Simulation of Jump-.pdf:C\:\\Users\\isido\\Zotero\\storage\\ABLIPAX8\\Metwally en Atiya - 2002 - Using Brownian Bridge for Fast Simulation of Jump-.pdf:application/pdf},
}

@misc{noauthor_reflection_2022,
	title = {Reflection principle ({Wiener} process)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Reflection_principle_(Wiener_process)&oldid=1111342822},
	abstract = {In the theory of probability for stochastic processes, the reflection principle for a Wiener process states that if the path of a Wiener process f(t)  reaches a value f(s) = a at time t = s,  then the subsequent path after time s has the same distribution as the reflection of the subsequent path about the value a. More formally, the reflection principle refers to a lemma concerning the distribution of the supremum of the Wiener process, or Brownian motion. The result relates the distribution of the supremum of Brownian motion up to time t to the distribution of the process at time t. It is a corollary of the strong Markov property of Brownian motion.},
	language = {en},
	urldate = {2022-12-08},
	journal = {Wikipedia},
	month = sep,
	year = {2022},
	note = {Page Version ID: 1111342822},
	keywords = {first passage},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\LHGS64H3\\Reflection_principle_(Wiener_process).html:text/html},
}

@article{deaconu_random_2006,
	title = {A {Random} {Walk} on {Rectangles} {Algorithm}},
	volume = {8},
	issn = {1387-5841, 1573-7713},
	url = {http://link.springer.com/10.1007/s11009-006-7292-3},
	doi = {10.1007/s11009-006-7292-3},
	abstract = {In this article, we introduce an algorithm that simulates eﬃciently the ﬁrst exit time and position from a rectangle (or a parallelepiped) for a Brownian motion that starts at any point inside. This method provides an exact way to simulate the ﬁrst exit time and position from any polygonal domain and then to solve some Dirichlet problems, whatever the dimension. This method can be used as a replacement or complement of the method of the random walk on spheres and can be easily adapted to deal with Neumann boundary conditions or Brownian motion with a constant drift.},
	language = {en},
	number = {1},
	urldate = {2022-12-13},
	journal = {Methodology and Computing in Applied Probability},
	author = {Deaconu, Madalina and Lejay, Antoine},
	month = mar,
	year = {2006},
	keywords = {monte carlo, PDE, walk on spheres},
	pages = {135--151},
	file = {Deaconu en Lejay - 2006 - A Random Walk on Rectangles Algorithm.pdf:C\:\\Users\\isido\\Zotero\\storage\\LETY9EC9\\Deaconu en Lejay - 2006 - A Random Walk on Rectangles Algorithm.pdf:application/pdf},
}

@article{hwang_off-centered_2015,
	title = {Off-centered “{Walk}-on-{Spheres}” ({WOS}) algorithm},
	volume = {303},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999115006646},
	doi = {10.1016/j.jcp.2015.10.002},
	abstract = {The “Walk-on-Spheres” (WOS) algorithm has played the central role in simulating the diffusion process in Diffusion Monte Carlo methods. In this paper, based on the isomorphism between the electrostatic Poisson problem and the corresponding diffusion motion expectation of the first-passage, we develop an off-centered WOS algorithm to replace the old WOS one. We find that the new off-centered WOS algorithm is much more efficient than the old one.},
	language = {en},
	urldate = {2022-12-13},
	journal = {Journal of Computational Physics},
	author = {Hwang, Chi-Ok and Hong, Sungpyo and Kim, Jinwoo},
	month = dec,
	year = {2015},
	keywords = {monte carlo, walk on spheres},
	pages = {331--335},
	file = {Hwang e.a. - 2015 - Off-centered “Walk-on-Spheres” (WOS) algorithm.pdf:C\:\\Users\\isido\\Zotero\\storage\\3YHCI5ZF\\Hwang e.a. - 2015 - Off-centered “Walk-on-Spheres” (WOS) algorithm.pdf:application/pdf},
}

@misc{yilmazer_solving_2022,
	title = {Solving {Inverse} {PDE} {Problems} using {Grid}-{Free} {Monte} {Carlo} {Estimators}},
	url = {http://arxiv.org/abs/2208.02114},
	abstract = {Modeling physical phenomena like heat transport and diffusion is crucially dependent on the numerical solution of partial differential equations (PDEs). A PDE solver finds the solution given coefficients and a boundary condition, whereas an inverse PDE solver goes the opposite way and reconstructs these inputs from an existing solution. In this article, we investigate techniques for solving inverse PDE problems using a gradient-based methodology. Conventional PDE solvers based on the finite element method require a domain meshing step that can be fragile and costly. Grid-free Monte Carlo methods instead stochastically sample paths using variations of the walk on spheres algorithm to construct an unbiased estimator of the solution. The uncanny similarity of these methods to physically-based rendering algorithms has been observed by several recent works. In the area of rendering, recent progress has led to the development of efficient unbiased derivative estimators. They solve an adjoint form of the problem and exploit arithmetic invertibility to compute gradients using a constant amount of memory and linear time complexity. Could these two lines of work be combined to compute cheap parametric derivatives of a grid-free PDE solver? We investigate this question and present preliminary results. CCS Concepts: • Mathematics of computing → Partial differential equations; • Computing methodologies → Rendering.},
	language = {en},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Yılmazer, Ekrem Fatih and Vicini, Delio and Jakob, Wenzel},
	month = aug,
	year = {2022},
	note = {arXiv:2208.02114 [cs, math]},
	keywords = {monte carlo, walk on spheres, Computer Science - Graphics, Mathematics - Analysis of PDEs, rendering, inverse problem},
	annote = {Comment: 9 pages (2 pages references and appendix), 9 figures},
	file = {Yılmazer e.a. - 2022 - Solving Inverse PDE Problems using Grid-Free Monte.pdf:C\:\\Users\\isido\\Zotero\\storage\\RQAZ53HP\\Yılmazer e.a. - 2022 - Solving Inverse PDE Problems using Grid-Free Monte.pdf:application/pdf},
}

@article{kosztin_introduction_1996,
	title = {Introduction to the diffusion {Monte} {Carlo} method},
	volume = {64},
	issn = {0002-9505, 1943-2909},
	url = {http://aapt.scitation.org/doi/10.1119/1.18168},
	doi = {10.1119/1.18168},
	abstract = {A self–contained and tutorial presentation of the diffusion Monte Carlo method for determining the ground state energy and wave function of quantum systems is provided. First, the theoretical basis of the method is derived and then a numerical algorithm is formulated. The algorithm is applied to determine the ground state of the harmonic oscillator, the Morse oscillator, the hydrogen atom, and the electronic ground state of the H+ 2 ion and of the H2 molecule. A computer program on which the sample calculations are based is available upon request.},
	language = {en},
	number = {5},
	urldate = {2022-12-13},
	journal = {American Journal of Physics},
	author = {Kosztin, Ioan and Faber, Byron and Schulten, Klaus},
	month = may,
	year = {1996},
	keywords = {walk on spheres},
	pages = {633--644},
	file = {Kosztin e.a. - 1996 - Introduction to the diffusion Monte Carlo method.pdf:C\:\\Users\\isido\\Zotero\\storage\\UJQSNZJ3\\Kosztin e.a. - 1996 - Introduction to the diffusion Monte Carlo method.pdf:application/pdf},
}

@article{hwang_analysis_2003,
	title = {Analysis and comparison of {Green}’s function first-passage algorithms with “{Walk} on {Spheres}” algorithms},
	volume = {63},
	issn = {03784754},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378475403000910},
	doi = {10.1016/S0378-4754(03)00091-0},
	abstract = {We analyze the optimization of the running times of Green’s function ﬁrst-passage (GFFP) algorithms. The running times for these new ﬁrst-passage (FP) algorithms [1–4], which use exact Green’s functions for the Laplacian to eliminate the absorption layer in the “walk on spheres” (WOS) method [5–9], are compared with those for WOS algorithms. It has been empirically observed that GFFP algorithms are more eﬃcient than WOS algorithms when high accuracy is required [2–4]. Additionally, it has been observed that there is always an optimal distance from the surface of the absorbing boundary, δI , for a GFFP algorithm within which a FP surface can be permitted to intersect the boundary [2–4]. In this paper, we will provide a rigorous complexity analysis consistent with these observations. This analysis is based on estimating the numbers of WOS and GFFP steps needed for absorption on the boundary, and the complexity and running times of each WOS and GFFP step. As an illustration, we analyze the running times for calculating the capacitance of the unit cube using both GFFP and WOS.},
	language = {en},
	number = {6},
	urldate = {2022-12-18},
	journal = {Mathematics and Computers in Simulation},
	author = {Hwang, Chi-Ok and Mascagni, Michael},
	month = nov,
	year = {2003},
	keywords = {walk on spheres, green function},
	pages = {605--613},
	file = {Hwang en Mascagni - 2003 - Analysis and comparison of Green’s function first-.pdf:C\:\\Users\\isido\\Zotero\\storage\\HBPK37PS\\Hwang en Mascagni - 2003 - Analysis and comparison of Green’s function first-.pdf:application/pdf},
}

@article{hwang_simulationtabulation_2001,
	title = {The {Simulation}–{Tabulation} {Method} for {Classical} {Diffusion} {Monte} {Carlo}},
	volume = {174},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999101969475},
	doi = {10.1006/jcph.2001.6947},
	abstract = {Many important classes of problems in materials science and biotechnology require the solution of the Laplace or Poisson equation in disordered two-phase domains in which the phase interface is extensive and convoluted. Green’s function first-passage (GFFP) methods solve such problems efficiently by generalizing the “walk on spheres” (WOS) method to allow first-passage (FP) domains to be not just spheres but a wide variety of geometrical shapes. (In particular, this solves the difficulty of slow convergence with WOS by allowing FP domains that contain patches of the phase interface.) Previous studies accomplished this by using geometries for which the Green’s function was available in quasi-analytic form. Here, we extend these studies by using the simulation–tabulation (ST) method. We simulate and then tabulate surface Green’s functions that cannot be obtained analytically. The ST method is applied to the Solc–Stockmayer model with zero potential, to the mean trapping rate of a diffusing particle in a domain of nonoverlapping spherical traps, and to the effective conductivity for perfectly insulating, nonoverlapping spherical inclusions in a matrix of finite conductivity. In all cases, this class of algorithms provides the most efficient methods known to solve these problems to high accuracy.},
	language = {en},
	number = {2},
	urldate = {2022-12-18},
	journal = {Journal of Computational Physics},
	author = {Hwang, Chi-Ok and Given, James A. and Mascagni, Michael},
	month = dec,
	year = {2001},
	keywords = {green function, first passage},
	pages = {925--946},
	file = {Hwang e.a. - 2001 - The Simulation–Tabulation Method for Classical Dif.pdf:C\:\\Users\\isido\\Zotero\\storage\\BTSXYC8S\\Hwang e.a. - 2001 - The Simulation–Tabulation Method for Classical Dif.pdf:application/pdf},
}

@article{milstein_simulation_1999,
	title = {Simulation of a space-time bounded diffusion},
	volume = {9},
	issn = {1050-5164},
	url = {https://projecteuclid.org/journals/annals-of-applied-probability/volume-9/issue-3/Simulation-of-a-space-time-bounded-diffusion/10.1214/aoap/1029962812.full},
	doi = {10.1214/aoap/1029962812},
	abstract = {Mean-square approximations, which ensure boundedness of both time and space increments, are constructed for stochastic differential equations in a bounded domain. The proposed algorithms are based on a space–time discretization using a random walk over boundaries of small space–time parallelepipeds. To realize the algorithms, exact distributions for exit points of the space–time Brownian motion from a space–time parallelepiped are given. Convergence theorems are stated for the proposed algorithms. A method of approximate searching for exit points of the space–time diffusion from the bounded domain is constructed. Results of several numerical tests are presented.},
	language = {en},
	number = {3},
	urldate = {2022-12-20},
	journal = {The Annals of Applied Probability},
	author = {Milstein, G. N. and Tretyakov, M. V.},
	month = aug,
	year = {1999},
	keywords = {first passage},
	file = {Milstein en Tretyakov - 1999 - Simulation of a space-time bounded diffusion.pdf:C\:\\Users\\isido\\Zotero\\storage\\3A6SZGK6\\Milstein en Tretyakov - 1999 - Simulation of a space-time bounded diffusion.pdf:application/pdf},
}

@article{allab_first-passage_2016,
	title = {First-passage {Time} {Estimation} of {Diffusion} {Processes} through {Time}-{Varying} {Boundaries} with an {Application} in {Finance}},
	volume = {6},
	issn = {1927-7040, 1927-7032},
	url = {http://www.ccsenet.org/journal/index.php/ijsp/article/view/63088},
	doi = {10.5539/ijsp.v6n1p59},
	abstract = {In this paper, we develop a Monte Carlo based algorithm for estimating the FPT (ﬁrst passage time) density of the solution of a one-dimensional time-homogeneous SDE (stochastic diﬀerential equation) through a time-dependent frontier. We consider Brownian bridges as well as local Daniels curve approximations to obtain tractable estimations of the FPT probability between successive points of a simulated path of the process. Under mild assumptions, a (unique) Daniels curve local approximation can easily be obtained by explicitly solving a non-linear system of equations.},
	language = {en},
	number = {1},
	urldate = {2022-12-20},
	journal = {International Journal of Statistics and Probability},
	author = {Allab, Imene and Watier, Francois},
	month = dec,
	year = {2016},
	keywords = {first passage},
	pages = {59},
	file = {Allab en Watier - 2016 - First-passage Time Estimation of Diffusion Process.pdf:C\:\\Users\\isido\\Zotero\\storage\\8DTMPTXR\\Allab en Watier - 2016 - First-passage Time Estimation of Diffusion Process.pdf:application/pdf},
}

@article{herrmann_first-passage_2016,
	title = {The first-passage time of the {Brownian} motion to a curved boundary: an algorithmic approach},
	volume = {38},
	issn = {1064-8275, 1095-7197},
	shorttitle = {The first-passage time of the {Brownian} motion to a curved boundary},
	url = {http://arxiv.org/abs/1501.07060},
	doi = {10.1137/151006172},
	abstract = {Under some weak conditions, the ﬁrst-passage time of the Brownian motion to a continuous curved boundary is an almost surely ﬁnite stopping time. Its probability density function (pdf) is explicitly known only in few particular cases. Several mathematical studies proposed to approximate the pdf in a quite general framework or even to simulate this hitting time using a discrete time approximation of the Brownian motion. The authors study a new algorithm which permits to simulate the ﬁrst-passage time using an iterating procedure. The convergence rate presented in this paper suggests that the method is very eﬃcient.},
	language = {en},
	number = {1},
	urldate = {2022-12-20},
	journal = {SIAM Journal on Scientific Computing},
	author = {Herrmann, Samuel and Tanré, Etienne},
	month = jan,
	year = {2016},
	note = {arXiv:1501.07060 [math]},
	keywords = {Mathematics - Probability, first passage},
	pages = {A196--A215},
	file = {Herrmann en Tanré - 2016 - The first-passage time of the Brownian motion to a.pdf:C\:\\Users\\isido\\Zotero\\storage\\5NRAE68H\\Herrmann en Tanré - 2016 - The first-passage time of the Brownian motion to a.pdf:application/pdf},
}

@article{lejay_new_2013,
	title = {New {Monte} {Carlo} schemes for simulating diffusions in discontinuous media},
	volume = {245},
	issn = {03770427},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377042712005444},
	doi = {10.1016/j.cam.2012.12.013},
	abstract = {We introduce new Monte Carlo simulation schemes for diﬀusions in a discontinuous media divided in subdomains with piecewise constant diﬀusivity. These schemes are higher order extensions of the usual schemes and take into account the two dimensional aspects of the diﬀusion at the interface between subdomains. This is achieved using either stochastic processes techniques or an approach based on ﬁnite diﬀerences. Numerical tests on elliptic, parabolic and eigenvalue problems involving an operator in divergence form show the eﬃciency of these new schemes.},
	language = {en},
	urldate = {2022-12-20},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Lejay, Antoine and Maire, Sylvain},
	month = jun,
	year = {2013},
	keywords = {first passage},
	pages = {97--116},
	file = {Lejay en Maire - 2013 - New Monte Carlo schemes for simulating diffusions .pdf:C\:\\Users\\isido\\Zotero\\storage\\U84MVIQQ\\Lejay en Maire - 2013 - New Monte Carlo schemes for simulating diffusions .pdf:application/pdf},
}

@misc{yang_walk_2015,
	title = {Walk on {Spheres} {Algorithm} for {Helmholtz} and {Yukawa} {Equations} via {Duffin} {Correspondence}},
	url = {http://arxiv.org/abs/1512.07725},
	abstract = {We show that a constant-potential time-independent Schro¨dinger equation with Dirichlet boundary data can be reformulated as a Laplace equation with Dirichlet boundary data. With this reformulation, which we call the Duﬃn correspondence, we provide a classical Walk On Spheres (WOS) algorithm for Monte Carlo simulation of the solutions of the boundary value problem. We compare the obtained Duﬃn WOS algorithm with existing modiﬁed WOS algorithms.},
	language = {en},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Yang, Xuxin and Rasila, Antti and Sottinen, Tommi},
	month = dec,
	year = {2015},
	note = {arXiv:1512.07725 [math]},
	keywords = {monte carlo, Mathematics - Probability, PDE, walk on spheres, Mathematics - Numerical Analysis, Mathematics - Analysis of PDEs},
	annote = {Comment: 14 pages, 5 figures},
	file = {Yang e.a. - 2015 - Walk on Spheres Algorithm for Helmholtz and Yukawa.pdf:C\:\\Users\\isido\\Zotero\\storage\\3XFZ3E64\\Yang e.a. - 2015 - Walk on Spheres Algorithm for Helmholtz and Yukawa.pdf:application/pdf},
}

@misc{noauthor_volterra_2022,
	title = {Volterra integral equation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Volterra_integral_equation&oldid=1125547908},
	abstract = {In mathematics, the Volterra integral equations are a special type of integral equations.[1] They are divided into two groups referred to as the first and the second kind.

A linear Volterra equation of the first kind is

    f ( t ) = ∫ a t K ( t , s ) x ( s ) d s f(t)={\textbackslash}int \_\{a\}{\textasciicircum}\{t\}K(t,s){\textbackslash},x(s){\textbackslash},ds

where f is a given function and x is an unknown function to be solved for. A linear Volterra equation of the second kind is

    x ( t ) = f ( t ) + ∫ a t K ( t , s ) x ( s ) d s . x(t)=f(t)+{\textbackslash}int \_\{a\}{\textasciicircum}\{t\}K(t,s)x(s){\textbackslash},ds.

In operator theory, and in Fredholm theory, the corresponding operators are called Volterra operators. A useful method to solve such equations, the Adomian decomposition method, is due to George Adomian.

A linear Volterra integral equation is a convolution equation if

    x ( t ) = f ( t ) + ∫ t 0 t K ( t − s ) x ( s ) d s . x(t)=f(t)+{\textbackslash}int \_\{\{t\_\{0\}\}\}{\textasciicircum}\{t\}K(t-s)x(s){\textbackslash},ds.

The function K K in the integral is called the kernel. Such equations can be analyzed and solved by means of Laplace transform techniques.

For a weakly singular kernel of the form K ( t , s ) = ( t 2 − s 2 ) − α \{{\textbackslash}displaystyle K(t,s)=(t{\textasciicircum}\{2\}-s{\textasciicircum}\{2\}){\textasciicircum}\{-{\textbackslash}alpha \}\} with 0 {\textless} α {\textless} 1 0{\textless}{\textbackslash}alpha{\textless}1, Volterra integral equation of the first kind can conveniently be transformed into a classical Abel integral equation.

The Volterra integral equations were introduced by Vito Volterra and then studied by Traian Lalescu in his 1908 thesis, Sur les équations de Volterra, written under the direction of Émile Picard. In 1911, Lalescu wrote the first book ever on integral equations.

Volterra integral equations find application in demography as Lotka's integral equation,[2] the study of viscoelastic materials, in actuarial science through the renewal equation,[3] and in fluid mechanics to describe the flow behavior near finite-sized boundaries.[4][5]},
	language = {en},
	urldate = {2022-12-20},
	journal = {Wikipedia},
	month = dec,
	year = {2022},
	note = {Page Version ID: 1125547908},
	keywords = {integral equations},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\BXA6JUFN\\Volterra_integral_equation.html:text/html},
}

@misc{herrmann_exact_2017,
	title = {Exact simulation of the first-passage time of diffusions},
	url = {http://arxiv.org/abs/1705.06881},
	abstract = {Since diﬀusion processes arise in so many diﬀerent ﬁelds, eﬃcient technics for the simulation of sample paths, like discretization schemes, represent crucial tools in applied probability. Such methods permit to obtain approximations of the ﬁrst-passage times as a by-product. For eﬃciency reasons, it is particularly challenging to simulate directly this hitting time by avoiding to construct the whole paths. In the Brownian case, the distribution of the ﬁrst-passage time is explicitly known and can be easily used for simulation purposes. The authors introduce a new rejection sampling algorithm which permits to perform an exact simulation of the ﬁrst-passage time for general one-dimensional diﬀusion processes. The efﬁciency of the method, which is essentially based on Girsanov’s transformation, is described through theoretical results and numerical examples.},
	language = {en},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Herrmann, Samuel and Zucca, Cristina},
	month = may,
	year = {2017},
	note = {arXiv:1705.06881 [math]},
	keywords = {Mathematics - Probability, first passage},
	file = {Herrmann en Zucca - 2017 - Exact simulation of the first-passage time of diff.pdf:C\:\\Users\\isido\\Zotero\\storage\\UZ8C3VE3\\Herrmann en Zucca - 2017 - Exact simulation of the first-passage time of diff.pdf:application/pdf},
}

@article{drabeck_monte_nodate,
	title = {Monte {Carlo} {Simulation} of {Boundary} {Crossing} {Probabilities} for a {Brownian} {Motion} and {Curved} {Boundaries}},
	abstract = {We are concerned with the probability that a standard Brownian motion W (t) crosses a curved boundary c(t) on a  nite interval [0, T ]. Let this probability be denoted by Q(c(t); T ). Except for linear functions c(t) and a few other special cases no explicit, analytic formula for Q(c(t); T ) is available. Thus numerical methods need to be applied for general boundaries to obtain approximate solutions. Some authors use for example integral equations. However, most of these numerical methods are either intractable or di cult to asses in terms of their accuracy. Due to recent advances in research another way of estimating Q(c(t); T ) seems feasible: Monte Carlo Simulation. Wang and Pötzelberger (1997) derived an explicit formula for the boundary crossing probability of piecewise linear functions which has the form of an expectation. Based on this formula we proceed as follows: First we approximate the general boundary c(t) by a piecewise linear function cm(t) on a uniform partition 0 = t0 {\textless} t1... {\textless} tm = T . Then we simulate Brownian sample paths in order to evaluate the expectation in the formula of the authors for cm(t). The bias resulting when estimating Q(cm(t); T ) rather than Q(c(t); T ) can be bounded by a formula of Borovkov and Novikov (2005). Whereas the bias decreases at a rate of O(1/m2) for a partition rank m, the standard error due to Monte Carlo simulation only decays at a rate of O(1/√n), where n is the number of simulation cycles. Hence the standard deviation   or the variance respectively   is the main limiting factor when increasing the accuracy. The main goal of this dissertation is to  nd and evaluate variance reducing techniques in order to enhance the quality of the Monte Carlo estimator for Q(c(t); T ). Among the techniques we discuss are: • Antithetic Sampling, • Strati ed Sampling, • Importance Sampling, • Control Variates, • Transforming the original problem. We analyze each of these techniques thoroughly from a theoretical point of view. Further, we test each technique empirically through simulation experiments on several carefully chosen boundaries. In order to asses our results we set them in relation to a previously established benchmark. We are interested in the relative reduction in the mean squared error (= sum of the squared bias and variance) due to a given technique, where the computational e ort remains constant. As a result of this dissertation we derive some very potent techniques that yield a substantial improvement in terms of accuracy. We even discuss an approach that improves the rate at which our biased Monte Carlo estimator converges to the correct result.},
	language = {en},
	author = {Drabeck, Florian},
	keywords = {first passage},
	file = {Drabeck - Monte Carlo Simulation of Boundary Crossing Probab.pdf:C\:\\Users\\isido\\Zotero\\storage\\36MEIWXP\\Drabeck - Monte Carlo Simulation of Boundary Crossing Probab.pdf:application/pdf},
}

@article{jin_first_2017,
	title = {First {Passage} {Time} for {Brownian} {Motion} and {Piecewise} {Linear} {Boundaries}},
	volume = {19},
	issn = {1387-5841, 1573-7713},
	url = {http://link.springer.com/10.1007/s11009-015-9475-2},
	doi = {10.1007/s11009-015-9475-2},
	abstract = {We propose a new approach to calculating the first passage time densities for Brownian motion crossing piecewise linear boundaries which can be discontinuous. Using this approach we obtain explicit formulas for the first passage densities and show that they are continuously differentiable except at the break points of the boundaries. Furthermore, these formulas can be used to approximate the first passage time distributions for general nonlinear boundaries. The numerical computation can be easily done by using the Monte Carlo integration, which is straightforward to implement. Some numerical examples are presented for illustration. This approach can be further extended to compute two-sided boundary crossing distributions.},
	language = {en},
	number = {1},
	urldate = {2022-12-20},
	journal = {Methodology and Computing in Applied Probability},
	author = {Jin, Zhiyong and Wang, Liqun},
	month = mar,
	year = {2017},
	keywords = {first passage},
	pages = {237--253},
	file = {Jin en Wang - 2017 - First Passage Time for Brownian Motion and Piecewi.pdf:C\:\\Users\\isido\\Zotero\\storage\\BNBKMEFU\\Jin en Wang - 2017 - First Passage Time for Brownian Motion and Piecewi.pdf:application/pdf},
}

@article{sabelfeld_application_2018,
	title = {Application of the von {Mises}–{Fisher} distribution to {Random} {Walk} on {Spheres} method for solving high-dimensional diffusion–advection–reaction equations},
	volume = {138},
	issn = {01677152},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167715218301160},
	doi = {10.1016/j.spl.2018.03.002},
	abstract = {We suggest a new efficient and reliable random walk method, continuous both in space and time, for solving high-dimensional diffusion–advection–reaction equations. It is based on a discovered intrinsic relation between the von Mises–Fisher distribution on a sphere with this type of equations. It can be formulated as follows: the von Mises–Fisher distribution uniquely defines the solution of a diffusion–advection equation in any bounded or unbounded domain if the relevant boundary value problem for this equation satisfies regular existence and uniqueness conditions. Both two- and three-dimensional transient equations are included in our considerations. The accuracy and the cost of the suggested random walk on spheres method are estimated.},
	language = {en},
	urldate = {2022-12-20},
	journal = {Statistics \& Probability Letters},
	author = {Sabelfeld, Karl K.},
	month = jul,
	year = {2018},
	keywords = {first passage},
	pages = {137--142},
	file = {Sabelfeld - 2018 - Application of the von Mises–Fisher distribution t.pdf:C\:\\Users\\isido\\Zotero\\storage\\E78HU5DQ\\Sabelfeld - 2018 - Application of the von Mises–Fisher distribution t.pdf:application/pdf},
}

@article{qi_bidirectional_2022,
	title = {A bidirectional formulation for {Walk} on {Spheres}},
	volume = {41},
	issn = {0167-7055, 1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14586},
	doi = {10.1111/cgf.14586},
	abstract = {Numerically solving partial differential equations (PDEs) is central to many applications in computer graphics and scientific modeling. Conventional methods for solving PDEs often need to discretize the space first, making them less efficient for complex geometry. Unlike conventional methods, the walk on spheres (WoS) algorithm recently introduced to graphics is a grid-free Monte Carlo method that can provide numerical solutions of Poisson equations without discretizing space. We draw analogies between WoS and classical rendering algorithms, and find that the WoS algorithm is conceptually equivalent to forward path tracing. Inspired by similar approaches in light transport, we propose a novel WoS reformulation that operates in the reverse direction, starting at source points and estimating the Green’s function at “sensor” points. Implementations of this algorithm show improvement over classical WoS in solving Poisson equation with sparse sources. Our approach opens exciting avenues for future algorithms for PDE estimation which, analogous to light transport, connect WoS walks starting from sensors and sources and combine different strategies for robust solution algorithms in all cases.},
	language = {en},
	number = {4},
	urldate = {2022-12-22},
	journal = {Computer Graphics Forum},
	author = {Qi, Yang and Seyb, Dario and Bitterli, Benedikt and Jarosz, Wojciech},
	month = jul,
	year = {2022},
	keywords = {monte carlo, PDE, walk on spheres},
	pages = {51--62},
	file = {Qi e.a. - 2022 - A bidirectional formulation for Walk on Spheres.pdf:C\:\\Users\\isido\\Zotero\\storage\\Y6LCPRIN\\Qi e.a. - 2022 - A bidirectional formulation for Walk on Spheres.pdf:application/pdf},
}

@article{rioux-lavoie_monte_2022,
	title = {A {Monte} {Carlo} {Method} for {Fluid} {Simulation}},
	volume = {41},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3550454.3555450},
	doi = {10.1145/3550454.3555450},
	abstract = {We present a novel Monte Carlo-based fluid simulation approach capable of pointwise and stochastic estimation of fluid motion. Drawing on the Feynman-Kac representation of the vorticity transport equation, we propose a recursive Monte Carlo estimator of the Biot-Savart law and extend it with a stream function formulation that allows us to treat free-slip boundary conditions using a Walk-on-Spheres algorithm. Inspired by the Monte Carlo literature in rendering, we design and compare variance reduction schemes suited to a fluid simulation context for the first time, show its applicability to complex boundary settings, and detail a simple and practical implementation with temporal grid caching. We validate the correctness of our approach via quantitative and qualitative evaluations – across a range of settings and domain geometries – and thoroughly explore its parameters’ design space. Finally, we provide an in-depth discussion of several axes of future work building on this new numerical simulation modality. CCS Concepts: • Mathematics of computing → Probabilistic algorithms; Partial differential equations; • Computing methodologies → Modeling and simulation.},
	language = {en},
	number = {6},
	urldate = {2022-12-22},
	journal = {ACM Transactions on Graphics},
	author = {Rioux-Lavoie, Damien and Sugimoto, Ryusuke and Özdemir, Tümay and Shimada, Naoharu H. and Batty, Christopher and Nowrouzezahrai, Derek and Hachisuka, Toshiya},
	month = dec,
	year = {2022},
	keywords = {monte carlo, PDE, walk on spheres},
	pages = {1--16},
	file = {Rioux-Lavoie e.a. - 2022 - A Monte Carlo Method for Fluid Simulation.pdf:C\:\\Users\\isido\\Zotero\\storage\\76CW9YCY\\Rioux-Lavoie e.a. - 2022 - A Monte Carlo Method for Fluid Simulation.pdf:application/pdf},
}

@article{sabelfeld_random_2019,
	title = {Random walk on rectangles and parallelepipeds algorithm for solving transient anisotropic drift-diffusion-reaction problems},
	volume = {25},
	doi = {10.1515/mcma-2019-2039},
	abstract = {In this paper a random walk on arbitrary rectangles (2D) and parallelepipeds (3D) algorithm is developed for solving transient anisotropic drift-diffusion-reaction equations. The method is meshless, both in space and time. The approach is based on a rigorous representation of the first passage time and exit point distributions for arbitrary rectangles and parallelepipeds. The probabilistic representation is then transformed to a form convenient for stochastic simulation. The method can be used to calculate fluxes to any desired part of the boundary, from arbitrary sources. A global version of the method we call here as a stochastic expansion from cell to cell (SECC) algorithm for calculating the whole solution field is suggested. Application of this method to solve a system of transport equations for electrons and holes in a semicoductor is discussed. This system consists of the continuity equations for particle densities and a Poisson equation for electrostatic potential. To validate the method we have derived a series of exact solutions of the drift-diffusion-reaction problem in a three-dimensional layer presented in the last section in details.},
	journal = {Monte Carlo Methods and Applications},
	author = {Sabelfeld, Karl},
	month = may,
	year = {2019},
	keywords = {monte carlo, PDE, walk on spheres},
}

@article{sabelfeld_random_2017,
	title = {Random walk on spheres algorithm for solving transient drift-diffusion-reaction problems},
	volume = {23},
	issn = {1569-3961},
	url = {https://www.degruyter.com/document/doi/10.1515/mcma-2017-0113/html},
	doi = {10.1515/mcma-2017-0113},
	abstract = {We suggest in this paper a Random Walk on Spheres (RWS) method for solving transient drift-diffusion-reaction problems which is an extension of our algorithm we developed recently [26] for solving steady-state drift-diffusion problems. Both two-dimensional and three-dimensional problems are solved. Survival probability, first passage time and the exit position for a sphere (disc) of the drift-diffusion-reaction process are explicitly derived from a generalized spherical integral relation we prove both for two-dimensional and three-dimensional problems. The distribution of the exit position on the sphere has the form of the von Mises–Fisher distribution which can be simulated efficiently. Rigorous expressions are derived in the case of constant velocity drift, but the algorithm is then extended to solve drift-diffusion-reaction problems with arbitrary varying drift velocity vector. The method can efficiently be applied to calculate the fluxes of the solution to any part of the boundary. This can be done by applying a reciprocity theorem which we prove here for the drift-diffusion-reaction problems with general boundary conditions. Applications of this approach to methods of cathodoluminescence (CL) and electron beam induced current (EBIC) imaging of defects and dislocations in semiconductors are presented.},
	language = {en},
	number = {3},
	urldate = {2022-12-22},
	journal = {Monte Carlo Methods and Applications},
	author = {Sabelfeld, Karl K.},
	month = sep,
	year = {2017},
	note = {Publisher: De Gruyter},
	keywords = {walk on spheres, Drift-diffusion-reaction equation, reciprocity relation, Robin boundary conditions, spherical integral relation, von Mises–Fisher distribution},
	pages = {189--212},
}

@article{veach_robust_1997,
	title = {Robust {Monte} {Carlo} {Methods} for {Light} {Transport} {Simulation}. {Ph}.{D}. {Dissertation}. {Stanford} {University}.},
	abstract = {Light transport algorithms generate realistic images by simulating the emission and scattering of light in an artificial environment. Applications include lighting design, architecture, and computer animation, while related engineering disciplines include neutron transport and radiative heat transfer. The main challenge with these algorithms is the high complexity of the geometric, scattering, and illumination models that are typically used. In this dissertation, we develop new Monte Carlo techniques that greatly extend the range of input models for which light transport simulations are practical. Our contributions include new theoretical models, statistical methods, and rendering algorithms. We start by developing a rigorous theoretical basis for bidirectional light transport algorithms (those that combine direct and adjoint techniques). First, we propose a linear operator formulation that does not depend on any assumptions about the physical validity of the input scene. We show how to obtain mathematically correct results using a variety of bidirectional techniques. Next we derive a different formulation, such that for any physically valid input scene, the transport operators are symmetric. This symmetry is important for both theory and implementations, and is based on a new reciprocity condition that we derive for transmissive materials. Finally, we show how light transport can be formulated as an integral over a space of paths. This framework allows new sampling and integration techniques to be applied, such as the Metropolis sampling algorithm. We also use this model to investigate the limitations of unbiased Monte Carlo methods, and to show that certain kinds of paths cannot be sampled. Our statistical contributions include a new technique called multiple importance sampling, which can greatly increase the robustness of Monte Carlo integration. It uses more than one sampling technique to evaluate an integral, and then combines these samples in a vi

way that is provably close to optimal. This leads to estimators that have low variance for a broad class of integrands. We also describe a new variance reduction technique called efficiency-optimized Russian roulette. Finally, we link these ideas together to obtain new Monte Carlo light transport algorithms. Bidirectional path tracing uses a family of different path sampling techniques that generate some path vertices starting from a light source, and some starting from a sensor. We show that when these techniques are combined using multiple importance sampling, a large range of difficult lighting effects can be handled efficiently. The algorithm is unbiased, handles arbitrary geometry and materials, and is relatively simple to implement. The second algorithm we describe is Metropolis light transport, inspired by the Metropolis sampling method from computational physics. Paths are generated by following a random walk through path space, such that the probability density of visiting each path is proportional to the contribution it makes to the ideal image. The resulting algorithm is unbiased, uses little storage, handles arbitrary geometry and materials, and can be orders of magnitude more efficient than previous unbiased approaches. It performs especially well on problems that are usually considered difficult, e.g. those involving bright indirect light, small geometric holes, or glossy surfaces. To our knowledge, this is the first application of the Metropolis method to transport problems of any kind.},
	language = {en},
	journal = {Robust Monte Carlo Methods for Light Transport Simulation.},
	author = {Veach, Eric},
	year = {1997},
	keywords = {monte carlo, rendering, importance sampling},
	file = {Veach - 1997 - Robust Monte Carlo Methods for Light Transport Sim.pdf:C\:\\Users\\isido\\Zotero\\storage\\HDZ56Y9U\\Veach - 1997 - Robust Monte Carlo Methods for Light Transport Sim.pdf:application/pdf},
}

@article{novak_monte_2018,
	title = {Monte {Carlo} {Methods} for {Volumetric} {Light} {Transport} {Simulation}},
	volume = {37},
	issn = {01677055},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.13383},
	doi = {10.1111/cgf.13383},
	abstract = {The wide adoption of path-tracing algorithms in high-end realistic rendering has stimulated many diverse research initiatives. In this paper we present a coherent survey of methods that utilize Monte Carlo integration for estimating light transport in scenes containing participating media. Our work complements the volume-rendering state-of-the-art report by Cerezo et al. [CPP∗05]; we review publications accumulated since its publication over a decade ago, and include earlier methods that are key for building light transport paths in a stochastic manner. We begin by describing analog and non-analog procedures for freepath sampling and discuss various expected-value, collision, and track-length estimators for computing transmittance. We then review the various rendering algorithms that employ these as building blocks for path sampling. Special attention is devoted to null-collision methods that utilize ﬁctitious matter to handle spatially varying densities; we import two “next-ﬂight” estimators originally developed in nuclear sciences. Whenever possible, we draw connections between image-synthesis techniques and methods from particle physics and neutron transport to provide the reader with a broader context.},
	language = {en},
	number = {2},
	urldate = {2022-12-23},
	journal = {Computer Graphics Forum},
	author = {Novák, Jan and Georgiev, Iliyan and Hanika, Johannes and Jarosz, Wojciech},
	month = may,
	year = {2018},
	keywords = {rendering},
	pages = {551--576},
	file = {Novák e.a. - 2018 - Monte Carlo Methods for Volumetric Light Transport.pdf:C\:\\Users\\isido\\Zotero\\storage\\8Y3IJRVP\\Novák e.a. - 2018 - Monte Carlo Methods for Volumetric Light Transport.pdf:application/pdf},
}

@article{delaurentis_monte_1990,
	title = {A {Monte} {Carlo} method for poisson's equation},
	volume = {90},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/002199919090199B},
	doi = {10.1016/0021-9991(90)90199-B},
	abstract = {This investigation presents an analysis of a Monte Carlo method for estimating local solutions to the Dirichlet problem for Poisson’s equation. The probabilistic algorithm consists of a modified “walk on spheres” that includes the effects from internal sources as part of the random process. A new derivation of the asymptotic expressions for the rate of convergence and average runtime of the algorithm is presented. These estimates are used to compare the Monte Carlo method with discrete difference schemes.Numerical experiments involving some two-dimensional problems contirm the efficiency of the probabilistic scheme. ‘87 1990 Academic Press. Inc},
	language = {en},
	number = {1},
	urldate = {2022-12-26},
	journal = {Journal of Computational Physics},
	author = {Delaurentis, J.M and Romero, L.A},
	month = sep,
	year = {1990},
	keywords = {monte carlo, PDE, walk on spheres},
	pages = {123--140},
	file = {Delaurentis en Romero - 1990 - A Monte Carlo method for poisson's equation.pdf:C\:\\Users\\isido\\Zotero\\storage\\HV3ST26K\\Delaurentis en Romero - 1990 - A Monte Carlo method for poisson's equation.pdf:application/pdf},
}

@misc{noauthor_screened_nodate,
	title = {screened poisson equation via {WoS}},
	keywords = {PDE, walk on spheres},
	file = {1-s2.0-0041555369900706-main.pdf:C\:\\Users\\isido\\Zotero\\storage\\M6U2RW8L\\screened poisson with WoS.pdf:application/pdf},
}

@article{halton_sequential_2008,
	title = {Sequential {Monte} {Carlo} for linear systems – a practical summary},
	volume = {14},
	issn = {0929-9629, 1569-3961},
	url = {https://www.degruyter.com/document/doi/10.1515/MCMA.2008.001/html},
	doi = {10.1515/MCMA.2008.001},
	abstract = {This paper has been written in response to many requests for a practical guide to the use of the technique of sequential Monte Carlo in the fast numerical solving of large systems of linear equations. This method, which I have used with considerable success to solve such problems, improving the tricks of the trade as I learned more about it, has suffered from some neglect through the mathematical difﬁculty, for some of those who are more interested in using the tool than in thinking about it, of some of the theoretical aspects of rigorously proving its validity, which – at this juncture – is no longer in question. I hope that I have now closed this gap in the related literature.},
	language = {en},
	number = {1},
	urldate = {2023-01-04},
	journal = {Monte Carlo Methods and Applications},
	author = {Halton, John H.},
	month = jan,
	year = {2008},
	keywords = {monte carlo, linear systems},
	pages = {1--27},
	file = {Halton - 2008 - Sequential Monte Carlo for linear systems – a prac.pdf:C\:\\Users\\isido\\Zotero\\storage\\YGVFCZZK\\Halton - 2008 - Sequential Monte Carlo for linear systems – a prac.pdf:application/pdf},
}

@article{efendiev_multiscale_2004,
	title = {Multiscale {Finite} {Element} {Methods} for {Nonlinear} {Problems} and {Their} {Applications}},
	volume = {2},
	issn = {15396746, 19450796},
	url = {http://www.intlpress.com/site/pub/pages/journals/items/cms/content/vols/0002/0004/a002/},
	doi = {10.4310/CMS.2004.v2.n4.a2},
	abstract = {In this paper we propose a generalization of multiscale ﬁnite element methods (MsFEM) to nonlinear problems. We study the convergence of the proposed method for nonlinear elliptic equations and propose an oversampling technique. Numerical examples demonstrate that the oversampling technique greatly reduces the error. The application of MsFEM to porous media ﬂows is considered. Finally, we describe further generalizations of MsFEM to nonlinear time-dependent equations and discuss the convergence of the method for various kinds of heterogeneities.},
	language = {en},
	number = {4},
	urldate = {2023-01-05},
	journal = {Communications in Mathematical Sciences},
	author = {Efendiev, Y. and Ginting, V. and Hou, T. Y.},
	year = {2004},
	keywords = {PDE},
	pages = {553--589},
	file = {Efendiev e.a. - 2004 - Multiscale Finite Element Methods for Nonlinear Pr.pdf:C\:\\Users\\isido\\Zotero\\storage\\T66753T4\\Efendiev e.a. - 2004 - Multiscale Finite Element Methods for Nonlinear Pr.pdf:application/pdf},
}

@article{linetsky_path_nodate,
	title = {The {Path} {Integral} {Approach} to {Financial} {Modeling} and {Options} {Pricing}},
	abstract = {In this paper we review some applications of the path integral methodology of quantum mechanics to ﬁnancial modeling and options pricing. A path integral is deﬁned as a limit of the sequence of ﬁnite-dimensional integrals, in a much the same way as the Riemannian integral is deﬁned as a limit of the sequence of ﬁnite sums. The risk-neutral valuation formula for path-dependent options contingent upon multiple underlying assets admits an elegant representation in terms of path integrals (Feynman–Kac formula). The path integral representation of transition probability density (Green’s function) explicitly satisﬁes the diffusion PDE. Gaussian path integrals admit a closed-form solution given by the Van Vleck formula. Analytical approximations are obtained by means of the semiclassical (moments) expansion. Difﬁcult path integrals are computed by numerical procedures, such as Monte Carlo simulation or deterministic discretization schemes. Several examples of pathdependent options are treated to illustrate the theory (weighted Asian options, ﬂoating barrier options, and barrier options with ladder-like barriers).},
	language = {en},
	author = {Linetsky, Vadim},
	keywords = {examen padintegralen},
	file = {Linetsky - The Path Integral Approach to Financial Modeling a.pdf:C\:\\Users\\isido\\Zotero\\storage\\3ZDBN84W\\Linetsky - The Path Integral Approach to Financial Modeling a.pdf:application/pdf},
}

@article{das_sarma_fast_2015,
	title = {Fast distributed {PageRank} computation},
	volume = {561},
	issn = {03043975},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304397514002709},
	doi = {10.1016/j.tcs.2014.04.003},
	abstract = {Over the last decade, PageRank has gained importance in a wide range of applications and domains, ever since it ﬁrst proved to be effective in determining node importance in large graphs (and was a pioneering idea behind Google’s search engine). In distributed computing alone, PageRank vector, or more generally random walk based quantities have been used for several different applications ranging from determining important nodes, load balancing, search, and identifying connectivity structures. Surprisingly, however, there has been little work towards designing provably eﬃcient fully-distributed algorithms for computing PageRank. The diﬃculty is that traditional matrix–vector multiplication style iterative methods may not always adapt well to the distributed setting owing to communication bandwidth restrictions and convergence rates.},
	language = {en},
	urldate = {2023-01-05},
	journal = {Theoretical Computer Science},
	author = {Das Sarma, Atish and Molla, Anisur Rahaman and Pandurangan, Gopal and Upfal, Eli},
	month = jan,
	year = {2015},
	keywords = {page rank},
	pages = {113--121},
	file = {Das Sarma e.a. - 2015 - Fast distributed PageRank computation.pdf:C\:\\Users\\isido\\Zotero\\storage\\FLUGEZHI\\Das Sarma e.a. - 2015 - Fast distributed PageRank computation.pdf:application/pdf},
}

@misc{zhang_advanced_2021,
	title = {An {Advanced} {Parallel} {PageRank} {Algorithm}},
	url = {http://arxiv.org/abs/2112.07363},
	abstract = {Initially used to rank web pages, PageRank has now been applied in may ﬁelds. In general case, there are plenty of special vertices such as dangling vertices and unreferenced vertices in the graph. Existing PageRank algorithms usually consider them as ‘bad‘ vertices since they may take troubles. However, in this paper, we propose a parallel PageRank algorithm which can take advantage of these special vertices. For this end, we ﬁrstly interpret PageRank from the information transmitting perspective and give a constructive deﬁnition of PageRank. Then, based on the information transmitting interpretation, a parallel PageRank algorithm which we call the Information Transmitting Algorithm(ITA) is proposed. We prove that the dangling vertices can increase ITA’s convergence rate and the unreferenced vertices and weak unreferenced vertices can decrease ITA’s calculations. Compared with the MONTE CARLO method, ITA has lower bandwidth requirement. Compared with the power method, ITA has higher convergence rate and generates less calculations. Finally, experimental results on four data sets demonstrate that ITA is 1.5-4 times faster than the power method and converges more uniformly.},
	language = {en},
	urldate = {2023-01-05},
	publisher = {arXiv},
	author = {Zhang, Qi and Yao, Zhengan and Liang, Jun and Zhang, Zanbo},
	month = dec,
	year = {2021},
	note = {arXiv:2112.07363 [cs]},
	keywords = {Computer Science - Networking and Internet Architecture, page rank},
	file = {Zhang e.a. - 2021 - An Advanced Parallel PageRank Algorithm.pdf:C\:\\Users\\isido\\Zotero\\storage\\YZS4ETMH\\Zhang e.a. - 2021 - An Advanced Parallel PageRank Algorithm.pdf:application/pdf},
}

@inproceedings{yang_efficient_2019,
	title = {Efficient {Estimation} of {Heat} {Kernel} {PageRank} for {Local} {Clustering}},
	url = {http://arxiv.org/abs/1904.02707},
	doi = {10.1145/3299869.3319886},
	abstract = {Given an undirected graph G and a seed node s, the local clustering problem aims to identify a high-quality cluster containing s in time roughly proportional to the size of the cluster, regardless of the size of G. This problem finds numerous applications on large-scale graphs. Recently, heat kernel PageRank (HKPR), which is a measure of the proximity of nodes in graphs, is applied to this problem and found to be more efficient compared with prior methods. However, existing solutions for computing HKPR either are prohibitively expensive or provide unsatisfactory error approximation on HKPR values, rendering them impractical especially on billion-edge graphs. In this paper, we present TEA and TEA+, two novel local graph clustering algorithms based on HKPR, to address the aforementioned limitations. Specifically, these algorithms provide non-trivial theoretical guarantees in relative error of HKPR values and the time complexity. The basic idea is to utilize deterministic graph traversal to produce a rough estimation of exact HKPR vector, and then exploit Monte-Carlo random walks to refine the results in an optimized and non-trivial way. In particular, TEA+ offers practical efficiency and effectiveness due to non-trivial optimizations. Extensive experiments on real-world datasets demonstrate that TEA+ outperforms the state-of-the-art algorithm by more than four times on most benchmark datasets in terms of computational time when achieving the same clustering quality, and in particular, is an order of magnitude faster on large graphs including the widely studied Twitter and Friendster datasets.},
	language = {en},
	urldate = {2023-01-05},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Management} of {Data}},
	author = {Yang, Renchi and Xiao, Xiaokui and Wei, Zhewei and Bhowmick, Sourav S. and Zhao, Jun and Li, Rong-Hua},
	month = jun,
	year = {2019},
	note = {arXiv:1904.02707 [cs]},
	keywords = {Computer Science - Databases, Computer Science - Social and Information Networks, page rank},
	pages = {1339--1356},
	annote = {Comment: The technical report for the full research paper accepted in the SIGMOD 2019},
	file = {Yang e.a. - 2019 - Efficient Estimation of Heat Kernel PageRank for L.pdf:C\:\\Users\\isido\\Zotero\\storage\\AZE6CUJG\\Yang e.a. - 2019 - Efficient Estimation of Heat Kernel PageRank for L.pdf:application/pdf},
}

@misc{salaun_regression-based_2022,
	title = {Regression-based {Monte} {Carlo} {Integration}},
	url = {http://arxiv.org/abs/2211.07422},
	abstract = {Monte Carlo integration is typically interpreted as an estimator of the expected value using stochastic samples. There exists an alternative interpretation in calculus where Monte Carlo integration can be seen as estimating a {\textbackslash}emph\{constant\} function -- from the stochastic evaluations of the integrand -- that integrates to the original integral. The integral mean value theorem states that this {\textbackslash}emph\{constant\} function should be the mean (or expectation) of the integrand. Since both interpretations result in the same estimator, little attention has been devoted to the calculus-oriented interpretation. We show that the calculus-oriented interpretation actually implies the possibility of using a more {\textbackslash}emph\{complex\} function than a {\textbackslash}emph\{constant\} one to construct a more efficient estimator for Monte Carlo integration. We build a new estimator based on this interpretation and relate our estimator to control variates with least-squares regression on the stochastic samples of the integrand. Unlike prior work, our resulting estimator is {\textbackslash}emph\{provably\} better than or equal to the conventional Monte Carlo estimator. To demonstrate the strength of our approach, we introduce a practical estimator that can act as a simple drop-in replacement for conventional Monte Carlo integration. We experimentally validate our framework on various light transport integrals. The code is available at {\textbackslash}url\{https://github.com/iribis/regressionmc\}.},
	language = {en},
	urldate = {2023-01-05},
	publisher = {arXiv},
	author = {Salaün, Corentin and Gruson, Adrien and Hua, Binh-Son and Hachisuka, Toshiya and Singh, Gurprit},
	month = nov,
	year = {2022},
	note = {arXiv:2211.07422 [cs]},
	keywords = {monte carlo, Computer Science - Graphics, rendering, control variates},
	annote = {Comment: 14 pages, 16 figures, ACM Trans. Graph., Vol. 41, No. 4, Article 79. Publication date: July 2022},
	annote = {see randomised trapezoidal
},
	file = {Salaün e.a. - 2022 - Regression-based Monte Carlo Integration.pdf:C\:\\Users\\isido\\Zotero\\storage\\FDCESJF7\\Salaün e.a. - 2022 - Regression-based Monte Carlo Integration.pdf:application/pdf},
}

@misc{albert_tau-leaping_2023,
	title = {A tau-leaping method for computing joint probability distributions of the first-passage time and position of a {Brownian} particle},
	url = {http://arxiv.org/abs/2301.00647},
	abstract = {First passage time (FPT), also known as ﬁrst hitting time, is the time a particle, subject to some stochastic process, hits or crosses a closed surface for the very ﬁrst time. τ -leaping methods are a class of stochastic algorithms in which, instead of simulating every single reaction, many reactions are “leaped” over in order to shorten the computing time. In this paper we developed a τ -leaping method for computing the FPT and position in arbitrary volumes for a Brownian particle governed by the Langevin equation. The τ -leaping method proposed here works as follows. A sphere is inscribed within the volume of interest (VOI) centered at the initial particle’s location. On this sphere, the FPT is sampled, as well as the position, which becomes the new initial position. Then, another sphere, centered at this new location, is inscribed. This process continues until the sphere becomes smaller than some minimal radius Rmin. When this occurs, the τ -leaping switches to the conventional Monte Carlo, which runs until the particle either crosses the surface of the VOI or ﬁnds its way to a position where a sphere of radius {\textgreater} Rmin can be inscribed. The switching between τ -leaping and MC continues until the particle crosses the surface of the VOI. The purpose of a minimal radius is to avoid having to sample the velocities, which become irrelevant when the particle diﬀuses beyond a certain distance, i. e. Rmin The size of this radius depends on the system parameters and on one’s notion of accuracy: the larger this radius the more accurate the τ -leaping method, but also less eﬃcient. This trade oﬀ between accuracy and eﬃciency is discussed. For two VOI, the τ -leaping method is shown to be accurate and more eﬃcient than MC by at least a factor of 10 and up to a factor of about 110. However, while MC becomes exponentially slower with increasing VOI, the eﬃciency of the τ -leaping method remains relatively unchanged. Thus, the τ -leaping method can potentially be many orders of magnitude more eﬃcient than MC.},
	language = {en},
	urldate = {2023-01-05},
	publisher = {arXiv},
	author = {Albert, Jaroslav},
	month = jan,
	year = {2023},
	note = {arXiv:2301.00647 [cond-mat, q-bio]},
	keywords = {Condensed Matter - Soft Condensed Matter, Quantitative Biology - Molecular Networks, first passage},
	file = {Albert - 2023 - A tau-leaping method for computing joint probabili.pdf:C\:\\Users\\isido\\Zotero\\storage\\J5YXETE7\\Albert - 2023 - A tau-leaping method for computing joint probabili.pdf:application/pdf},
}

@article{kondapaneni_optimal_2019,
	title = {Optimal multiple importance sampling},
	volume = {38},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3306346.3323009},
	doi = {10.1145/3306346.3323009},
	abstract = {Multiple Importance Sampling (MIS) is a key technique for achieving robustness of Monte Carlo estimators in computer graphics and other fields. We derive optimal weighting functions for MIS that provably minimize the variance of an MIS estimator, given a set of sampling techniques. We show that the resulting variance reduction over the balance heuristic can be higher than predicted by the variance bounds derived by Veach and Guibas, who assumed only non-negative weights in their proof. We theoretically analyze the variance of the optimal MIS weights and show the relation to the variance of the balance heuristic. Furthermore, we establish a connection between the new weighting functions and control variates as previously applied to mixture sampling. We apply the new optimal weights to integration problems in light transport and show that they allow for new design considerations when choosing the appropriate sampling techniques for a given integration problem.},
	language = {en},
	number = {4},
	urldate = {2023-01-06},
	journal = {ACM Transactions on Graphics},
	author = {Kondapaneni, Ivo and Vevoda, Petr and Grittmann, Pascal and Skřivan, Tomáš and Slusallek, Philipp and Křivánek, Jaroslav},
	month = aug,
	year = {2019},
	keywords = {monte carlo, rendering, importance sampling},
	pages = {1--14},
	file = {Kondapaneni e.a. - 2019 - Optimal multiple importance sampling.pdf:C\:\\Users\\isido\\Zotero\\storage\\LN9YUIJZ\\Kondapaneni e.a. - 2019 - Optimal multiple importance sampling.pdf:application/pdf},
}

@misc{muller_neural_2020,
	title = {Neural {Control} {Variates}},
	url = {http://arxiv.org/abs/2006.01524},
	abstract = {We propose neural control variates (NCV) for unbiased variance reduction in parametric Monte Carlo integration. So far, the core challenge of applying the method of control variates has been finding a good approximation of the integrand that is cheap to integrate. We show that a set of neural networks can face that challenge: a normalizing flow that approximates the shape of the integrand and another neural network that infers the solution of the integral equation. We also propose to leverage a neural importance sampler to estimate the difference between the original integrand and the learned control variate. To optimize the resulting parametric estimator, we derive a theoretically optimal, variance-minimizing loss function, and propose an alternative, composite loss for stable online training in practice. When applied to light transport simulation, neural control variates are capable of matching the state-of-the-art performance of other unbiased approaches, while providing means to develop more performant, practical solutions. Specifically, we show that the learned light-field approximation is of sufficient quality for high-order bounces, allowing us to omit the error correction and thereby dramatically reduce the noise at the cost of negligible visible bias.},
	language = {en},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Müller, Thomas and Rousselle, Fabrice and Novák, Jan and Keller, Alexander},
	month = sep,
	year = {2020},
	note = {arXiv:2006.01524 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, monte carlo, Computer Science - Graphics, rendering, control variates, machine learning},
	annote = {Comment: To appear at SIGGRAPH Asia 2020. Updated with better loss function, leading to better results. 19 pages, 14 figures},
	file = {Müller e.a. - 2020 - Neural Control Variates.pdf:C\:\\Users\\isido\\Zotero\\storage\\VJD8P7DL\\Müller e.a. - 2020 - Neural Control Variates.pdf:application/pdf},
}

@misc{muller_neural_2019,
	title = {Neural {Importance} {Sampling}},
	url = {http://arxiv.org/abs/1808.03856},
	abstract = {We propose to use deep neural networks for generating samples in Monte Carlo integration. Our work is based on non-linear independent components estimation (NICE), which we extend in numerous ways to improve performance and enable its application to integration problems. First, we introduce piecewise-polynomial coupling transforms that greatly increase the modeling power of individual coupling layers. Second, we propose to preprocess the inputs of neural networks using one-blob encoding, which stimulates localization of computation and improves inference. Third, we derive a gradient-descent-based optimization for the KL and the \${\textbackslash}chi{\textasciicircum}2\$ divergence for the specific application of Monte Carlo integration with unnormalized stochastic estimates of the target distribution. Our approach enables fast and accurate inference and efficient sample generation independently of the dimensionality of the integration domain. We show its benefits on generating natural images and in two applications to light-transport simulation: first, we demonstrate learning of joint path-sampling densities in the primary sample space and importance sampling of multi-dimensional path prefixes thereof. Second, we use our technique to extract conditional directional densities driven by the product of incident illumination and the BSDF in the rendering equation, and we leverage the densities for path guiding. In all applications, our approach yields on-par or higher performance than competing techniques at equal sample count.},
	language = {en},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Müller, Thomas and McWilliams, Brian and Rousselle, Fabrice and Gross, Markus and Novák, Jan},
	month = sep,
	year = {2019},
	note = {arXiv:1808.03856 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, monte carlo, Computer Science - Graphics, rendering, importance sampling, machine learning},
	annote = {Comment: 19 pages, 15 figures. Accepted for publication in ACM Transactions on Graphics; presented at SIGGRAPH 2019},
	file = {Müller e.a. - 2019 - Neural Importance Sampling.pdf:C\:\\Users\\isido\\Zotero\\storage\\UQDR2FVH\\Müller e.a. - 2019 - Neural Importance Sampling.pdf:application/pdf},
}

@article{ruppert_robust_2020,
	title = {Robust fitting of parallax-aware mixtures for path guiding},
	volume = {39},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3386569.3392421},
	doi = {10.1145/3386569.3392421},
	abstract = {Effective local light transport guiding demands for high quality guiding information, i.e., a precise representation of the directional incident radiance distribution at every point inside the scene. We introduce a parallax-aware distribution model based on parametric mixtures. By parallax-aware warping of the distribution, the local approximation of the 5D radiance field remains valid and precise across large spatial regions, even for close-by contributors. Our robust optimization scheme fits parametric mixtures to radiance samples collected in previous rendering passes. Robustness is achieved by splitting and merging of components refining the mixture. These splitting and merging decisions minimize and bound the expected variance of the local radiance estimator. In addition, we extend the fitting scheme to a robust, iterative update method, which allows for incremental training of our model using smaller sample batches. This results in more frequent training updates and, at the same time, significantly reduces the required sample memory footprint. The parametric representation of our model allows for the application of advanced importance sampling methods such as radiance-based, cosine-aware, and even product importance sampling. Our method further smoothly integrates next-event estimation (NEE) into path guiding, avoiding importance sampling of contributions better covered by NEE. The proposed robust fitting and update scheme, in combination with the parallax-aware representation, results in faster learning and lower variance compared to state-of-the-art path guiding approaches.},
	language = {en},
	number = {4},
	urldate = {2023-01-06},
	journal = {ACM Transactions on Graphics},
	author = {Ruppert, Lukas and Herholz, Sebastian and Lensch, Hendrik P. A.},
	month = aug,
	year = {2020},
	keywords = {rendering},
	file = {Ruppert e.a. - 2020 - Robust fitting of parallax-aware mixtures for path.pdf:C\:\\Users\\isido\\Zotero\\storage\\Y3W4KYQT\\Ruppert e.a. - 2020 - Robust fitting of parallax-aware mixtures for path.pdf:application/pdf},
}

@misc{dahm_learning_2017,
	title = {Learning {Light} {Transport} the {Reinforced} {Way}},
	url = {http://arxiv.org/abs/1701.07403},
	abstract = {We show that the equations of reinforcement learning and light transport simulation are related integral equations. Based on this correspondence, a scheme to learn importance while sampling path space is derived. The new approach is demonstrated in a consistent light transport simulation algorithm that uses reinforcement learning to progressively learn where light comes from. As using this information for importance sampling includes information about visibility, too, the number of light transport paths with zero contribution is dramatically reduced, resulting in much less noisy images within a ﬁxed time budget.},
	language = {en},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Dahm, Ken and Keller, Alexander},
	month = aug,
	year = {2017},
	note = {arXiv:1701.07403 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Graphics, rendering, machine learning, reinforcement learning},
	annote = {Comment: Revised version},
	file = {Dahm en Keller - 2017 - Learning Light Transport the Reinforced Way.pdf:C\:\\Users\\isido\\Zotero\\storage\\2D4V4CM6\\Dahm en Keller - 2017 - Learning Light Transport the Reinforced Way.pdf:application/pdf},
}

@article{manzi_gradient-domain_nodate,
	title = {Gradient-{Domain} {Bidirectional} {Path} {Tracing}},
	abstract = {Gradient-domain path tracing has recently been introduced as an efﬁcient realistic image synthesis algorithm. This paper introduces a bidirectional gradient-domain sampler that outperforms traditional bidirectional path tracing often by a factor of two to ﬁve in terms of squared error at equal render time. It also improves over unidirectional gradient-domain path tracing in challenging visibility conditions, similarly to how conventional bidirectional path tracing improves over its unidirectional counterpart. Our algorithm leverages a novel multiple importance sampling technique and an efﬁcient implementation of a high-quality shift mapping suitable for bidirectional path tracing. We demonstrate the versatility of our approach in several challenging light transport scenarios.},
	language = {en},
	author = {Manzi, Marco and Kettunen, Markus and Aittala, Miika and Lehtinen, Jaakko and Durand, Frédo and Zwicker, Matthias},
	keywords = {rendering},
	file = {Manzi e.a. - Gradient-Domain Bidirectional Path Tracing.pdf:C\:\\Users\\isido\\Zotero\\storage\\G9T7TZDT\\Manzi e.a. - Gradient-Domain Bidirectional Path Tracing.pdf:application/pdf},
}

@misc{durkan_cubic-spline_2019,
	title = {Cubic-{Spline} {Flows}},
	url = {http://arxiv.org/abs/1906.02145},
	abstract = {A normalizing ﬂow models a complex probability density as an invertible transformation of a simple density. The invertibility means that we can evaluate densities and generate samples from a ﬂow. In practice, autoregressive ﬂow-based models are slow to invert, making either density estimation or sample generation slow. Flows based on coupling transforms are fast for both tasks, but have previously performed less well at density estimation than autoregressive ﬂows. We stack a new coupling transform, based on monotonic cubic splines, with LU-decomposed linear layers. The resulting cubic-spline ﬂow retains an exact onepass inverse, can be used to generate high-quality images, and closes the gap with autoregressive ﬂows on a suite of density-estimation tasks.},
	language = {en},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
	month = jun,
	year = {2019},
	note = {arXiv:1906.02145 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, machine learning},
	annote = {Comment: Appeared at the 1st Workshop on Invertible Neural Networks and Normalizing Flows at ICML 2019},
	file = {Durkan e.a. - 2019 - Cubic-Spline Flows.pdf:C\:\\Users\\isido\\Zotero\\storage\\FG5VAIUR\\Durkan e.a. - 2019 - Cubic-Spline Flows.pdf:application/pdf},
}

@misc{durkan_neural_2019,
	title = {Neural {Spline} {Flows}},
	url = {http://arxiv.org/abs/1906.04032},
	abstract = {A normalizing ﬂow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the ﬂexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the ﬂexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline ﬂows improve density estimation, variational inference, and generative modeling of images.},
	language = {en},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
	month = dec,
	year = {2019},
	note = {arXiv:1906.04032 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, machine learning},
	annote = {Comment: Published at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada},
	file = {Durkan e.a. - 2019 - Neural Spline Flows.pdf:C\:\\Users\\isido\\Zotero\\storage\\RNWFF9CZ\\Durkan e.a. - 2019 - Neural Spline Flows.pdf:application/pdf},
}

@misc{papamakarios_normalizing_2021,
	title = {Normalizing {Flows} for {Probabilistic} {Modeling} and {Inference}},
	url = {http://arxiv.org/abs/1912.02762},
	abstract = {Normalizing ﬂows provide a general mechanism for deﬁning expressive probability distributions, only requiring the speciﬁcation of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing ﬂows, ranging from improving their expressive power to expanding their application. We believe the ﬁeld has now matured and is in need of a uniﬁed perspective. In this review, we attempt to provide such a perspective by describing ﬂows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of ﬂow design, and discuss foundational topics such as expressive power and computational trade-oﬀs. We also broaden the conceptual framing of ﬂows by relating them to more general probability transformations. Lastly, we summarize the use of ﬂows for tasks such as generative modeling, approximate inference, and supervised learning.},
	language = {en},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	month = apr,
	year = {2021},
	note = {arXiv:1912.02762 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, importance sampling, machine learning},
	annote = {Comment: Review article, 64 pages, 9 figures. Published in the Journal of Machine Learning Research (see https://jmlr.org/papers/v22/19-1028.html)},
	file = {Papamakarios e.a. - 2021 - Normalizing Flows for Probabilistic Modeling and I.pdf:C\:\\Users\\isido\\Zotero\\storage\\WS7G7WN2\\Papamakarios e.a. - 2021 - Normalizing Flows for Probabilistic Modeling and I.pdf:application/pdf},
}

@misc{jaini_sum--squares_2019,
	title = {Sum-of-{Squares} {Polynomial} {Flow}},
	url = {http://arxiv.org/abs/1905.02325},
	abstract = {Triangular map is a recent construct in probability theory that allows one to transform any source probability density function to any target density function. Based on triangular maps, we propose a general framework for high-dimensional density estimation, by specifying one-dimensional transformations (equivalently conditional densities) and appropriate conditioner networks. This framework (a) reveals the commonalities and differences of existing autoregressive and ﬂow based methods, (b) allows a uniﬁed understanding of the limitations and representation power of these recent approaches and, (c) motivates us to uncover a new Sum-of-Squares (SOS) ﬂow that is interpretable, universal, and easy to train. We perform several synthetic experiments on various density geometries to demonstrate the beneﬁts (and shortcomings) of such transformations. SOS ﬂows achieve competitive results in simulations and several real-world datasets.},
	language = {en},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Jaini, Priyank and Selby, Kira A. and Yu, Yaoliang},
	month = jun,
	year = {2019},
	note = {arXiv:1905.02325 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, importance sampling, machine learning},
	annote = {Comment: 13 pages, ICML'2019},
	file = {Jaini e.a. - 2019 - Sum-of-Squares Polynomial Flow.pdf:C\:\\Users\\isido\\Zotero\\storage\\PCM8RNZR\\Jaini e.a. - 2019 - Sum-of-Squares Polynomial Flow.pdf:application/pdf},
}

@misc{beznea_monte_2022,
	title = {From {Monte} {Carlo} to neural networks approximations of boundary value problems},
	url = {http://arxiv.org/abs/2209.01432},
	abstract = {In this paper we study probabilistic and neural network approximations for solutions to Poisson equation subject to H¨older or C2 data in general bounded domains of Rd. We aim at two fundamental goals.},
	language = {en},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Beznea, Lucian and Cimpean, Iulian and Lupascu-Stamate, Oana and Popescu, Ionel and Zarnescu, Arghir},
	month = sep,
	year = {2022},
	note = {arXiv:2209.01432 [cs, math]},
	keywords = {Computer Science - Machine Learning, monte carlo, Mathematics - Probability, PDE, Mathematics - Numerical Analysis, Computer Science - Artificial Intelligence, Mathematics - Analysis of PDEs, machine learning, neural networks, boundary value problems},
	file = {Beznea e.a. - 2022 - From Monte Carlo to neural networks approximations.pdf:C\:\\Users\\isido\\Zotero\\storage\\G4JL3TT7\\Beznea e.a. - 2022 - From Monte Carlo to neural networks approximations.pdf:application/pdf},
}

@article{zergainoh_construction_2007,
	title = {Construction of {Orthonormal} {Piecewise} {Polynomial} {Scaling} and {Wavelet} {Bases} on {Non}-{Equally} {Spaced} {Knots}},
	volume = {2007},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/2007/27427},
	doi = {10.1155/2007/27427},
	abstract = {his paper investigates the mathematical framework of multiresolution analysis based on irregularly spaced knots sequence. Our presentation is based on the construction of nested nonuniform spline multiresolution spaces. From these spaces, we present the construction of orthonormal scaling and wavelet basis functions on bounded intervals. For any arbitrary degree of the spline function, we provide an explicit generalization allowing the construction of the scaling and wavelet bases on the nontraditional sequences. We show that the orthogonal decomposition is implemented using filter banks where the coefficients depend on the location of the knots on the sequence. Examples of orthonormal spline scaling and wavelet bases are provided. This approach can be used to interpolate irregularly sampled signals in an efficient way, by keeping the multiresolution approach.},
	language = {en},
	number = {1},
	urldate = {2023-01-08},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Zergaïnoh, Anissa and Chihab, Najat and Astruc, Jean Pierre},
	month = dec,
	year = {2007},
	keywords = {wavelets},
	pages = {027427},
	file = {Zergaïnoh e.a. - 2007 - Construction of Orthonormal Piecewise Polynomial S.pdf:C\:\\Users\\isido\\Zotero\\storage\\IA4NHCZX\\Zergaïnoh e.a. - 2007 - Construction of Orthonormal Piecewise Polynomial S.pdf:application/pdf},
}

@misc{subr_q-net_2021,
	title = {Q-{NET}: {A} {Network} for {Low}-{Dimensional} {Integrals} of {Neural} {Proxies}},
	shorttitle = {Q-{NET}},
	url = {http://arxiv.org/abs/2006.14396},
	abstract = {Many applications require the calculation of integrals of multidimensional functions. A general and popular procedure is to estimate integrals by averaging multiple evaluations of the function. Often, each evaluation of the function entails costly computations. The use of a {\textbackslash}emph\{proxy\} or surrogate for the true function is useful if repeated evaluations are necessary. The proxy is even more useful if its integral is known analytically and can be calculated practically. We propose the use of a versatile yet simple class of artificial neural networks -- sigmoidal universal approximators -- as a proxy for functions whose integrals need to be estimated. We design a family of fixed networks, which we call Q-NETs, that operate on parameters of a trained proxy to calculate exact integrals over {\textbackslash}emph\{any subset of dimensions\} of the input domain. We identify transformations to the input space for which integrals may be recalculated without resampling the integrand or retraining the proxy. We highlight the benefits of this scheme for a few applications such as inverse rendering, generation of procedural noise, visualization and simulation. The proposed proxy is appealing in the following contexts: the dimensionality is low (\${\textless}10\$D); the estimation of integrals needs to be decoupled from the sampling strategy; sparse, adaptive sampling is used; marginal functions need to be known in functional form; or when powerful Single Instruction Multiple Data/Thread (SIMD/SIMT) pipelines are available for computation.},
	language = {en},
	urldate = {2023-01-08},
	publisher = {arXiv},
	author = {Subr, Kartic},
	month = mar,
	year = {2021},
	note = {arXiv:2006.14396 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, machine learning, neural networks},
	annote = {Comment: 11 pages (including appendix and references)},
	file = {Subr - 2021 - Q-NET A Network for Low-Dimensional Integrals of .pdf:C\:\\Users\\isido\\Zotero\\storage\\N8BQBFKD\\Subr - 2021 - Q-NET A Network for Low-Dimensional Integrals of .pdf:application/pdf},
}

@inproceedings{duminil-copin_sixty_2019,
	address = {Rio de Janeiro, Brazil},
	title = {{SIXTY} {YEARS} {OF} {PERCOLATION}},
	isbn = {978-981-327-287-3 978-981-327-288-0},
	url = {https://www.worldscientific.com/doi/abs/10.1142/9789813272880_0162},
	doi = {10.1142/9789813272880_0162},
	abstract = {Percolation models describe the inside of a porous material. The theory emerged timidly in the middle of the twentieth century before becoming one of the major objects of interest in probability and mathematical physics. The golden age of percolation is probably the eighties, during which most of the major results were obtained for the most classical of these models, named Bernoulli percolation, but it is really the two following decades which put percolation theory at the crossroad of several domains of mathematics. In this broad review, we propose to describe brieﬂy some recent progress as well as some famous challenges remaining in the ﬁeld. This review is not intended to probabilists (and a fortiori not to specialists in percolation theory): the target audience is mathematicians of all kinds.},
	language = {en},
	urldate = {2023-01-09},
	booktitle = {Proceedings of the {International} {Congress} of {Mathematicians} ({ICM} 2018)},
	publisher = {WORLD SCIENTIFIC},
	author = {Duminil-Copin, Hugo},
	month = may,
	year = {2019},
	pages = {2829--2856},
	file = {Duminil-Copin - 2019 - SIXTY YEARS OF PERCOLATION.pdf:C\:\\Users\\isido\\Zotero\\storage\\KAH28XWG\\Duminil-Copin - 2019 - SIXTY YEARS OF PERCOLATION.pdf:application/pdf},
}

@misc{enthought_umap_2018,
	title = {{UMAP} {Uniform} {Manifold} {Approximation} and {Projection} for {Dimension} {Reduction} {\textbar} {SciPy} 2018 {\textbar}},
	url = {https://www.youtube.com/watch?v=nq6iPZVUxZU},
	urldate = {2023-01-10},
	author = {{Enthought}},
	month = jul,
	year = {2018},
	keywords = {machine learning},
}

@misc{pydata_bluffers_2019,
	title = {A {Bluffer}'s {Guide} to {Dimension} {Reduction} - {Leland} {McInnes}},
	url = {https://www.youtube.com/watch?v=9iol3Lk6kyU},
	urldate = {2023-01-10},
	author = {{PyData}},
	month = feb,
	year = {2019},
	keywords = {machine learning},
}

@misc{stitch_fix_multithreaded_algo_2020,
	title = {Algo {Hour} - {Nearest} {Neighbor} {Descent} (and friends) {\textbar} {Dr}. {Leland} {McInnes}},
	url = {https://www.youtube.com/watch?v=OvT2NY_FV_g},
	urldate = {2023-01-10},
	author = {{Stitch Fix Multithreaded}},
	month = may,
	year = {2020},
	keywords = {machine learning},
}

@misc{enthought_high_2016,
	title = {High {Quality}, {High} {Performance} {Clustering} with {HDBSCAN} {\textbar} {SciPy} 2016 {\textbar} {Leland} {McInnes}},
	url = {https://www.youtube.com/watch?v=AgPQ76RIi6A},
	urldate = {2023-01-10},
	author = {{Enthought}},
	month = jul,
	year = {2016},
	keywords = {machine learning},
}

@misc{hajimohammadi_legendre_2021,
	title = {Legendre {Deep} {Neural} {Network} ({LDNN}) and its application for approximation of nonlinear {Volterra} {Fredholm} {Hammerstein} integral equations},
	url = {http://arxiv.org/abs/2106.14320},
	abstract = {Various phenomena in biology, physics, and engineering are modeled by diﬀerential equations. These diﬀerential equations including partial diﬀerential equations and ordinary diﬀerential equations can be converted and represented as integral equations. In particular, Volterra–Fredholm–Hammerstein integral equations are the main type of these integral equations and researchers are interested in investigating and solving these equations. In this paper, we propose Legendre Deep Neural Network (LDNN) for solving nonlinear Volterra–Fredholm–Hammerstein integral equations (V-F-H-IEs). LDNN utilizes Legendre orthogonal polynomials as activation functions of the Deep structure. We present how LDNN can be used to solve nonlinear V-F-H-IEs. We show using the Gaussian quadrature collocation method in combination with LDNN results in a novel numerical solution for nonlinear V-F-H-IEs. Several examples are given to verify the performance and accuracy of LDNN.},
	language = {en},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Hajimohammadi, Zeinab and Parand, Kourosh and Ghodsi, Ali},
	month = jun,
	year = {2021},
	note = {arXiv:2106.14320 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, integral equations, machine learning},
	file = {Hajimohammadi e.a. - 2021 - Legendre Deep Neural Network (LDNN) and its applic.pdf:C\:\\Users\\isido\\Zotero\\storage\\PYEIWQ2R\\Hajimohammadi e.a. - 2021 - Legendre Deep Neural Network (LDNN) and its applic.pdf:application/pdf},
}

@misc{formica_method_2021,
	title = {Method {Monte}-{Carlo} for solving of non-linear integral equations},
	url = {http://arxiv.org/abs/2102.07859},
	abstract = {We oﬀer in this short report a simple Monte - Carlo method for solving a well posed non - linear integral equations of second Fredholm’s and Volterra’s type and built a conﬁdence region for solution in an uniform norm, applying the grounded Central Limit Theorem in the Banach space of continuous functions.},
	language = {en},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Formica, M. R. and Ostrovsky, E. and Sirota, L.},
	month = feb,
	year = {2021},
	note = {arXiv:2102.07859 [cs, math]},
	keywords = {monte carlo, Mathematics - Numerical Analysis, integral equations},
	file = {Formica e.a. - 2021 - Method Monte-Carlo for solving of non-linear integ.pdf:C\:\\Users\\isido\\Zotero\\storage\\INK9IZXC\\Formica e.a. - 2021 - Method Monte-Carlo for solving of non-linear integ.pdf:application/pdf},
}

@misc{azze_optimal_2022,
	title = {Optimal exercise of {American} options under time-dependent {Ornstein}-{Uhlenbeck} processes},
	url = {http://arxiv.org/abs/2211.04095},
	abstract = {We study the barrier that gives the optimal time to exercise an American option written on a time-dependent Ornstein–Uhlenbeck process, a diﬀusion often adopted by practitioners to model commodity prices and interest rates. By framing the optimal exercise of the American option as a problem of optimal stopping and relying on probabilistic arguments, we provide a non-linear Volterra-type integral equation characterizing the exercise boundary, develop a novel comparison argument to derive upper and lower bounds for such a boundary, and prove its diﬀerentiability and Lipschitz continuity in any closed interval that excludes the expiration date. We implement a Picard iteration algorithm to solve the Volterra integral equation and show illustrative examples that shed light on the boundary’s dependence on the process’s drift and volatility.},
	language = {en},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Azze, Abel and D'Auria, Bernardo and García-Portugués, Eduardo},
	month = nov,
	year = {2022},
	note = {arXiv:2211.04095 [math, q-fin]},
	keywords = {Mathematics - Probability, Quantitative Finance - Mathematical Finance, Quantitative Finance - Pricing of Securities, finance},
	annote = {Comment: 22 pages, 3 figures},
	file = {Azze e.a. - 2022 - Optimal exercise of American options under time-de.pdf:C\:\\Users\\isido\\Zotero\\storage\\G2H9S8M4\\Azze e.a. - 2022 - Optimal exercise of American options under time-de.pdf:application/pdf},
}

@misc{schneider_itvolt_2022,
	title = {{ITVOLT}: {An} {Iterative} {Solver} for {Volterra} {Integral} {Equations} with {Application} to the {Time}-{Dependent} {Schr}{\textbackslash}"odinger {Equation}},
	shorttitle = {{ITVOLT}},
	url = {http://arxiv.org/abs/2210.15677},
	abstract = {We present a novel iterative method for solving Volterra integral equations of the second kind. Based on global Lagrange interpolation, the method is simple to implement and applicable to a wide variety of problems. Here, we present the method in detail and discuss several applications, emphasizing in particular its use on the time-dependent Schro¨dinger equation.},
	language = {en},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Schneider, Ryan and Gharibnejad, Heman and Schneider, Barry I.},
	month = oct,
	year = {2022},
	note = {arXiv:2210.15677 [physics]},
	keywords = {Mathematics - Numerical Analysis, Physics - Computational Physics, integral equations},
	annote = {Comment: 20 pages, 9 tables, 5 figures},
	file = {Schneider e.a. - 2022 - ITVOLT An Iterative Solver for Volterra Integral .pdf:C\:\\Users\\isido\\Zotero\\storage\\VKU8FNSZ\\Schneider e.a. - 2022 - ITVOLT An Iterative Solver for Volterra Integral .pdf:application/pdf},
}

@misc{crucinio_particle_2021,
	title = {A {Particle} {Method} for {Solving} {Fredholm} {Equations} of the {First} {Kind}},
	url = {http://arxiv.org/abs/2009.09974},
	abstract = {Fredholm integral equations of the ﬁrst kind are the prototypical example of ill-posed linear inverse problems. They model, among other things, reconstruction of distorted noisy observations and indirect density estimation and also appear in instrumental variable regression. However, their numerical solution remains a challenging problem. Many techniques currently available require a preliminary discretization of the domain of the solution and make strong assumptions about its regularity. For example, the popular expectation maximization smoothing (EMS) scheme requires the assumption of piecewise constant solutions which is inappropriate for most applications. We propose here a novel particle method that circumvents these two issues. This algorithm can be thought of as a Monte Carlo approximation of the EMS scheme which not only performs an adaptive stochastic discretization of the domain but also results in smooth approximate solutions. We analyze the theoretical properties of the EMS iteration and of the corresponding particle algorithm. Compared to standard EMS, we show experimentally that our novel particle method provides state-of-the-art performance for realistic systems, including motion deblurring and reconstruction of cross-section images of the brain from positron emission tomography.},
	language = {en},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Crucinio, Francesca R. and Doucet, Arnaud and Johansen, Adam M.},
	month = apr,
	year = {2021},
	note = {arXiv:2009.09974 [stat]},
	keywords = {Statistics - Computation, Statistics - Methodology, integral equations},
	file = {Crucinio e.a. - 2021 - A Particle Method for Solving Fredholm Equations o.pdf:C\:\\Users\\isido\\Zotero\\storage\\HMHHAWGP\\Crucinio e.a. - 2021 - A Particle Method for Solving Fredholm Equations o.pdf:application/pdf},
}

@misc{keller_integral_2019,
	title = {Integral {Equations} and {Machine} {Learning}},
	url = {http://arxiv.org/abs/1712.06115},
	abstract = {As both light transport simulation and reinforcement learning are ruled by the same Fredholm integral equation of the second kind, reinforcement learning techniques may be used for photorealistic image synthesis: Eﬃciency may be dramatically improved by guiding light transport paths by an approximate solution of the integral equation that is learned during rendering. In the light of the recent advances in reinforcement learning for playing games, we investigate the representation of an approximate solution of an integral equation by artiﬁcial neural networks and derive a loss function for that purpose. The resulting Monte Carlo and quasi-Monte Carlo methods train neural networks with standard information instead of linear information and naturally are able to generate an arbitrary number of training samples. The methods are demonstrated for applications in light transport simulation.},
	language = {en},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Keller, Alexander and Dahm, Ken},
	month = jan,
	year = {2019},
	note = {arXiv:1712.06115 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Graphics, integral equations, machine learning, reinforcement learning},
	file = {Keller en Dahm - 2019 - Integral Equations and Machine Learning.pdf:C\:\\Users\\isido\\Zotero\\storage\\Q43J8XEA\\Keller en Dahm - 2019 - Integral Equations and Machine Learning.pdf:application/pdf},
}

@misc{gopalakrishna_note_2021,
	title = {A note on {Fredholm} integral equation},
	url = {http://arxiv.org/abs/2106.07194},
	abstract = {This note gives results on the existence of semi-continuous solutions of a Fredholm integral equation of the second kind using Tarski’s ﬁxed point theorem.},
	language = {en},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Gopalakrishna, Chaitanya},
	month = jun,
	year = {2021},
	note = {arXiv:2106.07194 [math]},
	keywords = {Mathematics - Analysis of PDEs, Mathematics - Rings and Algebras, integral equations},
	annote = {Comment: 7 pages},
	file = {Gopalakrishna - 2021 - A note on Fredholm integral equation.pdf:C\:\\Users\\isido\\Zotero\\storage\\BF5MEFDI\\Gopalakrishna - 2021 - A note on Fredholm integral equation.pdf:application/pdf},
}

@article{yang_new_2021,
	title = {A {New} {Fast} {Monte} {Carlo} {Code} for {Solving} {Radiative} {Transfer} {Equations} based on {Neumann} {Solution}},
	volume = {254},
	issn = {0067-0049, 1538-4365},
	url = {http://arxiv.org/abs/2104.07007},
	doi = {10.3847/1538-4365/abec73},
	abstract = {In this paper, we proposed a new Monte Carlo radiative transport (MCRT) scheme, which is based completely on the Neumann series solution of Fredholm integral equation. This scheme indicates that the essence of MCRT is the calculation of inﬁnite terms of multiple integrals in Neumann solution simultaneously. Under this perspective we redescribed MCRT procedure systematically, in which the main work amounts to choose an associated probability distribution function (PDF) for a set of random variables and the corresponding unbiased estimation functions. We can select a relatively optimal estimation procedure that has a lower variance from an inﬁnite possible choices, such as the term by term estimation. In this scheme, MCRT can be regarded as a pure problem of integral evaluation, rather than as the tracing of random walking photons. Keeping this in mind, one can avert some subtle intuitive mistakes. In addition the δ-functions in these integrals can be eliminated in advance by integrating them out directly. This fact together with the optimal chosen random variables can remarkably improve the Monte Carlo (MC) computational eﬃciency and accuracy, especially in systems with axial or spherical symmetry. An MCRT code, Lemona(Linear Integral Equations’ Monte Carlo Solver Based on the Neumann solution), has been developed completely based on this scheme. Finally, we intend to verify the validation of Lemon, a suite of test problems mainly restricted to ﬂat spacetime have been reproduced and the corresponding results are illustrated in detail.},
	language = {en},
	number = {2},
	urldate = {2023-01-10},
	journal = {The Astrophysical Journal Supplement Series},
	author = {Yang, Xiao-lin and Wang, Jian-cheng and Yang, Chu-yuan},
	month = jun,
	year = {2021},
	note = {arXiv:2104.07007 [astro-ph, physics:physics]},
	keywords = {monte carlo, Physics - Computational Physics, Astrophysics - High Energy Astrophysical Phenomena, integral equations},
	pages = {29},
	annote = {Comment: 37 pages, 28 figures. The code can be download from: https://github.com/yangxiaolinyn/Lemon (or https://bitbucket.org/yangxiaolinsc/lemonsourcecode/src/main/) and https://doi.org/10.5281/zenodo.4686355. Comments are welcome},
	file = {2104.07007.pdf:C\:\\Users\\isido\\Zotero\\storage\\5R9X8WF9\\2104.07007.pdf:application/pdf},
}

@misc{ostrovsky_unbiased_2014,
	title = {Unbiased {Monte} {Carlo} estimation for solving of linear integral equation, with error estimate},
	url = {http://arxiv.org/abs/1408.4205},
	abstract = {We oﬀer a new Monte-Carlo method for solving linear integral equation which gives the unbiased estimation for solution of Volterra’s and Fredholm’s type, and consider the problem of conﬁdence region building.},
	language = {en},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Ostrovsky, E. and Sirota, L.},
	month = aug,
	year = {2014},
	note = {arXiv:1408.4205 [math]},
	keywords = {monte carlo, Mathematics - Numerical Analysis, integral equations},
	file = {Ostrovsky en Sirota - 2014 - Unbiased Monte Carlo estimation for solving of lin.pdf:C\:\\Users\\isido\\Zotero\\storage\\UBUR3CE9\\Ostrovsky en Sirota - 2014 - Unbiased Monte Carlo estimation for solving of lin.pdf:application/pdf},
}

@misc{carmon_resqueing_2023,
	title = {{ReSQueing} {Parallel} and {Private} {Stochastic} {Convex} {Optimization}},
	url = {http://arxiv.org/abs/2301.00457},
	abstract = {We introduce a new tool for stochastic convex optimization (SCO): a Reweighted Stochastic Query (ReSQue) estimator for the gradient of a function convolved with a (Gaussian) probability density. Combining ReSQue with recent advances in ball oracle acceleration [CJJ+20, ACJ+21], we develop algorithms achieving state-of-the-art complexities for SCO in parallel and private settings. For a SCO objective constrained to the unit ball in Rd, we obtain the following results (up to polylogarithmic factors).},
	language = {en},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Carmon, Yair and Jambulapati, Arun and Jin, Yujia and Lee, Yin Tat and Liu, Daogao and Sidford, Aaron and Tian, Kevin},
	month = jan,
	year = {2023},
	note = {arXiv:2301.00457 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control, optimization, Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms},
	file = {Carmon e.a. - 2023 - ReSQueing Parallel and Private Stochastic Convex O.pdf:C\:\\Users\\isido\\Zotero\\storage\\JDNVG82B\\Carmon e.a. - 2023 - ReSQueing Parallel and Private Stochastic Convex O.pdf:application/pdf},
}

@misc{he_accelerating_2022,
	title = {Accelerating {Parallel} {Stochastic} {Gradient} {Descent} via {Non}-blocking {Mini}-batches},
	url = {http://arxiv.org/abs/2211.00889},
	abstract = {SOTA decentralized SGD algorithms can overcome the bandwidth bottleneck at the parameter server by using communication collectives like Ring All-Reduce for synchronization. While the parameter updates in distributed SGD may happen asynchronously there is still a synchronization barrier to make sure that the local training epoch at every learner is complete before the learners can advance to the next epoch. The delays in waiting for the slowest learners(stragglers) remain to be a problem in the synchronization steps of these state-of-the-art decentralized frameworks. In this paper, we propose the (de)centralized Non-blocking SGD (Non-blocking SGD) which can address the straggler problem in a heterogeneous environment. The main idea of Non-blocking SGD is to split the original batch into mini-batches, then accumulate the gradients and update the model based on ﬁnished mini-batches. The Non-blocking idea can be implemented using decentralized algorithms including Ring All-reduce, D-PSGD, and MATCHA to solve the straggler problem. Moreover, using gradient accumulation to update the model also guarantees convergence and avoids gradient staleness. Run-time analysis with random straggler delays and computational efﬁciency/throughput of devices is also presented to show the advantage of Non-blocking SGD. Experiments on a suite of datasets and deep learning networks validate the theoretical analyses and demonstrate that Non-blocking SGD speeds up the training and fastens the convergence. Compared with the state-of-the-art decentralized asynchronous algorithms like D-PSGD and MACHA, Non-blocking SGD takes up to 2x fewer time to reach the same training loss in a heterogeneous environment.},
	language = {en},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {He, Haoze and Dube, Parijat},
	month = nov,
	year = {2022},
	note = {arXiv:2211.00889 [cs]},
	keywords = {Computer Science - Machine Learning, optimization, Computer Science - Distributed, Parallel, and Cluster Computing, gradient descent},
	annote = {Comment: 12 pages, 4 figures},
	file = {He en Dube - 2022 - Accelerating Parallel Stochastic Gradient Descent .pdf:C\:\\Users\\isido\\Zotero\\storage\\5TLNEYY3\\He en Dube - 2022 - Accelerating Parallel Stochastic Gradient Descent .pdf:application/pdf},
}

@misc{mohamad_scaling_2022,
	title = {Scaling up {Stochastic} {Gradient} {Descent} for {Non}-convex {Optimisation}},
	url = {http://arxiv.org/abs/2210.02882},
	abstract = {Stochastic gradient descent (SGD) is a widely adopted iterative method for optimizing differentiable objective functions. In this paper, we propose and discuss a novel approach to scale up SGD in applications involving non-convex functions and large datasets. We address the bottleneck problem arising when using both shared and distributed memory. Typically, the former is bounded by limited computation resources and bandwidth whereas the latter suffers from communication overheads. We propose a uniﬁed distributed and parallel implementation of SGD (named DPSGD) that relies on both asynchronous distribution and lock-free parallelism. By combining two strategies into a uniﬁed framework, DPSGD is able to strike a better trade-off between local computation and communication. The convergence properties of DPSGD are studied for non-convex problems such as those arising in statistical modelling and machine learning. Our theoretical analysis shows that DPSGD leads to speed-up with respect to the number of cores a√nd number of workers while guaranteeing an asymptotic convergence rate of O(1/ T ) given that the number of cores is bounded by T 1/4 and the number of workers is bounded by T 1/2 where T is the number of iterations. The potential gains that can be achieved by DPSGD are demonstrated empirically on a stochastic variational inference problem (Latent Dirichlet Allocation) and on a deep reinforcement learning (DRL) problem (advantage actor critic - A2C) resulting in two algorithms: DPSVI and HSA2C. Empirical results validate our theoretical ﬁndings. Comparative studies are conducted to show the performance of the proposed DPSGD against the state-of-the-art DRL algorithms.},
	language = {en},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Mohamad, Saad and Alamri, Hamad and Bouchachia, Abdelhamid},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02882 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control, optimization, Computer Science - Distributed, Parallel, and Cluster Computing, gradient descent},
	file = {Mohamad e.a. - 2022 - Scaling up Stochastic Gradient Descent for Non-con.pdf:C\:\\Users\\isido\\Zotero\\storage\\UJS9IZZ9\\Mohamad e.a. - 2022 - Scaling up Stochastic Gradient Descent for Non-con.pdf:application/pdf},
}

@article{zinkevich_parallelized_nodate,
	title = {Parallelized {Stochastic} {Gradient} {Descent}},
	abstract = {With the increase in available data parallel machine learning has become an increasingly pressing problem. In this paper we present the ﬁrst parallel stochastic gradient descent algorithm including a detailed analysis and experimental evidence. Unlike prior work on parallel optimization algorithms [5, 7] our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints, which might only be available in the multicore setting. Our analysis introduces a novel proof technique — contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits. As a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime [1, 8].},
	language = {en},
	author = {Zinkevich, Martin A and Smola, Alex and Weimer, Markus and Li, Lihong},
	keywords = {optimization, gradient descent},
	file = {Zinkevich e.a. - Parallelized Stochastic Gradient Descent.pdf:C\:\\Users\\isido\\Zotero\\storage\\7SDXTQBD\\Zinkevich e.a. - Parallelized Stochastic Gradient Descent.pdf:application/pdf},
}

@misc{abhishek_gupta_recursive_2020,
	title = {Recursive {Stochastic} {Algorithms}: {A} {Markov} {Chain} {Perspective}},
	shorttitle = {Recursive {Stochastic} {Algorithms}},
	url = {https://www.youtube.com/watch?v=f1IP6rpqaEE},
	abstract = {Creative Commons Attribution license (reuse allowed)},
	urldate = {2023-01-11},
	author = {{Abhishek Gupta}},
	month = oct,
	year = {2020},
	keywords = {optimization},
}

@misc{gupta_convergence_2021,
	title = {Convergence of {Recursive} {Stochastic} {Algorithms} using {Wasserstein} {Divergence}},
	url = {http://arxiv.org/abs/2003.11403},
	abstract = {This paper develops a uniﬁed framework, based on iterated random operator theory, to analyze the convergence of constant stepsize recursive stochastic algorithms (RSAs). RSAs use randomization to eﬃciently compute expectations, and so their iterates form a stochastic process. The key idea of our analysis is to lift the RSA into an appropriate higher-dimensional space and then express it as an equivalent Markov chain. Instead of determining the convergence of this Markov chain (which may not converge under constant stepsize), we study the convergence of the distribution of this Markov chain. To study this, we deﬁne a new notion of Wasserstein divergence. We show that if the distribution of the iterates in the Markov chain satisfy a contraction property with respect to the Wasserstein divergence, then the Markov chain admits an invariant distribution. We show that convergence of a large family of constant stepsize RSAs can be understood using this framework, and we provide several detailed examples.},
	language = {en},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Gupta, Abhishek and Haskell, William B.},
	month = jan,
	year = {2021},
	note = {arXiv:2003.11403 [cs, eess, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Probability, Mathematics - Optimization and Control, Electrical Engineering and Systems Science - Systems and Control, unsorted important},
	annote = {Comment: 34 pages, submitted to SIMODS},
	file = {Gupta en Haskell - 2021 - Convergence of Recursive Stochastic Algorithms usi.pdf:C\:\\Users\\isido\\Zotero\\storage\\IAT28HNB\\Gupta en Haskell - 2021 - Convergence of Recursive Stochastic Algorithms usi.pdf:application/pdf},
}

@article{akhtar_solving_2015,
	title = {{SOLVING} {INITIAL} {VALUE} {ORDINARY} {DIFFERENTIAL} {EQUATIONS} {BY} {MONTE} {CARLO} {METHOD}},
	abstract = {The objective of this paper is to perform a computational analysis of an existing Monte Carlo based algorithm to solve initial value problem of ordinary differential equations (ODEs). Firstly the problems associated with the existing algorithm have been rectified by suggesting a new elaborate algorithm. Then the new algorithm has been applied to solve different types of ODEs including simple, explicit coupled, implicit and coupled system of first order ODEs. Furthermore the same has also been implemented to known physical systems such as Van der Pol equation and SIR epidemic model. The limitations of proposed algorithm have also been identified by applying Lipschitz continuity check for an exemplary ODE. Finally it has been demonstrated that it still very difficult to propose a computationally efficient algorithm to solve ODEs with considerable accuracy using Monte Carlo method.},
	language = {en},
	author = {Akhtar, Muhammad Naveed and Durad, Muhammad Hanif and Ahmed, Asad},
	year = {2015},
	keywords = {monte carlo, ODE},
	file = {Akhtar e.a. - 2015 - SOLVING INITIAL VALUE ORDINARY DIFFERENTIAL EQUATI.pdf:C\:\\Users\\isido\\Zotero\\storage\\WTUKCBSR\\Akhtar e.a. - 2015 - SOLVING INITIAL VALUE ORDINARY DIFFERENTIAL EQUATI.pdf:application/pdf},
}

@misc{noauthor_magnus_2022,
	title = {Magnus expansion},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Magnus_expansion&oldid=1117728056},
	abstract = {In mathematics and physics, the Magnus expansion, named after Wilhelm Magnus (1907–1990), provides an exponential representation of the solution of a first-order homogeneous linear differential equation for a linear operator. In particular, it furnishes the fundamental matrix of a system of linear ordinary differential equations of order n with varying coefficients. The exponent is aggregated as an infinite series, whose terms involve multiple integrals and nested commutators.},
	language = {en},
	urldate = {2023-01-12},
	journal = {Wikipedia},
	month = oct,
	year = {2022},
	note = {Page Version ID: 1117728056},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\WKBPK8ZV\\Magnus_expansion.html:text/html},
}

@article{vaibhav_efficient_2019,
	title = {Efficient {Nonlinear} {Fourier} {Transform} {Algorithms} of {Order} {Four} on {Equispaced} {Grid}},
	volume = {31},
	issn = {1041-1135, 1941-0174},
	url = {http://arxiv.org/abs/1903.11702},
	doi = {10.1109/LPT.2019.2925052},
	abstract = {We explore two classes of exponential integrators in this letter to design nonlinear Fourier transform (NFT) algorithms with a desired accuracy-complexity trade-off and a convergence order of 4 on an equispaced grid. The integrating factor based method in the class of Runge-Kutta methods yield algorithms with complexity O(N log2 N) (where N is the number of samples of the signal) which have superior accuracy-complexity trade-off than any of the fast methods known currently. The integrators based on Magnus series expansion, namely, standard and commutator-free Magnus methods yield algorithms of complexity O(N2) that have superior error behavior even for moderately small step-sizes and higher signal strengths.},
	language = {en},
	number = {15},
	urldate = {2023-01-12},
	journal = {IEEE Photonics Technology Letters},
	author = {Vaibhav, Vishal},
	month = aug,
	year = {2019},
	note = {arXiv:1903.11702 [physics]},
	keywords = {Mathematics - Numerical Analysis, Physics - Computational Physics},
	pages = {1269--1272},
	annote = {Comment: 4 pages},
	file = {Vaibhav - 2019 - Efficient Nonlinear Fourier Transform Algorithms o.pdf:C\:\\Users\\isido\\Zotero\\storage\\Y6ZICUXL\\Vaibhav - 2019 - Efficient Nonlinear Fourier Transform Algorithms o.pdf:application/pdf},
}

@misc{noauthor_bakercampbellhausdorff_2022,
	title = {Baker–{Campbell}–{Hausdorff} formula},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Baker%E2%80%93Campbell%E2%80%93Hausdorff_formula&oldid=1115922051},
	abstract = {In mathematics, the Baker–Campbell–Hausdorff formula is the solution for Z Z to the equation
e X e Y = e Z
\{{\textbackslash}displaystyle e{\textasciicircum}\{X\}e{\textasciicircum}\{Y\}=e{\textasciicircum}\{Z\}\}
for possibly noncommutative X and Y in the Lie algebra of a Lie group. There are various ways of writing the formula, but all ultimately yield an expression for Z Z in Lie algebraic terms, that is, as a formal series (not necessarily convergent) in X X and Y Y and iterated commutators thereof. The first few terms of this series are:
Z = X + Y + 1 2 [ X , Y ] + 1 12 [ X , [ X , Y ] ] − 1 12 [ Y , [ X , Y ] ] + ⋯ ,
\{{\textbackslash}displaystyle Z=X+Y+\{{\textbackslash}frac \{1\}\{2\}\}[X,Y]+\{{\textbackslash}frac \{1\}\{12\}\}[X,[X,Y]]-\{{\textbackslash}frac \{1\}\{12\}\}[Y,[X,Y]]+{\textbackslash}cdots {\textbackslash},,\}
where " ⋯ {\textbackslash}cdots " indicates terms involving higher commutators of X X and Y Y. If X X and Y Y are sufficiently small elements of the Lie algebra g \{{\textbackslash}mathfrak \{g\}\} of a Lie group G G, the series is convergent. Meanwhile, every element g g sufficiently close to the identity in G G can be expressed as g = e X \{{\textbackslash}displaystyle g=e{\textasciicircum}\{X\}\} for a small X X in g \{{\textbackslash}mathfrak \{g\}\}. Thus, we can say that near the identity the group multiplication in G G—written as e X e Y = e Z \{{\textbackslash}displaystyle e{\textasciicircum}\{X\}e{\textasciicircum}\{Y\}=e{\textasciicircum}\{Z\}\}—can be expressed in purely Lie algebraic terms. The Baker–Campbell–Hausdorff formula can be used to give comparatively simple proofs of deep results in the Lie group–Lie algebra correspondence.

If X X and Y Y are sufficiently small n × n n{\textbackslash}times n matrices, then Z Z can be computed as the logarithm of e X e Y \{{\textbackslash}displaystyle e{\textasciicircum}\{X\}e{\textasciicircum}\{Y\}\}, where the exponentials and the logarithm can be computed as power series. The point of the Baker–Campbell–Hausdorff formula is then the highly nonobvious claim that Z := log ⁡ ( e X e Y ) \{{\textbackslash}displaystyle Z:={\textbackslash}log {\textbackslash}left(e{\textasciicircum}\{X\}e{\textasciicircum}\{Y\}{\textbackslash}right)\} can be expressed as a series in repeated commutators of X X and Y Y.

Modern expositions of the formula can be found in, among other places, the books of Rossmann[1] and Hall.[2]},
	language = {en},
	urldate = {2023-01-12},
	journal = {Wikipedia},
	month = oct,
	year = {2022},
	note = {Page Version ID: 1115922051},
	keywords = {exponential integrators},
}

@article{galtier_integral_2013,
	title = {Integral formulation of null-collision {Monte} {Carlo} algorithms},
	volume = {125},
	issn = {00224073},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022407313001350},
	doi = {10.1016/j.jqsrt.2013.04.001},
	abstract = {At the kinetic level, the meaning of null-collisions is straightforward: they correspond to pureforward scattering events. We here discuss their technical signiﬁcance in integral terms. We ﬁrst consider a most standard null-collision Monte Carlo algorithm and show how it can be rigorously justiﬁed starting from a Fredholm equivalent to the radiative transfer equation. Doing so, we also prove that null-collision algorithms can be slightly modiﬁed so that they deal with unexpected occurrences of negative values of the null-collision coeﬃcient (when the upper bound of the heterogeneous extinction coeﬃcient is nonstrict). We then describe technically, in full details, the resulting algorithm, when applied to the evaluation of the local net-power density within a bounded, heterogeneous, multiple scattering and emitting/absorbing medium. The corresponding integral formulation is then explored theoretically in order to distinguish the statistical signiﬁcance of introducing null-collisions from that of the integral-structure underlying modiﬁcation.},
	language = {en},
	urldate = {2023-01-12},
	journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},
	author = {Galtier, M. and Blanco, S. and Caliot, C. and Coustet, C. and Dauchet, J. and El Hafi, M. and Eymet, V. and Fournier, R. and Gautrais, J. and Khuong, A. and Piaud, B. and Terrée, G.},
	month = aug,
	year = {2013},
	keywords = {rendering},
	pages = {57--68},
	file = {Galtier e.a. - 2013 - Integral formulation of null-collision Monte Carlo.pdf:C\:\\Users\\isido\\Zotero\\storage\\7WE6YFYN\\Galtier e.a. - 2013 - Integral formulation of null-collision Monte Carlo.pdf:application/pdf},
}

@article{kutz_spectral_2017,
	title = {Spectral and decomposition tracking for rendering heterogeneous volumes},
	volume = {36},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3072959.3073665},
	doi = {10.1145/3072959.3073665},
	abstract = {We present two novel unbiased techniques for sampling free paths in heterogeneous participating media. Our
              decomposition tracking
              accelerates free-path construction by splitting the medium into a control component and a residual component and sampling each of them separately. To minimize expensive evaluations of spatially varying collision coefficients, we define the control component to allow constructing free paths in closed form. The residual heterogeneous component is then homogenized by adding a fictitious medium and handled using weighted delta tracking, which removes the need for computing strict bounds of the extinction function. Our second contribution,
              spectral tracking
              , enables efficient light transport simulation in chromatic media. We modify free-path distributions to minimize the fluctuation of path throughputs and thereby reduce the estimation variance. To demonstrate the correctness of our algorithms, we derive them
              directly
              from the radiative transfer equation by extending the integral formulation of null-collision algorithms recently developed in reactor physics. This mathematical framework, which we thoroughly review, encompasses existing trackers and postulates an entire family of new estimators for solving transport problems; our algorithms are examples of such. We analyze the proposed methods in canonical settings and on production scenes, and compare to the current state of the art in simulating light transport in heterogeneous participating media.},
	language = {en},
	number = {4},
	urldate = {2023-01-12},
	journal = {ACM Transactions on Graphics},
	author = {Kutz, Peter and Habel, Ralf and Li, Yining Karl and Novák, Jan},
	month = aug,
	year = {2017},
	keywords = {rendering},
	pages = {1--16},
	file = {Kutz e.a. - 2017 - Spectral and decomposition tracking for rendering .pdf:C\:\\Users\\isido\\Zotero\\storage\\49PCP84W\\Kutz e.a. - 2017 - Spectral and decomposition tracking for rendering .pdf:application/pdf},
}

@incollection{billaud-friess_stochastic_2020,
	title = {Stochastic methods for solving high-dimensional partial differential equations},
	volume = {324},
	url = {http://arxiv.org/abs/1905.05423},
	abstract = {We propose algorithms for solving high-dimensional Partial Diﬀerential Equations (PDEs) that combine a probabilistic interpretation of PDEs, through Feynman-Kac representation, with sparse interpolation. Monte-Carlo methods and time-integration schemes are used to estimate pointwise evaluations of the solution of a PDE. We use a sequential control variates algorithm, where control variates are constructed based on successive approximations of the solution of the PDE. Two diﬀerent algorithms are proposed, combining in diﬀerent ways the sequential control variates algorithm and adaptive sparse interpolation. Numerical examples will illustrate the behavior of these algorithms.},
	language = {en},
	urldate = {2023-01-14},
	author = {Billaud-Friess, Marie and Macherey, Arthur and Nouy, Anthony and Prieur, Clémentine},
	year = {2020},
	doi = {10.1007/978-3-030-43465-6_6},
	note = {arXiv:1905.05423 [math]},
	keywords = {PDE, Mathematics - Numerical Analysis},
	pages = {125--141},
	file = {Billaud-Friess e.a. - 2020 - Stochastic methods for solving high-dimensional pa.pdf:C\:\\Users\\isido\\Zotero\\storage\\9PRXYL9X\\Billaud-Friess e.a. - 2020 - Stochastic methods for solving high-dimensional pa.pdf:application/pdf},
}

@misc{leluc_quadrature_2022,
	title = {A {Quadrature} {Rule} combining {Control} {Variates} and {Adaptive} {Importance} {Sampling}},
	url = {http://arxiv.org/abs/2205.11890},
	abstract = {Driven by several successful applications such as in stochastic gradient descent or in Bayesian computation, control variates have become a major tool for Monte Carlo integration. However, standard methods do not allow the distribution of the particles to evolve during the algorithm, as is the case in sequential simulation methods. Within the standard adaptive importance sampling framework, a simple weighted least squares approach is proposed to improve the procedure with control variates. The procedure takes the form of a quadrature rule with adapted quadrature weights to reﬂect the information brought in by the control variates. The quadrature points and weights do not depend on the integrand, a computational advantage in case of multiple integrands. Moreover, the target density needs to be known only up to a multiplicative constant. Our main result is a non-asymptotic bound on the probabilistic error of the procedure. The bound proves that for improving the estimate’s accuracy, the beneﬁts from adaptive importance sampling and control variates can be combined. The good behavior of the method is illustrated empirically on synthetic examples and real-world data for Bayesian linear regression.},
	language = {en},
	urldate = {2023-01-14},
	publisher = {arXiv},
	author = {Leluc, Rémi and Portier, François and Segers, Johan and Zhuman, Aigerim},
	month = oct,
	year = {2022},
	note = {arXiv:2205.11890 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Statistics Theory, importance sampling, integration, control variates},
	file = {Leluc e.a. - 2022 - A Quadrature Rule combining Control Variates and A.pdf:C\:\\Users\\isido\\Zotero\\storage\\BVHY6BXX\\Leluc e.a. - 2022 - A Quadrature Rule combining Control Variates and A.pdf:application/pdf},
}

@misc{fu_convergence_2022,
	title = {Convergence analysis of a quasi-{Monte} {Carlo}-based deep learning algorithm for solving partial differential equations},
	url = {http://arxiv.org/abs/2210.16196},
	abstract = {Deep learning methods have achieved great success in solving partial diﬀerential equations (PDEs), where the loss is often deﬁned as an integral. The accuracy and eﬃciency of these algorithms depend greatly on the quadrature method. We propose to apply quasi-Monte Carlo (QMC) methods to the Deep Ritz Method (DRM) for solving the Neumann problems for the Poisson equation and the static Schr¨odinger equation. For error estimation, we decompose the error of using the deep learning algorithm to solve PDEs into the generalization error, the approximation error and the training error. We establish the upper bounds and prove that QMC-based DRM achieves an asymptotically smaller error bound than DRM. Numerical experiments show that the proposed method converges faster in all cases and the variances of the gradient estimators of randomized QMC-based DRM are much smaller than those of DRM, which illustrates the superiority of QMC in deep learning over MC.},
	language = {en},
	urldate = {2023-01-14},
	publisher = {arXiv},
	author = {Fu, Fengjiang and Wang, Xiaoqun},
	month = oct,
	year = {2022},
	note = {arXiv:2210.16196 [cs, math]},
	keywords = {Computer Science - Machine Learning, monte carlo, Mathematics - Numerical Analysis},
	annote = {Comment: 27 pages, 4 figures, 2 tables},
	file = {Fu en Wang - 2022 - Convergence analysis of a quasi-Monte Carlo-based .pdf:C\:\\Users\\isido\\Zotero\\storage\\SAFYTMXD\\Fu en Wang - 2022 - Convergence analysis of a quasi-Monte Carlo-based .pdf:application/pdf},
}

@article{wang_stochastic_2022,
	title = {Stochastic {Trust} {Region} {Methods} with {Trust} {Region} {Radius} {Depending} on {Probabilistic} {Models}},
	volume = {40},
	issn = {0254-9409, 1991-7139},
	url = {http://arxiv.org/abs/1904.03342},
	doi = {10.4208/jcm.2012-m2020-0144},
	abstract = {We present a stochastic trust-region model-based framework in which its radius is related to the probabilistic models. Especially, we propose a speciﬁc algorithm, termed STRME, in which the trust-region radius depends linearly on the latest model gradient. The complexity of STRME method in non-convex, convex and strongly convex settings has all been analyzed, which matches the existing algorithms based on probabilistic properties. In addition, several numerical experiments are carried out to reveal the beneﬁts of the proposed methods compared to the existing stochastic trust-region methods and other relevant stochastic gradient methods.},
	language = {en},
	number = {2},
	urldate = {2023-01-15},
	journal = {Journal of Computational Mathematics},
	author = {Wang, Xiaoyu and Yuan, Ya-xiang},
	month = jun,
	year = {2022},
	note = {arXiv:1904.03342 [math]},
	keywords = {Mathematics - Optimization and Control, optimization},
	pages = {295--336},
	file = {Wang en Yuan - 2022 - Stochastic Trust Region Methods with Trust Region .pdf:C\:\\Users\\isido\\Zotero\\storage\\78W97RZH\\Wang en Yuan - 2022 - Stochastic Trust Region Methods with Trust Region .pdf:application/pdf},
}

@misc{fang_fully_2022,
	title = {Fully {Stochastic} {Trust}-{Region} {Sequential} {Quadratic} {Programming} for {Equality}-{Constrained} {Optimization} {Problems}},
	url = {http://arxiv.org/abs/2211.15943},
	abstract = {We propose a trust-region stochastic sequential quadratic programming algorithm (TR-StoSQP) to solve nonlinear optimization problems with stochastic objectives and deterministic equality constraints. We consider a fully stochastic setting, where in each iteration a single sample is generated to estimate the objective gradient. The algorithm adaptively selects the trust-region radius and, compared to the existing line-search StoSQP schemes, allows us to employ indeﬁnite Hessian matrices (i.e., Hessians without modiﬁcation) in SQP subproblems. As a trust-region method for constrained optimization, our algorithm needs to address an infeasibility issue—the linearized equality constraints and trust-region constraints might lead to infeasible SQP subproblems. In this regard, we propose an adaptive relaxation technique to compute the trial step that consists of a normal step and a tangential step. To control the lengths of the two steps, we adaptively decompose the trust-region radius into two segments based on the proportions of the feasibility and optimality residuals to the full KKT residual. The normal step has a closed form, while the tangential step is solved from a trust-region subproblem, to which a solution ensuring the Cauchy reduction is suﬃcient for our study. We establish the global almost sure convergence guarantee for TR-StoSQP, and illustrate its empirical performance on both a subset of problems in the CUTEst test set and constrained logistic regression problems using data from the LIBSVM collection.},
	language = {en},
	urldate = {2023-01-15},
	publisher = {arXiv},
	author = {Fang, Yuchen and Na, Sen and Mahoney, Michael W. and Kolar, Mladen},
	month = nov,
	year = {2022},
	note = {arXiv:2211.15943 [math, stat]},
	keywords = {Statistics - Machine Learning, Mathematics - Optimization and Control, Statistics - Computation, optimization},
	annote = {Comment: 6 figures, 28 pages},
	file = {Fang e.a. - 2022 - Fully Stochastic Trust-Region Sequential Quadratic.pdf:C\:\\Users\\isido\\Zotero\\storage\\HIZ86IVL\\Fang e.a. - 2022 - Fully Stochastic Trust-Region Sequential Quadratic.pdf:application/pdf},
}

@misc{curtis_fully_2019,
	title = {A {Fully} {Stochastic} {Second}-{Order} {Trust} {Region} {Method}},
	url = {http://arxiv.org/abs/1911.06920},
	abstract = {A stochastic second-order trust region method is proposed, which can be viewed as a second-order extension of the trust-region-ish (TRish) algorithm proposed by Curtis et al. [INFORMS J. Optim. 1(3) 200–220, 2019]. In each iteration, a search direction is computed by (approximately) solving a trust region subproblem deﬁned by stochastic gradient and Hessian estimates. The algorithm has convergence guarantees for stochastic minimization in the fully stochastic regime, meaning that guarantees hold when each stochastic gradient is required merely to be an unbiased estimate of the true gradient with bounded variance and when the stochastic Hessian estimates are bounded uniformly in norm. The algorithm is also equipped with a worst-case complexity guarantee in the nearly deterministic regime, i.e., when the stochastic gradient and Hessian estimates are very close in expectation to the true gradients and Hessians. The results of numerical experiments for training convolutional neural networks for image classiﬁcation and training a recurrent neural network for time series forecasting are presented. These results show that the algorithm can outperform a stochastic gradient approach and the ﬁrst-order TRish algorithm in practice.},
	language = {en},
	urldate = {2023-01-15},
	publisher = {arXiv},
	author = {Curtis, Frank E. and Shi, Rui},
	month = nov,
	year = {2019},
	note = {arXiv:1911.06920 [math]},
	keywords = {Mathematics - Optimization and Control, optimization},
	file = {Curtis en Shi - 2019 - A Fully Stochastic Second-Order Trust Region Metho.pdf:C\:\\Users\\isido\\Zotero\\storage\\3QTRRJEB\\Curtis en Shi - 2019 - A Fully Stochastic Second-Order Trust Region Metho.pdf:application/pdf},
}

@article{chauhan_stochastic_2020,
	title = {Stochastic {Trust} {Region} {Inexact} {Newton} {Method} for {Large}-scale {Machine} {Learning}},
	volume = {11},
	issn = {1868-8071, 1868-808X},
	url = {http://arxiv.org/abs/1812.10426},
	doi = {10.1007/s13042-019-01055-9},
	abstract = {Nowadays stochastic approximation methods are one of the major research direction to deal with the large-scale machine learning problems. From stochastic ﬁrst order methods, now the focus is shifting to stochastic second order methods due to their faster convergence and availability of computing resources. In this paper, we have proposed a novel Stochastic Trust RegiOn Inexact Newton method, called as STRON, to solve large-scale learning problems which uses conjugate gradient (CG) to inexactly solve trust region subproblem. The method uses progressive subsampling in the calculation of gradient and Hessian values to take the advantage of both, stochastic and full-batch regimes. We have extended STRON using existing variance reduction techniques to deal with the noisy gradients and using preconditioned conjugate gradient (PCG) as subproblem solver, and empirically proved that they do not work as expected, for the large-scale learning problems. Finally, our empirical results prove eﬃcacy of the proposed method against existing methods with bench marked datasets.},
	language = {en},
	number = {7},
	urldate = {2023-01-15},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Chauhan, Vinod Kumar and Sharma, Anuj and Dahiya, Kalpana},
	month = jul,
	year = {2020},
	note = {arXiv:1812.10426 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, optimization},
	pages = {1541--1555},
	annote = {Comment: 32 figures, accepted in International Journal of Machine Learning and Cybernetics},
	file = {Chauhan e.a. - 2020 - Stochastic Trust Region Inexact Newton Method for .pdf:C\:\\Users\\isido\\Zotero\\storage\\7YWNREP2\\Chauhan e.a. - 2020 - Stochastic Trust Region Inexact Newton Method for .pdf:application/pdf},
}

@misc{stand-up_maths_how_2021,
	title = {How does {Dobble} ({Spot} {It}) work?},
	url = {https://www.youtube.com/watch?v=VTDKqW_GLkw},
	urldate = {2023-01-16},
	author = {{Stand-up Maths}},
	month = apr,
	year = {2021},
}

@misc{noauthor_aitkens_2022,
	title = {Aitken's delta-squared process},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Aitken%27s_delta-squared_process&oldid=1115576594},
	abstract = {In numerical analysis, Aitken's delta-squared process or Aitken extrapolation is a series acceleration method, used for accelerating the rate of convergence of a sequence. It is named after Alexander Aitken, who introduced this method in 1926. Its early form was known to Seki Kōwa (end of 17th century) and was found for rectification of the circle, i.e. the calculation of π. It is most useful for accelerating the convergence of a sequence that is converging linearly.},
	language = {en},
	urldate = {2023-01-16},
	journal = {Wikipedia},
	month = oct,
	year = {2022},
	note = {Page Version ID: 1115576594},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\QPP3CBBR\\Aitken's_delta-squared_process.html:text/html},
}

@misc{noauthor_runges_2022,
	title = {Runge's phenomenon},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Runge%27s_phenomenon&oldid=1127648067},
	abstract = {In the mathematical field of numerical analysis, Runge's phenomenon (German: [ˈʁʊŋə]) is a problem of oscillation at the edges of an interval that occurs when using polynomial interpolation with polynomials of high degree over a set of equispaced interpolation points. It was discovered by Carl David Tolmé Runge (1901) when exploring the behavior of errors when using polynomial interpolation to approximate certain functions.
The discovery was important because it shows that going to higher degrees does not always improve accuracy. The phenomenon is similar to the Gibbs phenomenon in Fourier series approximations.},
	language = {en},
	urldate = {2023-01-18},
	journal = {Wikipedia},
	month = dec,
	year = {2022},
	note = {Page Version ID: 1127648067},
}

@misc{noauthor_gibbs_2022,
	title = {Gibbs phenomenon},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Gibbs_phenomenon&oldid=1120811333},
	abstract = {In mathematics, the Gibbs phenomenon, discovered by Henry Wilbraham (1848)  and rediscovered by J. Willard Gibbs (1899), is the oscillatory behavior of the Fourier series of a piecewise continuously differentiable periodic function around a jump discontinuity. The function's 
  
    
      
        N
      
    
    \{{\textbackslash}displaystyle N\}
  th partial Fourier series (formed by summing its 
  
    
      
        N
      
    
    \{{\textbackslash}displaystyle N\}
   lowest constituent sinusoids) produces large peaks around the jump which overshoot and undershoot the function's actual values. This approximation error approaches a limit of about 9\% of the jump as more sinusoids are used, though the infinite Fourier series sum does eventually converge almost everywhere except the point of discontinuity.The Gibbs phenomenon was observed by experimental physicists, but was believed to be due to imperfections in the measuring apparatus, and it is one cause of ringing artifacts in signal processing.},
	language = {en},
	urldate = {2023-01-18},
	journal = {Wikipedia},
	month = nov,
	year = {2022},
	note = {Page Version ID: 1120811333},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\H38JL4E9\\Gibbs_phenomenon.html:text/html},
}

@misc{penent_numerical_2022,
	title = {Numerical evaluation of {ODE} solutions by {Monte} {Carlo} enumeration of {Butcher} series},
	url = {http://arxiv.org/abs/2201.05998},
	abstract = {We present an algorithm for the numerical solution of ordinary diﬀerential equations by random enumeration of the Butcher trees used in the implementation of the RungeKutta method. Our Monte Carlo scheme allows for the direct numerical evaluation of an ODE solution at any given time within a certain interval, without iteration through multiple time steps. In particular, this approach does not involve a discretization step size, and it does not require the truncation of Taylor series.},
	language = {en},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Penent, Guillaume and Privault, Nicolas},
	month = aug,
	year = {2022},
	note = {arXiv:2201.05998 [cs, math]},
	keywords = {monte carlo, Mathematics - Probability, Mathematics - Numerical Analysis, ODE, green function},
	file = {Penent en Privault - 2022 - Numerical evaluation of ODE solutions by Monte Car.pdf:C\:\\Users\\isido\\Zotero\\storage\\UXRFJ68P\\Penent en Privault - 2022 - Numerical evaluation of ODE solutions by Monte Car.pdf:application/pdf},
}

@article{mori_numerical_2006,
	title = {Numerical {Green}’s function method based on the {DE} transformation},
	volume = {23},
	issn = {0916-7005, 1868-937X},
	url = {http://link.springer.com/10.1007/BF03167550},
	doi = {10.1007/BF03167550},
	abstract = {A method for numerical solution of boundary value problems with ordinary diﬀerential equation based on the method of Green’s function incorporated with the double exponential transformation is presented. The method proposed does not require solving a system of linear equations and gives an approximate solution of very high accuracy with a small number of function evaluations. The error of the method is O (exp (−C1N/ log(C2N ))) where N is a parameter representing the number of function evaluations and C1 and C2 are some positive constants. Numerical examples also prove the high eﬃciency of the method. An alternative method via an integral equation is presented which can be used when the Green’s function corresponding to the given equation is not available.},
	language = {en},
	number = {2},
	urldate = {2023-01-20},
	journal = {Japan Journal of Industrial and Applied Mathematics},
	author = {Mori, Masatake and Echigo, Toshihiko},
	month = jun,
	year = {2006},
	keywords = {green function},
	pages = {193--205},
	file = {Mori en Echigo - 2006 - Numerical Green’s function method based on the DE .pdf:C\:\\Users\\isido\\Zotero\\storage\\U5ZLG7FE\\Mori en Echigo - 2006 - Numerical Green’s function method based on the DE .pdf:application/pdf},
}

@article{ermakov_monte_2019,
	title = {Monte {Carlo} {Method} for {Solving} {ODE} {Systems}},
	volume = {52},
	issn = {1063-4541, 1934-7855},
	url = {https://link.springer.com/10.1134/S1063454119030087},
	doi = {10.1134/S1063454119030087},
	abstract = {The Monte Carlo method is applied to solve Cauchy problems for a system of linear and nonlinear ordinary differential equations. The Monte Carlo method is relevant for the solution of large systems of equations and in the case of small smoothness of initial functions. In this case, the system is reduced to an equivalent system of integral equations of the Volterra type. For linear systems, this transformation allows removing constraints connected with a convergence of a majorizing process. Examples of estimates of solution functionals are provided, and a behavior of their variances are discussed. In the general case, a solution interval is divided into finite subintervals, on which the nonlinear function is approximated by a polynomial. The obtained integral equation is solved by using branched Markov chains with absorption. Algorithm parallelization problems arising in this case are discussed in this paper. A one-dimensional cubic equation is considered as an example. A choice of transition densities of branching is discussed. A method of generations is described in detail. Numerical results are compared with a solution obtained by the Runge–Kutta method.},
	language = {en},
	number = {3},
	urldate = {2023-01-20},
	journal = {Vestnik St. Petersburg University, Mathematics},
	author = {Ermakov, S. M. and Tovstik, T. M.},
	month = jul,
	year = {2019},
	keywords = {monte carlo, ODE},
	pages = {272--280},
	file = {Ermakov en Tovstik - 2019 - Monte Carlo Method for Solving ODE Systems.pdf:C\:\\Users\\isido\\Zotero\\storage\\UM598GXH\\Ermakov en Tovstik - 2019 - Monte Carlo Method for Solving ODE Systems.pdf:application/pdf},
}

@article{ermakov_monte_2021,
	title = {The {Monte} {Carlo} {Method} for {Solving} {Large} {Systems} of {Linear} {Ordinary} {Differential} {Equations}},
	volume = {54},
	issn = {1063-4541, 1934-7855},
	url = {https://link.springer.com/10.1134/S1063454121010064},
	doi = {10.1134/S1063454121010064},
	abstract = {The Monte Carlo method to solve the Cauchy problem for large systems of linear differential equations is proposed in this paper. Firstly, a quick overview of previously obtained results from applying the approach towards the Fredholm-type integral equations is made. In the main part of the paper, the method is applied towards a linear ODE system that is transformed into an equivalent system of the Volterra-type integral equations, which makes it possible to remove the limitations due to the conditions of convergence of the majorant series. The following key theorems are stated. Theorem 1 provides the necessary compliance conditions that should be imposed upon the transition propability and initial distribution densities that initiate the corresponding Markov chain, for which equality between the mathematical expectation of the estimate and the functional of interest would hold. Theorem 2 formulates the equation that governs the estimate’s variance. Theorem 3 states the Markov chain parameters that minimize the variance of the estimate of the functional. Proofs are given for all three theorems. In the practical part of this paper, the proposed method is used to solve a linear ODE system that describes a closed queueing system of ten conventional machines and seven conventional service persons. The solutions are obtained for systems with both constant and time-dependent matrices of coefficients, where the machine breakdown intensity is time dependent. In addition, the solutions obtained by the Monte Carlo and Runge–Kutta methods are compared. The results are presented in the corresponding tables.},
	language = {en},
	number = {1},
	urldate = {2023-01-20},
	journal = {Vestnik St. Petersburg University, Mathematics},
	author = {Ermakov, S. M. and Smilovitskiy, M. G.},
	month = jan,
	year = {2021},
	keywords = {monte carlo, linear systems},
	pages = {28--38},
	file = {Ermakov en Smilovitskiy - 2021 - The Monte Carlo Method for Solving Large Systems o.pdf:C\:\\Users\\isido\\Zotero\\storage\\EXRTHDXP\\Ermakov en Smilovitskiy - 2021 - The Monte Carlo Method for Solving Large Systems o.pdf:application/pdf},
}

@article{halton_sequential_nodate,
	title = {Sequential {Monte} {Carlo} techniques for solving non-linear systems},
	abstract = {Given a system of m equations F(x ) = 0 (where m is large and x is an unknown m-vector), we seek to apply sequential Monte Carlo [SMC] methods to find solutions efficiently. This paper follows up on a previous paper by the same author, in which consideration was limited to linear systems of the form Ax = a (where, again, m is large, A is a known (m¥m) matrix, a is a known m-vector, and x is an unknown m-vector). It was shown there that effective techniques could reduce computation times dramatically (speed-up factors of 550 to 26,000 were obtained in sample calculations).},
	language = {en},
	journal = {Linear Systems},
	author = {Halton, John H},
	keywords = {monte carlo, nonlinear systems},
	file = {Halton - Sequential Monte Carlo techniques for solving non-.pdf:C\:\\Users\\isido\\Zotero\\storage\\GNEKPJHM\\Halton - Sequential Monte Carlo techniques for solving non-.pdf:application/pdf},
}

@article{nekrutkin_direct_1974,
	title = {Direct and conjugate {Neumann}-{Ulam} schemes for solving non-linear integral equations},
	volume = {14},
	issn = {00415553},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0041555374901670},
	doi = {10.1016/0041-5553(74)90167-0},
	abstract = {ELEMENTARY unbiased estimates are constructed for a linear functional of the solution of a non-linear integral equation of fairly general type. The method of constructing the estimates, which is based on the “equivalence” of the initial equation to an infinite system of linear equations, makes it possible to transfer to the so-called conjugate Neumann-Ulam scheme, which can prove more advantageous when solving physical problems.},
	language = {en},
	number = {6},
	urldate = {2023-01-20},
	journal = {USSR Computational Mathematics and Mathematical Physics},
	author = {Nekrutkin, V.V.},
	month = jan,
	year = {1974},
	pages = {39--45},
	file = {Nekrutkin - 1974 - Direct and conjugate Neumann-Ulam schemes for solv.pdf:C\:\\Users\\isido\\Zotero\\storage\\48XNIP74\\Nekrutkin - 1974 - Direct and conjugate Neumann-Ulam schemes for solv.pdf:application/pdf},
}

@misc{noauthor_neyman-ulam_nodate,
	title = {{THE} {NEYMAN}-{ULAM} {SCHEME} {IN} {THE} {NON}-{LINEAR} {CASE}*},
	keywords = {monte carlo},
	file = {1-s2.0-0041555373900980-main.pdf:C\:\\Users\\isido\\Zotero\\storage\\SSN79QHC\\1-s2.0-0041555373900980-main.pdf:application/pdf},
}

@misc{noauthor_exponential_2022,
	title = {Exponential integrator},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Exponential_integrator&oldid=1123597413},
	abstract = {Exponential integrators are a class of numerical methods for the solution of ordinary differential equations, specifically initial value problems.  This large class of methods from numerical analysis is based on the exact integration of the linear part of the initial value problem. Because the linear part is integrated exactly, this can help to mitigate the stiffness of a differential equation. Exponential integrators can be constructed to be explicit or implicit for numerical ordinary differential equations or serve as the time integrator for numerical partial differential equations.},
	language = {en},
	urldate = {2023-01-22},
	journal = {Wikipedia},
	month = nov,
	year = {2022},
	note = {Page Version ID: 1123597413},
	keywords = {exponential integrators},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\XH6LIQZ3\\Exponential_integrator.html:text/html},
}

@article{hochbruck_exponential_2005,
	title = {Exponential {Runge}–{Kutta} methods for parabolic problems},
	volume = {53},
	issn = {01689274},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168927404001400},
	doi = {10.1016/j.apnum.2004.08.005},
	abstract = {The aim of this paper is to construct exponential Runge-Kutta methods of collocation type and to analyze their convergence properties for linear and semilinear parabolic problems. For the analysis, an abstract Banach space framework of sectorial operators and locally Lipschitz continuous nonlinearities is chosen. This framework includes interesting examples like reaction-diﬀusion equations. It is shown that the methods converge at least with their stage order, and that convergence of higher order (up to the classical order) occurs, if the problem has suﬃcient temporal and spatial smoothness. The latter, however, might require the source function to fulﬁl unnatural boundary conditions. Therefore, the classical order is not always obtained and an order reduction must be expected, in general.},
	language = {en},
	number = {2-4},
	urldate = {2023-01-22},
	journal = {Applied Numerical Mathematics},
	author = {Hochbruck, Marlis and Ostermann, Alexander},
	month = may,
	year = {2005},
	keywords = {ODE, exponential integrators},
	pages = {323--339},
	file = {Hochbruck en Ostermann - 2005 - Exponential Runge–Kutta methods for parabolic prob.pdf:C\:\\Users\\isido\\Zotero\\storage\\NHR96XVY\\Hochbruck en Ostermann - 2005 - Exponential Runge–Kutta methods for parabolic prob.pdf:application/pdf},
}

@article{hochbruck_exponential_2010,
	title = {Exponential integrators},
	volume = {19},
	issn = {0962-4929, 1474-0508},
	url = {https://www.cambridge.org/core/product/identifier/S0962492910000048/type/journal_article},
	doi = {10.1017/S0962492910000048},
	abstract = {In this paper we consider the construction, analysis, implementation and application of exponential integrators. The focus will be on two types of stiff problems. The first one is characterized by a Jacobian that possesses eigenvalues with large negative real parts. Parabolic partial differential equations and their spatial discretization are typical examples. The second class consists of highly oscillatory problems with purely imaginary eigenvalues of large modulus. Apart from motivating the construction of exponential integrators for various classes of problems, our main intention in this article is to present the mathematics behind these methods. We will derive error bounds that are independent of stiffness or highest frequencies in the system.
            Since the implementation of exponential integrators requires the evaluation of the product of a matrix function with a vector, we will briefly discuss some possible approaches as well. The paper concludes with some applications, in which exponential integrators are used.},
	language = {en},
	urldate = {2023-01-23},
	journal = {Acta Numerica},
	author = {Hochbruck, Marlis and Ostermann, Alexander},
	month = may,
	year = {2010},
	keywords = {ODE, exponential integrators},
	pages = {209--286},
	file = {Hochbruck en Ostermann - 2010 - Exponential integrators.pdf:C\:\\Users\\isido\\Zotero\\storage\\C73HIPVU\\Hochbruck en Ostermann - 2010 - Exponential integrators.pdf:application/pdf},
}

@article{bergamaschi_efficient_2000,
	title = {Efficient computation of the exponential operator for large, sparse, symmetric matrices},
	volume = {7},
	issn = {1070-5325, 1099-1506},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1099-1506(200001/02)7:1<27::AID-NLA185>3.0.CO;2-4},
	doi = {10.1002/(SICI)1099-1506(200001/02)7:1<27::AID-NLA185>3.0.CO;2-4},
	abstract = {In this paper we compare Krylov subspace methods with Chebyshev series expansion for approximating the matrix exponential operator on large, sparse, symmetric matrices. Experimental results upon negative-deﬁnite matrices with very large size, arising from (2D and 3D) FE and FD spatial discretization of linear parabolic PDEs, demonstrate that the Chebyshev method can be an effective alternative to Krylov techniques, especially when memory bounds do not allow the storage of all Ritz vectors. We discuss also sensitivity of Chebyshev convergence to extreme eigenvalue approximation, as well as reliability of various a priori and a posteriori error estimates for both methods.},
	language = {en},
	number = {1},
	urldate = {2023-01-23},
	journal = {Numerical Linear Algebra with Applications},
	author = {Bergamaschi, Luca and Vianello, Marco},
	month = jan,
	year = {2000},
	keywords = {exponential integrators},
	pages = {27--45},
	file = {Bergamaschi en Vianello - 2000 - Efficient computation of the exponential operator .pdf:C\:\\Users\\isido\\Zotero\\storage\\5HP78YEJ\\Bergamaschi en Vianello - 2000 - Efficient computation of the exponential operator .pdf:application/pdf},
}

@misc{noauthor_pade_2022,
	title = {Padé approximant},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Pad%C3%A9_approximant&oldid=1123396275},
	abstract = {In mathematics, a Padé approximant is the "best" approximation of a function near a specific point by a rational function of given order. Under this technique, the approximant's power series agrees with the power series of the function it is approximating.  The technique was developed around 1890 by Henri Padé, but goes back to Georg Frobenius, who introduced the idea and investigated the features of rational approximations of power series.
The Padé approximant often gives better approximation of the function than truncating its Taylor series, and it may still work where the Taylor series does not converge. For these reasons Padé approximants are used extensively in computer calculations. They have also been used as auxiliary functions in Diophantine approximation and transcendental number theory, though for sharp results ad hoc methods— in some sense inspired by the Padé theory— typically replace them. Since Padé approximant is a rational function, an artificial singular point may occur as an approximation, but this can be avoided by Borel–Padé analysis.
The reason why the Padé approximant tends to be a better approximation than a truncating Taylor series is clear from the viewpoint of the multi-point summation method. Since there are many cases in which the asymptotic expansion at infinity becomes 0 or a constant, it can be interpreted as the "incomplete two-point Padé approximation", in which the ordinary Padé approximation improves the method truncating a Taylor series.},
	language = {en},
	urldate = {2023-01-23},
	journal = {Wikipedia},
	month = nov,
	year = {2022},
	note = {Page Version ID: 1123396275},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\KVILJA4H\\Padé_approximant.html:text/html},
}

@article{sastre_accurate_2014,
	title = {Accurate and efficient matrix exponential computation},
	volume = {91},
	issn = {0020-7160, 1029-0265},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00207160.2013.791392},
	doi = {10.1080/00207160.2013.791392},
	abstract = {This work gives a new formula for the forward relative error of matrix exponential Taylor approximation and proposes new bounds for it depending on the matrix size and the Taylor approximation order, providing a new efficient scaling and squaring Taylor algorithm for the matrix exponential. A Matlab version of the new algorithm is provided and compared with Pade  ́ state-of-the-art algorithms obtaining higher accuracy in the majority of tests at similar or even lower cost.},
	language = {en},
	number = {1},
	urldate = {2023-01-23},
	journal = {International Journal of Computer Mathematics},
	author = {Sastre, J. and Ibáñez, J. and Ruiz, P. and Defez, E.},
	month = jan,
	year = {2014},
	keywords = {exponential integrators},
	pages = {97--112},
	file = {Sastre e.a. - 2014 - Accurate and efficient matrix exponential computat.pdf:C\:\\Users\\isido\\Zotero\\storage\\QEY4VDFZ\\Sastre e.a. - 2014 - Accurate and efficient matrix exponential computat.pdf:application/pdf},
}

@misc{noauthor_matrix_2022,
	title = {Matrix exponential},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Matrix_exponential&oldid=1122134034},
	abstract = {In mathematics, the matrix exponential is a matrix function on square matrices analogous to the ordinary exponential function. It is used to solve systems of linear differential equations. In the theory of Lie groups, the matrix exponential gives the exponential map between a matrix Lie algebra and the corresponding Lie group.
Let X  be an n×n real or complex matrix. The exponential of X, denoted by eX or exp(X), is the n×n matrix given by the power series

where 
  
    
      
        
          X
          
            0
          
        
      
    
    \{{\textbackslash}displaystyle X{\textasciicircum}\{0\}\}
   is defined to be the identity matrix 
  
    
      
        I
      
    
    \{{\textbackslash}displaystyle I\}
   with the same dimensions as 
  
    
      
        X
      
    
    \{{\textbackslash}displaystyle X\}
  .The above series always converges, so the exponential of X is well-defined. If X is a 1×1 matrix the matrix exponential of X is a 1×1 matrix whose single element is the ordinary exponential of the single element of X.},
	language = {en},
	urldate = {2023-01-23},
	journal = {Wikipedia},
	month = nov,
	year = {2022},
	note = {Page Version ID: 1122134034},
	keywords = {exponential integrators},
}

@article{ermakov_backward_2022,
	title = {Backward {Iterations} for {Solving} {Integral} {Equations} with {Polynomial} {Nonlinearity}},
	volume = {55},
	issn = {1063-4541, 1934-7855},
	url = {https://link.springer.com/10.1134/S1063454122010046},
	doi = {10.1134/S1063454122010046},
	abstract = {The theory of adjoint operators is widely used in solving applied multidimensional problems with the Monte Carlo method. Efficient algorithms are constructed using the duality principle for many problems described in linear integral equations of the second kind. On the other hand, important applications of adjoint equations for designing experiments were suggested by G.I. Marchuk and his colleagues in their respective works. Some results obtained in these fields are also generalized to the case of nonlinear operators. Linearization methods are mostly used for that purpose. The results for Lyapunov–Schmidt nonlinear polynomial equations are obtained in the theory of Monte Carlo methods. However, many interesting questions in this subject area remain open. New results about dual processes used for solving polynomial equations with the Monte Carlo method are presented. In particular, the adjoint Markov process for the branching process and corresponding unbiased estimate of the functional of the solution to the equation are constructed in the general form. The possibility of constructing an adjoint operator to a nonlinear one is discussed.},
	language = {en},
	number = {1},
	urldate = {2023-01-24},
	journal = {Vestnik St. Petersburg University, Mathematics},
	author = {Ermakov, S. M. and Surovikina, T. O.},
	month = mar,
	year = {2022},
	keywords = {integral equations},
	pages = {16--26},
	file = {Ermakov en Surovikina - 2022 - Backward Iterations for Solving Integral Equations.pdf:C\:\\Users\\isido\\Zotero\\storage\\MGBUUSKA\\Ermakov en Surovikina - 2022 - Backward Iterations for Solving Integral Equations.pdf:application/pdf},
}

@misc{li_quantum_2023,
	title = {Quantum {Monte} {Carlo} algorithm for solving {Black}-{Scholes} {PDEs} for high-dimensional option pricing in finance and its proof of overcoming the curse of dimensionality},
	url = {http://arxiv.org/abs/2301.09241},
	abstract = {In this paper we provide a quantum Monte Carlo algorithm to solve high-dimensional Black-Scholes PDEs with correlation for high-dimensional option pricing. The payoﬀ function of the option is of general form and is only required to be continuous and piece-wise aﬃne (CPWA), which covers most of the relevant payoﬀ functions used in ﬁnance. We provide a rigorous error analysis and complexity analysis of our algorithm. In particular, we prove that the computational complexity of our algorithm is bounded polynomially in the space dimension d of the PDE and the reciprocal of the prescribed accuracy ε and so demonstrate that our quantum Monte Carlo algorithm does not suﬀer from the curse of dimensionality.},
	language = {en},
	urldate = {2023-01-24},
	publisher = {arXiv},
	author = {Li, Yongming and Neufeld, Ariel},
	month = jan,
	year = {2023},
	note = {arXiv:2301.09241 [quant-ph, q-fin]},
	keywords = {monte carlo, Quantitative Finance - Computational Finance, Mathematics - Numerical Analysis, ODE, Quantitative Finance - Mathematical Finance, Quantum Physics},
	file = {Li en Neufeld - 2023 - Quantum Monte Carlo algorithm for solving Black-Sc.pdf:C\:\\Users\\isido\\Zotero\\storage\\2UE3LKA6\\Li en Neufeld - 2023 - Quantum Monte Carlo algorithm for solving Black-Sc.pdf:application/pdf},
}

@misc{noauthor_stochastic_2022,
	title = {Stochastic partial differential equation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Stochastic_partial_differential_equation&oldid=1129102419},
	abstract = {Stochastic partial differential equations (SPDEs) generalize partial differential equations via random force terms and coefficients, in the same way ordinary stochastic differential equations generalize ordinary differential equations.
They have relevance to quantum field theory, statistical mechanics, and spatial modeling.},
	language = {en},
	urldate = {2023-01-24},
	journal = {Wikipedia},
	month = dec,
	year = {2022},
	note = {Page Version ID: 1129102419},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\KXX22C9Y\\Stochastic_partial_differential_equation.html:text/html},
}

@misc{becker_learning_2022,
	title = {Learning the random variables in {Monte} {Carlo} simulations with stochastic gradient descent: {Machine} learning for parametric {PDEs} and financial derivative pricing},
	shorttitle = {Learning the random variables in {Monte} {Carlo} simulations with stochastic gradient descent},
	url = {http://arxiv.org/abs/2202.02717},
	abstract = {In financial engineering, prices of financial products are computed approximately many times each trading day with (slightly) different parameters in each calculation. In many financial models such prices can be approximated by means of Monte Carlo (MC) simulations. To obtain a good approximation the MC sample size usually needs to be considerably large resulting in a long computing time to obtain a single approximation. In this paper we introduce a new approximation strategy for parametric approximation problems including the parametric financial pricing problems described above. A central aspect of the approximation strategy proposed in this article is to combine MC algorithms with machine learning techniques to, roughly speaking, learn the random variables (LRV) in MC simulations. In other words, we employ stochastic gradient descent (SGD) optimization methods not to train parameters of standard artificial neural networks (ANNs) but to learn random variables appearing in MC approximations. We numerically test the LRV strategy on various parametric problems with convincing results when compared with standard MC simulations, Quasi-Monte Carlo simulations, SGD-trained shallow ANNs, and SGD-trained deep ANNs. Our numerical simulations strongly indicate that the LRV strategy might be capable to overcome the curse of dimensionality in the \$L{\textasciicircum}{\textbackslash}infty\$-norm in several cases where the standard deep learning approach has been proven not to be able to do so. This is not a contradiction to lower bounds established in the scientific literature because this new LRV strategy is outside of the class of algorithms for which lower bounds have been established in the scientific literature. The proposed LRV strategy is of general nature and not only restricted to the parametric financial pricing problems described above, but applicable to a large class of approximation problems.},
	language = {en},
	urldate = {2023-01-24},
	publisher = {arXiv},
	author = {Becker, Sebastian and Jentzen, Arnulf and Müller, Marvin S. and von Wurstemberger, Philippe},
	month = feb,
	year = {2022},
	note = {arXiv:2202.02717 [cs, math]},
	keywords = {monte carlo, Mathematics - Probability, PDE, Mathematics - Numerical Analysis, optimization, Mathematics - Analysis of PDEs, machine learning},
	file = {Becker e.a. - 2022 - Learning the random variables in Monte Carlo simul.pdf:C\:\\Users\\isido\\Zotero\\storage\\ZT28NZYQ\\Becker e.a. - 2022 - Learning the random variables in Monte Carlo simul.pdf:application/pdf},
}

@misc{nguwi_fully_2022,
	title = {A fully nonlinear {Feynman}-{Kac} formula with derivatives of arbitrary orders},
	url = {http://arxiv.org/abs/2201.03882},
	abstract = {We present an algorithm for the numerical solution of nonlinear parabolic partial diﬀerential equations. This algorithm extends the classical Feynman-Kac formula to fully nonlinear partial diﬀerential equations, by using random trees that carry information on nonlinearities on their branches. It applies to functional, non-polynomial nonlinearities that are not treated by standard branching arguments, and deals with derivative terms of arbitrary orders. A Monte Carlo numerical implementation is provided.},
	language = {en},
	urldate = {2023-01-24},
	publisher = {arXiv},
	author = {Nguwi, Jiang Yu and Penent, Guillaume and Privault, Nicolas},
	month = dec,
	year = {2022},
	note = {arXiv:2201.03882 [math]},
	keywords = {monte carlo, Mathematics - Probability, PDE, Mathematics - Analysis of PDEs},
	file = {Nguwi e.a. - 2022 - A fully nonlinear Feynman-Kac formula with derivat.pdf:C\:\\Users\\isido\\Zotero\\storage\\XRMIKRSC\\Nguwi e.a. - 2022 - A fully nonlinear Feynman-Kac formula with derivat.pdf:application/pdf},
}

@misc{noauthor_branching_2023,
	title = {Branching process},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Branching_process&oldid=1131945188},
	abstract = {In probability theory, a branching process is a type of mathematical object known as a stochastic process, which consists of collections of random variables. The random variables of a stochastic process are indexed by the natural numbers. The original purpose of branching processes was to serve as a mathematical model of a population in which each individual in generation 
  
    
      
        n
      
    
    \{{\textbackslash}displaystyle n\}
   produces some random number of individuals in generation 
  
    
      
        n
        +
        1
      
    
    \{{\textbackslash}displaystyle n+1\}
  , according, in the simplest case, to a fixed probability distribution that does not vary from individual to individual. Branching processes are used to model reproduction; for example, the individuals might correspond to bacteria, each of which generates 0, 1, or 2 offspring with some probability in a single time unit.  Branching processes can also be used to model other systems with similar dynamics, e.g., the spread of surnames in genealogy or the propagation of neutrons in a nuclear reactor.
A central question in the theory of branching processes is the probability of ultimate extinction, where no individuals exist after some finite number of generations.  Using Wald's equation, it can be shown that starting with one individual in generation zero, the expected size of generation n equals μn where μ is the expected number of children of each individual.  If μ {\textless} 1, then the expected number of individuals goes rapidly to zero, which implies ultimate extinction with probability 1 by Markov's inequality.  Alternatively, if μ {\textgreater} 1, then the probability of ultimate extinction is less than 1 (but not necessarily zero; consider a process where each individual either has 0 or 100 children with equal probability. In that case, μ = 50, but probability of ultimate extinction is greater than 0.5, since that's the probability that the first individual has 0 children).  If μ = 1, then ultimate extinction occurs with probability 1 unless each individual always has exactly one child.
In theoretical ecology, the parameter μ of a branching process is called the basic reproductive rate.},
	language = {en},
	urldate = {2023-01-24},
	journal = {Wikipedia},
	month = jan,
	year = {2023},
	note = {Page Version ID: 1131945188},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\NDK5UH98\\Branching_process.html:text/html},
}

@misc{henry-labordere_branching_2016,
	title = {Branching diffusion representation of semilinear {PDEs} and {Monte} {Carlo} approximation},
	url = {http://arxiv.org/abs/1603.01727},
	abstract = {We provide a representation result of parabolic semi-linear PD-Es, with polynomial nonlinearity, by branching diﬀusion processes. We extend the classical representation for KPP equations, introduced by Skorokhod [23], Watanabe [27] and McKean [18], by allowing for polynomial nonlinearity in the pair (u, Du), where u is the solution of the PDE with space gradient Du. Similar to the previous literature, our result requires a non-explosion condition which restrict to “small maturity” or “small nonlinearity” of the PDE. Our main ingredient is the automatic diﬀerentiation technique as in [15], based on the Malliavin integration by parts, which allows to account for the nonlinearities in the gradient. As a consequence, the particles of our branching diﬀusion are marked by the nature of the nonlinearity. This new representation has very important numerical implications as it is suitable for Monte Carlo simulation. Indeed, this provides the ﬁrst numerical method for high dimensional nonlinear PDEs with error estimate induced by the dimension-free Central limit theorem. The complexity is also easily seen to be of the order of the squared dimension. The ﬁnal section of this paper illustrates the eﬃciency of the algorithm by some high dimensional numerical experiments.},
	language = {en},
	urldate = {2023-01-24},
	publisher = {arXiv},
	author = {Henry-Labordere, Pierre and Oudjane, Nadia and Tan, Xiaolu and Touzi, Nizar and Warin, Xavier},
	month = mar,
	year = {2016},
	note = {arXiv:1603.01727 [math]},
	keywords = {monte carlo, Mathematics - Probability, PDE, Mathematics - Numerical Analysis},
	file = {Henry-Labordere e.a. - 2016 - Branching diffusion representation of semilinear P.pdf:C\:\\Users\\isido\\Zotero\\storage\\6HC4YVGI\\Henry-Labordere e.a. - 2016 - Branching diffusion representation of semilinear P.pdf:application/pdf},
}

@misc{hout_efficient_2022,
	title = {Efficient numerical valuation of {European} options under the two-asset {Kou} jump-diffusion model},
	url = {http://arxiv.org/abs/2207.10060},
	abstract = {This paper concerns the numerical solution of the two-dimensional time-dependent partial integro-diﬀerential equation (PIDE) that holds for the values of European-style options under the two-asset Kou jump-diﬀusion model. A main feature of this equation is the presence of a nonlocal double integral term. For its numerical evaluation, we extend a highly eﬃcient algorithm derived by Toivanen [30] in the case of the one-dimensional Kou integral. The acquired algorithm for the two-dimensional Kou integral has optimal computational cost: the number of basic arithmetic operations is directly proportional to the number of spatial grid points in the semidiscretization. For the eﬀective discretization in time, we study seven contemporary operator splitting schemes of the implicit-explicit (IMEX) and the alternating direction implicit (ADI) kind. All these schemes allow for a convenient, explicit treatment of the integral term. By ample numerical experiments for put-on-the-average option values, the stability and convergence behaviour as well as the mutual performance of the seven operator splitting schemes are investigated. Moreover, the Greeks Delta and Gamma are considered.},
	language = {en},
	urldate = {2023-01-24},
	publisher = {arXiv},
	author = {Hout, Karel in 't and Lamotte, Pieter},
	month = jul,
	year = {2022},
	note = {arXiv:2207.10060 [cs, math, q-fin]},
	keywords = {Quantitative Finance - Computational Finance, Mathematics - Numerical Analysis, finance},
	annote = {Comment: arXiv admin note: text overlap with arXiv:1901.03839},
	file = {Hout en Lamotte - 2022 - Efficient numerical valuation of European options .pdf:C\:\\Users\\isido\\Zotero\\storage\\65FUB6WN\\Hout en Lamotte - 2022 - Efficient numerical valuation of European options .pdf:application/pdf},
}

@article{cerny_simplified_2021,
	title = {Simplified stochastic calculus with applications in {Economics} and {Finance}},
	volume = {293},
	issn = {03772217},
	url = {http://arxiv.org/abs/1912.03651},
	doi = {10.1016/j.ejor.2020.12.037},
	abstract = {The paper introduces a simple way of recording and manipulating general stochastic processes without explicit reference to a probability measure. In the new calculus, operations traditionally presented in a measure-speciﬁc way are instead captured by tracing the behaviour of jumps (also when no jumps are physically present). The calculus is fail-safe in that, under minimal assumptions, all informal calculations yield mathematically well-deﬁned stochastic processes. The calculus is also intuitive as it allows the user to pretend all jumps are of compound Poisson type. The new calculus is very eﬀective when it comes to computing drifts and expected values that possibly involve a change of measure. Such drift calculations yield, for example, partial integro–diﬀerential equations, Hamilton–Jacobi–Bellman equations, Feynman–Kac formulae, or exponential moments needed in numerous applications. We provide several illustrations of the new technique, among them a novel result on the Margrabe option to exchange one defaultable asset for another.},
	language = {en},
	number = {2},
	urldate = {2023-01-24},
	journal = {European Journal of Operational Research},
	author = {Černý, Aleš and Ruf, Johannes},
	month = sep,
	year = {2021},
	note = {arXiv:1912.03651 [math, q-fin]},
	keywords = {Mathematics - Probability, Quantitative Finance - Mathematical Finance, finance},
	pages = {547--560},
	file = {Černý en Ruf - 2021 - Simplified stochastic calculus with applications i.pdf:C\:\\Users\\isido\\Zotero\\storage\\7NICUN3Z\\Černý en Ruf - 2021 - Simplified stochastic calculus with applications i.pdf:application/pdf},
}

@misc{blechschmidt_three_2021,
	title = {Three {Ways} to {Solve} {Partial} {Differential} {Equations} with {Neural} {Networks} -- {A} {Review}},
	url = {http://arxiv.org/abs/2102.11802},
	abstract = {Neural networks are increasingly used to construct numerical solution methods for partial diﬀerential equations. In this expository review, we introduce and contrast three important recent approaches attractive in their simplicity and their suitability for high-dimensional problems: physics-informed neural networks, methods based on the Feynman-Kac formula and methods based on the solution of backward stochastic diﬀerential equations. The article is accompanied by a suite of expository software in the form of Jupyter notebooks in which each basic methodology is explained step by step, allowing for a quick assimilation and experimentation. An extensive bibliography summarizes the state of the art.},
	language = {en},
	urldate = {2023-01-24},
	publisher = {arXiv},
	author = {Blechschmidt, Jan and Ernst, Oliver G.},
	month = apr,
	year = {2021},
	note = {arXiv:2102.11802 [cs, math]},
	keywords = {PDE, Mathematics - Numerical Analysis, machine learning, neural networks},
	annote = {Comment: 32 pages; for associated Jupyter notebooks, see https://github.com/janblechschmidt/PDEsByNNs},
	file = {Blechschmidt en Ernst - 2021 - Three Ways to Solve Partial Differential Equations.pdf:C\:\\Users\\isido\\Zotero\\storage\\3CQBUAIB\\Blechschmidt en Ernst - 2021 - Three Ways to Solve Partial Differential Equations.pdf:application/pdf},
}

@article{e_multilevel_2019,
	title = {On multilevel {Picard} numerical approximations for high-dimensional nonlinear parabolic partial differential equations and high-dimensional nonlinear backward stochastic differential equations},
	volume = {79},
	issn = {0885-7474, 1573-7691},
	url = {http://arxiv.org/abs/1708.03223},
	doi = {10.1007/s10915-018-00903-0},
	abstract = {Parabolic partial diﬀerential equations (PDEs) and backward stochastic diﬀerential equations (BSDEs) are key ingredients in a number of models in physics and ﬁnancial engineering. In particular, parabolic PDEs and BSDEs are fundamental tools in the state-of-the-art pricing and hedging of ﬁnancial derivatives. The PDEs and BSDEs appearing in such applications are often high-dimensional and nonlinear. Since explicit solutions of such PDEs and BSDEs are typically not available, it is a very active topic of research to solve such PDEs and BSDEs approximately. In the recent article [E, W., Hutzenthaler, M., Jentzen, A., \& Kruse, T. Linear scaling algorithms for solving high-dimensional nonlinear parabolic diﬀerential equations. arXiv:1607.03295 (2017)] we proposed a family of approximation methods based on Picard approximations and multilevel Monte Carlo methods and showed under suitable regularity assumptions on the exact solution for semilinear heat equations that the computational complexity is bounded by O(d ε−(4+δ)) for any δ ∈ (0, ∞), where d is the dimensionality of the problem and ε ∈ (0, ∞) is the prescribed accuracy. In this paper, we test the applicability of this algorithm on a variety of 100-dimensional nonlinear PDEs that arise in physics and ﬁnance by means of numerical simulations presenting approximation accuracy against runtime. The simulation results for these 100-dimensional example PDEs are very satisfactory in terms of accuracy and speed. In addition, we also provide a review of other approximation methods for nonlinear PDEs and BSDEs from the literature.},
	language = {en},
	number = {3},
	urldate = {2023-01-24},
	journal = {Journal of Scientific Computing},
	author = {E, Weinan and Hutzenthaler, Martin and Jentzen, Arnulf and Kruse, Thomas},
	month = jun,
	year = {2019},
	note = {arXiv:1708.03223 [math]},
	keywords = {PDE, Mathematics - Numerical Analysis},
	pages = {1534--1571},
	file = {E e.a. - 2019 - On multilevel Picard numerical approximations for .pdf:C\:\\Users\\isido\\Zotero\\storage\\GNSPNCPT\\E e.a. - 2019 - On multilevel Picard numerical approximations for .pdf:application/pdf},
}

@misc{warin_variations_2017,
	title = {Variations on branching methods for non linear {PDEs}},
	url = {http://arxiv.org/abs/1701.07660},
	abstract = {The branching methods developed in [9], [11] are eﬀective methods to solve some semi linear PDEs and are shown numerically to be able to solve some full non linear PDEs. These methods are however restricted to some small coeﬃcients in the PDE and small maturities. This article shows numerically that these methods can be adapted to solve the problems with longer maturities in the semi-linear case by using a new derivation scheme and some nested method. As for the case of full non linear PDEs, we introduce new schemes and we show numerically that they provide an eﬀective alternative to the schemes previously developed.},
	language = {en},
	urldate = {2023-01-24},
	publisher = {arXiv},
	author = {Warin, Xavier},
	month = jan,
	year = {2017},
	note = {arXiv:1701.07660 [math]},
	keywords = {monte carlo, Mathematics - Probability, PDE},
	annote = {Comment: 25 pages},
	file = {Warin - 2017 - Variations on branching methods for non linear PDE.pdf:C\:\\Users\\isido\\Zotero\\storage\\CK46H39I\\Warin - 2017 - Variations on branching methods for non linear PDE.pdf:application/pdf},
}

@misc{warin_nesting_2018,
	title = {Nesting {Monte} {Carlo} for high-dimensional {Non} {Linear} {PDEs}},
	url = {http://arxiv.org/abs/1804.08432},
	abstract = {A new method based on nesting Monte Carlo is developed to solve highdimensional semi-linear PDEs. Convergence of the method is proved and its convergence rate studied. Results in high dimension for diﬀerent kind of non-linearities show its eﬃciency.},
	language = {en},
	urldate = {2023-01-24},
	publisher = {arXiv},
	author = {Warin, Xavier},
	month = may,
	year = {2018},
	note = {arXiv:1804.08432 [math]},
	keywords = {monte carlo, Mathematics - Probability, PDE, Primary 65C05, secondary 49L25},
	annote = {Comment: 35 pages},
	file = {Warin - 2018 - Nesting Monte Carlo for high-dimensional Non Linea.pdf:C\:\\Users\\isido\\Zotero\\storage\\LV4I2MV7\\Warin - 2018 - Nesting Monte Carlo for high-dimensional Non Linea.pdf:application/pdf},
}

@misc{agarwal_branching_2018,
	title = {Branching diffusion representation of semi-linear elliptic {PDEs} and estimation using {Monte} {Carlo} method},
	url = {http://arxiv.org/abs/1704.00328},
	abstract = {We study semi-linear elliptic PDEs with polynomial non-linearity and provide a probabilistic representation of their solution using branching diﬀusion processes. When the non-linearity involves the unknown function but not its derivatives, we extend previous results in the literature by showing that our probabilistic representation provides a solution to the PDE without assuming its existence. In the general case, we derive a new representation of the solution by using marked branching diﬀusion processes and automatic diﬀerentiation formulas to account for the non-linear gradient term. In both cases, we develop new theoretical tools to provide explicit suﬃcient conditions under which our probabilistic representations hold. As an application, we consider several examples including multi-dimensional semi-linear elliptic PDEs and estimate their solution by using the Monte Carlo method.},
	language = {en},
	urldate = {2023-01-24},
	publisher = {arXiv},
	author = {Agarwal, Ankush and Claisse, Julien},
	month = feb,
	year = {2018},
	note = {arXiv:1704.00328 [math]},
	keywords = {Mathematics - Probability, PDE},
	file = {Agarwal en Claisse - 2018 - Branching diffusion representation of semi-linear .pdf:C\:\\Users\\isido\\Zotero\\storage\\A5D35SX4\\Agarwal en Claisse - 2018 - Branching diffusion representation of semi-linear .pdf:application/pdf},
}

@misc{noauthor_predictorcorrector_2020,
	title = {Predictor–corrector method},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Predictor%E2%80%93corrector_method&oldid=955920527},
	abstract = {In numerical analysis, predictor–corrector methods belong to a class of algorithms designed to integrate ordinary differential equations – to find an unknown function that satisfies a given differential equation.  All such algorithms proceed in two steps: 

The initial, "prediction" step, starts from a function fitted to the function-values and derivative-values at a preceding set of points to extrapolate ("anticipate") this function's value at a subsequent, new point.
The next, "corrector" step refines the initial approximation by using the predicted value of the function and another method to interpolate that unknown function's value at the same subsequent point.},
	language = {en},
	urldate = {2023-01-25},
	journal = {Wikipedia},
	month = may,
	year = {2020},
	note = {Page Version ID: 955920527},
	keywords = {ODE},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\8KLI757A\\Predictor–corrector_method.html:text/html},
}

@misc{noauthor_spectral_2023,
	title = {Spectral method},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Spectral_method&oldid=1134134983},
	abstract = {Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations. The idea is to write the solution of the differential equation as a sum of certain "basis functions" (for example, as a Fourier series which is a sum of sinusoids) and then to choose the coefficients in the sum in order to satisfy the differential equation as well as possible.
Spectral methods and finite element methods are closely related and built on the same ideas; the main difference between them is that spectral methods use basis functions that are generally nonzero over the whole domain, while finite element methods use basis functions that are nonzero only on small subdomains (compact support). Consequently, spectral methods connect variables globally while finite elements do so locally. Partially for this reason, spectral methods have excellent error properties, with the so-called "exponential convergence" being the fastest possible, when the solution is smooth. However, there are no known three-dimensional single domain spectral shock capturing results (shock waves are not smooth). In the finite element community, a method where the degree of the elements is very high or increases as the grid parameter h increases is sometimes called a spectral element method.
Spectral methods can be used to solve differential equations (PDEs, ODEs, eigenvalue, etc) and optimization problems. When applying spectral methods to time-dependent PDEs, the solution is typically written as a sum of basis functions with time-dependent coefficients; substituting this in the PDE yields a system of ODEs in the coefficients which can be solved using any numerical method for ODEs. Eigenvalue problems for ODEs are similarly converted to matrix eigenvalue problems.
Spectral methods were developed in a long series of papers by Steven Orszag starting in 1969 including, but not limited to, Fourier series methods for periodic geometry problems, polynomial spectral methods for finite and unbounded geometry problems, pseudospectral methods for highly nonlinear problems, and spectral iteration methods for fast solution of steady-state problems. The implementation of the spectral method is normally accomplished either with collocation or a Galerkin or a Tau approach . For very small problems, the spectral method is unique in that solutions may be written out symbolically, yielding a practical alternative to series solutions for differential equations.
Spectral methods can be computationally less expensive and easier to implement than finite element methods; they shine best when high accuracy is sought in simple domains with smooth solutions. However, because of their global nature, the matrices associated with step computation are dense and computational efficiency will quickly suffer when there are many degrees of freedom (with some exceptions, for example if matrix applications can be written as Fourier transforms). For larger problems and nonsmooth solutions, finite elements will generally work better due to sparse matrices and better modelling of discontinuities and sharp bends.},
	language = {en},
	urldate = {2023-01-25},
	journal = {Wikipedia},
	month = jan,
	year = {2023},
	note = {Page Version ID: 1134134983},
	keywords = {ODE},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\ZM4CLCDZ\\Spectral_method.html:text/html},
}

@misc{noauthor_galerkin_2023,
	title = {Galerkin method},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Galerkin_method&oldid=1130900253},
	abstract = {In mathematics, in the area of numerical analysis, Galerkin methods, named after the Russian mathematician Boris Galerkin, convert a continuous operator problem, such as a differential equation, commonly in a weak formulation, to a discrete problem by applying linear constraints determined by finite sets of basis functions.
Often when referring to a Galerkin method, one also gives the name along with typical assumptions and approximation methods used:

 Ritz–Galerkin method (after Walther Ritz) typically assumes symmetric and positive definite bilinear form in the weak formulation, where the differential equation for a physical system can be formulated via minimization of a quadratic function representing the system energy and the approximate solution is a linear combination of the given set of the basis functions.
Bubnov–Galerkin method (after Ivan Bubnov) does not require the bilinear form to be symmetric and substitutes the energy minimization with orthogonality constraints determined by the same basis functions that are used to approximate the solution. In an operator formulation of the differential equation, Bubnov–Galerkin method can be viewed as applying an orthogonal projection to the operator.
Petrov–Galerkin method (after Georgii I. Petrov) allows using basis functions for orthogonality constraints (called test basis functions) that are different from the basis functions used to approximate the solution. Petrov–Galerkin method can be viewed as an extension of Bubnov–Galerkin method, applying a projection that is not necessarily orthogonal in the operator formulation of the differential equation.Examples of Galerkin methods are:

the Galerkin method of weighted residuals, the most common method of calculating the global stiffness matrix in the finite element method,
the boundary element method for solving integral equations,
Krylov subspace methods.},
	language = {en},
	urldate = {2023-01-25},
	journal = {Wikipedia},
	month = jan,
	year = {2023},
	note = {Page Version ID: 1130900253},
	keywords = {ODE},
}

@misc{noauthor_collocation_2022,
	title = {Collocation method},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Collocation_method&oldid=1101629556},
	abstract = {In mathematics, a collocation method is a method for the numerical solution of ordinary differential equations, partial differential equations and integral equations. The idea is to choose a finite-dimensional space of candidate solutions (usually polynomials up to a certain degree) and a number of points in the domain (called collocation points), and to select that solution which satisfies the given equation at the collocation points.},
	language = {en},
	urldate = {2023-01-25},
	journal = {Wikipedia},
	month = aug,
	year = {2022},
	note = {Page Version ID: 1101629556},
	keywords = {ODE},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\GENR5CET\\Collocation_method.html:text/html},
}

@article{gobet_spectral_2004,
	title = {A spectral {Monte} {Carlo} method for the {Poisson} equation},
	volume = {10},
	issn = {1569-3961, 0929-9629},
	url = {https://www.degruyter.com/document/doi/10.1515/mcma.2004.10.3-4.275/html},
	doi = {10.1515/mcma.2004.10.3-4.275},
	abstract = {Using a sequential Monte Carlo algorithm, we compute a spectral approximation of the solution of the Poisson equation in dimension 1 and 2. The Feyman-Kac computation of the pointwise solution is achieved using either an integral representation or a modiﬁed walk on spheres method. The variances decrease geometrically with the number of steps. A global solution is obtained, accurate up to the interpolation error. Surprisingly, the accuracy depends very little on the absorption layer thickness of the walk on spheres.},
	language = {en},
	number = {3-4},
	urldate = {2023-01-25},
	journal = {Monte Carlo Methods and Applications},
	author = {Gobet, Emmanuel and Maire, Sylvain},
	month = jan,
	year = {2004},
	keywords = {monte carlo, PDE, walk on spheres, SALT},
	file = {Gobet en Maire - 2004 - A spectral Monte Carlo method for the Poisson equa.pdf:C\:\\Users\\isido\\Zotero\\storage\\I5NQN5MH\\Gobet en Maire - 2004 - A spectral Monte Carlo method for the Poisson equa.pdf:application/pdf},
}

@misc{maire_monte_2013,
	title = {Monte {Carlo} approximations of the {Neumann} problem},
	url = {http://arxiv.org/abs/1203.4910},
	abstract = {We introduce Monte Carlo methods to compute the solution of elliptic equations with pure Neumann boundary conditions. We ﬁrst prove that the solution obtained by the stochastic representation has a zero mean value with respect to the invariant measure of the stochastic process associated to the equation. Pointwise approximations are computed by means of standard and new simulation schemes especially devised for local time approximation on the boundary of the domain. Global approximations are computed thanks to a stochastic spectral formulation taking into account the property of zero mean value of the solution. This stochastic formulation is asymptotically perfect in terms of conditioning. Numerical examples are given on the Laplace operator on a square domain with both pure Neumann and mixed Dirichlet-Neumann boundary conditions. A more general convection-diﬀusion equation is also numerically studied.},
	language = {en},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Maire, Sylvain and Tanré, Etienne},
	month = aug,
	year = {2013},
	note = {arXiv:1203.4910 [math]},
	keywords = {Mathematics - Probability, PDE},
	file = {Maire en Tanré - 2013 - Monte Carlo approximations of the Neumann problem.pdf:C\:\\Users\\isido\\Zotero\\storage\\IAB5W5EV\\Maire en Tanré - 2013 - Monte Carlo approximations of the Neumann problem.pdf:application/pdf},
}

@incollection{l_ecuyer_stochastic_2009,
	address = {Berlin, Heidelberg},
	title = {Stochastic {Spectral} {Formulations} for {Elliptic} {Problems}},
	isbn = {978-3-642-04106-8 978-3-642-04107-5},
	url = {http://link.springer.com/10.1007/978-3-642-04107-5_33},
	abstract = {We describe new stochastic spectral formulations with very good properties in terms of conditioning. These formulations are built by combining Monte Carlo approximations of the Feynman-Kac formula and standard deterministic approximations on basis functions. We give error bounds on the solutions obtained using these formulations in the case of linear approximations. Some numerical tests are made on an anisotropic diﬀusion equation using a tensor product Tchebychef polynomial basis and one random point schemes quantiﬁed or not.},
	language = {en},
	urldate = {2023-01-25},
	booktitle = {Monte {Carlo} and {Quasi}-{Monte} {Carlo} {Methods} 2008},
	publisher = {Springer Berlin Heidelberg},
	author = {Maire, Sylvain and Tanré, Etienne},
	editor = {L' Ecuyer, Pierre and Owen, Art B.},
	year = {2009},
	doi = {10.1007/978-3-642-04107-5_33},
	pages = {513--528},
	file = {Maire en Tanré - 2009 - Stochastic Spectral Formulations for Elliptic Prob.pdf:C\:\\Users\\isido\\Zotero\\storage\\N6IB2UR9\\Maire en Tanré - 2009 - Stochastic Spectral Formulations for Elliptic Prob.pdf:application/pdf},
}

@article{maire_new_2008,
	title = {Some new simulations schemes for the evaluation of {Feynman}–{Kac} representations},
	volume = {14},
	issn = {0929-9629, 1569-3961},
	url = {https://www.degruyter.com/document/doi/10.1515/MCMA.2008.002/html},
	doi = {10.1515/MCMA.2008.002},
	abstract = {We describe new variants of the Euler scheme and of the walk on spheres method for the Monte Carlo computation of Feynman-Kac representations. We optimize these variants using quantization for both source and boundary terms. Numerical tests are given on basic examples and on Monte Carlo versions of spectral methods for the Poisson equation. We especially introduce a new stochastic spectral formulation with very good properties in terms of conditioning.},
	language = {en},
	number = {1},
	urldate = {2023-01-25},
	journal = {Monte Carlo Methods and Applications},
	author = {Maire, Sylvain and Tanré, Etienne},
	month = jan,
	year = {2008},
	file = {Maire en Tanré - 2008 - Some new simulations schemes for the evaluation of.pdf:C\:\\Users\\isido\\Zotero\\storage\\P47ARGF3\\Maire en Tanré - 2008 - Some new simulations schemes for the evaluation of.pdf:application/pdf},
}

@misc{meester_exponential_2018,
	title = {Exponential convergence of adaptive importance sampling estimators for {Markov} chain expectations},
	url = {http://arxiv.org/abs/1806.03029},
	abstract = {In this paper it is shown that adaptive importance sampling algorithms converge at exponential rate for Markov chain expectation problems that admit a combination of a ﬁltered estimator and a Markov zero-variance measure. It extends a chain of results—special purpose proofs were already known for several cases [8, 2, 6]. A recent paper [1] provides a complete description of the class of combinations of Markov process expectations of path functionals and ﬁltered estimators that admit zero-variance importance measures that retain the Markov property. In a way, this is the maximal class for which adaptive importance sampling algorithms might exhibit exponential convergence. The main purpose of this paper is to prove that this is the case: for (most of) those combinations the natural adaptive importance sampling algorithm converges at exponential rate. In addition, the applicability of general Markov chain theory for this purpose is discussed through the analysis of a counterexample presented in [7].},
	language = {en},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Meester, Ludolf E.},
	month = jul,
	year = {2018},
	note = {arXiv:1806.03029 [math]},
	keywords = {Mathematics - Probability},
	file = {Meester - 2018 - Exponential convergence of adaptive importance sam.pdf:C\:\\Users\\isido\\Zotero\\storage\\8BJ5II9Q\\Meester - 2018 - Exponential convergence of adaptive importance sam.pdf:application/pdf},
}

@article{gobet_new_2014,
	title = {A new sequential algorithm for {L2}-approximation and application to {Monte}-{Carlo} integration},
	abstract = {We design a new stochastic algorithm (called SALT) that sequentially approximates a given function in L2 w.r.t. a probability measure, using a ﬁnite sample of the distribution. By increasing the sets of approximating functions and the simulation eﬀort, we compute a L2-approximation with higher and higher accuracy. The simulation eﬀort is tuned in a robust way that ensures the convergence under rather general conditions. Then, we apply SALT to build eﬃcient control variates for accurate numerical integration. Examples and numerical experiments support the mathematical analysis.},
	language = {en},
	author = {Gobet, Emmanuel and Surana, Khushboo},
	year = {2014},
	keywords = {monte carlo, integration, SALT},
	file = {Gobet en Surana - A new sequential algorithm for L2-approximation an.pdf:C\:\\Users\\isido\\Zotero\\storage\\MFJ99N7L\\Gobet en Surana - A new sequential algorithm for L2-approximation an.pdf:application/pdf},
}

@misc{noauthor_eigenfunction_2022,
	title = {Eigenfunction},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Eigenfunction&oldid=1115272845},
	abstract = {In mathematics, an eigenfunction of a linear operator D defined on some function space is any non-zero function 
  
    
      
        f
      
    
    \{{\textbackslash}displaystyle f\}
   in that space that, when acted upon by D, is only multiplied by some scaling factor called an eigenvalue. As an equation, this condition can be written as

for some scalar eigenvalue 
  
    
      
        λ
        .
      
    
    \{{\textbackslash}displaystyle {\textbackslash}lambda .\}
   The solutions to this equation may also be subject to boundary conditions that limit the allowable eigenvalues and eigenfunctions.
An eigenfunction is a type of eigenvector.},
	language = {en},
	urldate = {2023-01-30},
	journal = {Wikipedia},
	month = oct,
	year = {2022},
	note = {Page Version ID: 1115272845},
}

@misc{hout_application_2015,
	title = {Application of {Operator} {Splitting} {Methods} in {Finance}},
	url = {http://arxiv.org/abs/1504.01022},
	abstract = {Financial derivatives pricing aims to ﬁnd the fair value of a ﬁnancial contract on an underlying asset. Here we consider option pricing in the partial differential equations framework. The contemporary models lead to one-dimensional or multidimensional parabolic problems of the convection-diffusion type and generalizations thereof. An overview of various operator splitting methods is presented for the efﬁcient numerical solution of these problems.},
	language = {en},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Hout, Karel in 't and Toivanen, Jari},
	month = apr,
	year = {2015},
	note = {arXiv:1504.01022 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance, ODE},
	file = {Hout en Toivanen - 2015 - Application of Operator Splitting Methods in Finan.pdf:C\:\\Users\\isido\\Zotero\\storage\\P4MV65GF\\Hout en Toivanen - 2015 - Application of Operator Splitting Methods in Finan.pdf:application/pdf},
}

@article{giles_smoking_nodate,
	title = {Smoking adjoints, part {II}: fast {Monte} {Carlo} {Greeks}},
	abstract = {The efficient calculation of price sensitivities continues to be among
the greatest practical challenges facing users of Monte Carlo methods
in the derivatives industry. Computing Greeks is essential to hedging
and risk management, but typically requires substantially more computing
time than pricing a derivative. This article shows how an adjoint formula-
tion can be used to accelerate the calculation of the Greeks. This method
is particularly well suited to applications requiring sensitivities to a large
number of parameters. Examples include interest rate derivatives requir-
ing sensitivities to all initial forward rates and equity derivatives requiring
sensitivities to all points on a volatility surface.
The simplest methods for estimating Greeks are based on finite differ-
ence approximations, in which a Monte Carlo pricing routine is rerun mul-
tiple times at different settings of the input parameters in order to estimate
sensitivities to the parameters. In the fixed-income setting, for example,
this would mean perturbing each initial forward rate and then rerunning
the Monte Carlo simulation to re-price a security or a whole book. The
main virtues of this method are that it is straightforward to understand and
requires no additional programming. But the bias and variance properties
of finite difference estimates can be rather poor, and their computing time
requirements grow with the number of input parameters.
Better estimates of price sensitivities can often be derived by using in-
formation about model dynamics in a Monte Carlo simulation. Techniques
for doing this include the pathwise method and likelihood ratio method,
both of which are reviewed in chapter 7 of Glasserman (2004). When ap-
plicable, these methods produce unbiased estimates of price sensitivities
from a single set of simulated paths, that is, without perturbing any para-
meters. The pathwise method accomplishes this by differentiating the evo-
lution of the underlying assets or state variables along each path; the
likelihood ratio method instead differentiates the transition density of the
underlying assets or state variables. In comparison with finite difference
estimates, these methods require additional model analysis and program-
ming, but the additional effort is often justified by the improvement in the
quality of calculated Greeks.
The adjoint method we develop here applies ideas used in computa-
tional fluid dynamics (Giles \& Pierce, 2000) to the calculation of pathwise
estimates of Greeks. The estimate calculated using the adjoint method is
identical to the ordinary pathwise estimate; its potential advantage is there-
fore computational, rather than statistical. The relative merits of the ordi-
nary (forward) calculation of pathwise Greeks and the adjoint calculation
can be summarised as follows: a) the adjoint method is advantageous for
calculating the sensitivities of a small number of securities with respect to
a large number of parameters; and b) the forward method is advantageous
for calculating the sensitivities of many securities with respect to a small
number of parameters. The ‘small number of securities’ in this dichotomy
could be an entire book, consisting of many individual securities, so long
as the sensitivities to be calculated are for the book as a whole and not for
the constituent securities.
The rest of this article is organised as follows. The next section reviews
the usual forward calculation of pathwise Greeks and the subsequent sec-
tion illustrates its application in the Libor market model. We then develop
the adjoint method for delta estimates, and extend it to applications such
as vega estimation requiring sensitivities to parameters of model dynam-
ics, rather than just sensitivities to initial conditions. We then extend it to
gamma estimation. We use the Libor market model as an illustrative ex-
ample in both settings. Lastly, we present numerical results that illustrate
the computational savings offered by the adjoint method.},
	language = {en},
	author = {Giles, Mike},
	keywords = {monte carlo, greeks},
	file = {Giles - Smoking adjoints, part II fast Monte Carlo Greeks.pdf:C\:\\Users\\isido\\Zotero\\storage\\D4MR342G\\Giles - Smoking adjoints, part II fast Monte Carlo Greeks.pdf:application/pdf},
}

@misc{maran_chebyshev_2021,
	title = {Chebyshev {Greeks}: {Smoothing} {Gamma} without {Bias}},
	shorttitle = {Chebyshev {Greeks}},
	url = {http://arxiv.org/abs/2106.12431},
	abstract = {The computation of Greeks is a fundamental task for risk managing of ﬁnancial instruments. The standard approach to their numerical evaluation is via ﬁnite diﬀerences. Most exotic derivatives are priced via Monte Carlo simulation: in these cases, it is hard to ﬁnd a fast and accurate approximation of Greeks, mainly because of the need of a tradeoﬀ between bias and variance. Recent improvements in Greeks computation, such as Adjoint Algorithmic Diﬀerentiation, are unfortunately uneﬀective on second order Greeks (such as Gamma), which are plagued by the most signiﬁcant instabilities, so that a viable alternative to standard ﬁnite diﬀerences is still lacking. We apply Chebyshev interpolation techniques to the computation of spot Greeks, showing how to improve the stability of ﬁnite diﬀerence Greeks of arbitrary order, in a simple and general way. The increased performance of the proposed technique is analyzed for a number of real payoﬀs commonly traded by ﬁnancial institutions.},
	language = {en},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Maran, Andrea and Pallavicini, Andrea and Scoleri, Stefano},
	month = jun,
	year = {2021},
	note = {arXiv:2106.12431 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance, Quantitative Finance - Mathematical Finance, Quantitative Finance - Pricing of Securities, Quantitative Finance - Risk Management, chebychev, greeks},
	annote = {Comment: 15 pages, 4 figures},
	annote = {Comment: 15 pages, 4 figures},
	file = {Maran e.a. - 2021 - Chebyshev Greeks Smoothing Gamma without Bias.pdf:C\:\\Users\\isido\\Zotero\\storage\\NAD6EA6C\\Maran e.a. - 2021 - Chebyshev Greeks Smoothing Gamma without Bias.pdf:application/pdf},
}

@article{trefethen_approximation_nodate,
	title = {Approximation {Theory} and {Approximation} {Practice}},
	abstract = {There are quite a number of excellent books on approximation theory. Three classics are [Cheney 1966], [Davis 1975], and [Meinardus 1967], and a slightly more recent computationally oriented classic is [Powell 1981]. Perhaps the first approximation theory text was [Borel 1905]. A good deal of my emphasis will be on ideas related to Chebyshev points and polynomials, whose origins go back more than a century to mathematicians including Chebyshev (1821–1894), de la Valle  ́e Poussin (1866–1962), Bernstein (1880–1968), and Jackson (1888–1946). In the computer era, some of the early figures who developed “Chebyshev technology,” in approximately chronological order, were Lanczos, Clenshaw, Good, Fox, Elliott, Mason, Orszag, Paszkowski, and V. I. Lebedev. Five books on Chebyshev polynomials are by Snyder [1966], Paszkowski [1975], Fox and Parker [1968], Rivlin [1990], and Mason and Handscomb [2003]. One reason we emphasize Chebyshev technology so much is that in practice, for working with functions on intervals, these methods are unbeatable. For example, we shall see in Chapter 16 that the difference in approximation power between Chebyshev and “optimal” interpolation points is utterly negligible. Another reason is that if you know the Chebyshev material well, this is the best possible foundation for work on other approximation topics, and for understanding the links with Fourier analysis.},
	language = {en},
	author = {Trefethen, Lloyd N},
	keywords = {chebychev, integration},
	file = {Trefethen - Approximation Theory and Approximation Practice.pdf:C\:\\Users\\isido\\Zotero\\storage\\MREQLLLP\\Trefethen - Approximation Theory and Approximation Practice.pdf:application/pdf},
}

@misc{noauthor_continuous_nodate,
	title = {A continuous analogue of {Krylov} subspace methods for {ODEs} » {Chebfun}},
	url = {https://www.chebfun.org/examples/ode-linear/Krylov.html},
	urldate = {2023-01-30},
	file = {A continuous analogue of Krylov subspace methods for ODEs » Chebfun:C\:\\Users\\isido\\Zotero\\storage\\DRV7T8ZF\\Krylov.html:text/html},
}

@misc{gilles_continuous_2018,
	title = {Continuous analogues of {Krylov} methods for differential operators},
	url = {http://arxiv.org/abs/1803.11049},
	abstract = {Analogues of the conjugate gradient method, MINRES, and GMRES are derived for solving boundary value problems (BVPs) involving second-order diﬀerential operators. Two challenges arise: imposing the boundary conditions on the solution while building up a Krylov subspace, and guaranteeing convergence of the Krylov-based method on unbounded operators. Our approach employs projection operators to guarantee that the boundary conditions are satisﬁed, and we develop an operator preconditioner that ensures that an approximate solution is computed after a ﬁnite number of iterations. The developed Krylov methods are practical iterative BVP solvers that are particularly eﬃcient when a fast operator-function product is available.},
	language = {en},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Gilles, Marc Aurèle and Townsend, Alex},
	month = apr,
	year = {2018},
	note = {arXiv:1803.11049 [math]},
	keywords = {Mathematics - Numerical Analysis},
	file = {Gilles en Townsend - 2018 - Continuous analogues of Krylov methods for differe.pdf:C\:\\Users\\isido\\Zotero\\storage\\K2ZL49ZL\\Gilles en Townsend - 2018 - Continuous analogues of Krylov methods for differe.pdf:application/pdf},
}

@misc{noauthor_constrained_nodate,
	title = {Constrained optimization in {Chebfun} » {Chebfun}},
	url = {https://www.chebfun.org/examples/opt/ConstrainedOptimization.html},
	urldate = {2023-01-30},
	file = {Constrained optimization in Chebfun » Chebfun:C\:\\Users\\isido\\Zotero\\storage\\QIPJE4L9\\ConstrainedOptimization.html:text/html},
}

@misc{noauthor_black-scholes_nodate,
	title = {Black-{Scholes} {PDE} using operator exponential » {Chebfun}},
	url = {https://www.chebfun.org/examples/pde/BSExponential.html},
	urldate = {2023-01-30},
	file = {Black-Scholes PDE using operator exponential » Chebfun:C\:\\Users\\isido\\Zotero\\storage\\F23C3NDI\\BSExponential.html:text/html},
}

@misc{noauthor_tricky_nodate,
	title = {Some tricky integrals » {Chebfun}},
	url = {https://www.chebfun.org/examples/quad/Tricky.html},
	urldate = {2023-01-30},
	file = {Some tricky integrals » Chebfun:C\:\\Users\\isido\\Zotero\\storage\\WBGFR5FD\\Tricky.html:text/html},
}

@misc{noauthor_integrating_nodate,
	title = {Integrating {Tj}(x)*{Tk}(y) over the unit disk » {Chebfun}},
	url = {https://www.chebfun.org/examples/quad/TjTkDisk.html},
	urldate = {2023-01-30},
	file = {Integrating Tj(x)*Tk(y) over the unit disk » Chebfun:C\:\\Users\\isido\\Zotero\\storage\\L63WYHKI\\TjTkDisk.html:text/html},
}

@misc{noauthor_white_nodate,
	title = {The white noise paradox » {Chebfun}},
	url = {https://www.chebfun.org/examples/ode-random/WhiteNoiseParadox.html},
	urldate = {2023-01-30},
	file = {The white noise paradox » Chebfun:C\:\\Users\\isido\\Zotero\\storage\\JEPR2VXQ\\WhiteNoiseParadox.html:text/html},
}

@misc{noauthor_geometric_nodate,
	title = {Geometric {Brownian} motion » {Chebfun}},
	url = {https://www.chebfun.org/examples/ode-random/GBM.html},
	urldate = {2023-01-30},
}

@misc{noauthor_brownian_nodate,
	title = {Brownian paths and random polynomials » {Chebfun}},
	url = {https://www.chebfun.org/examples/stats/RandomPolynomials.html},
	urldate = {2023-01-30},
	keywords = {finance},
	file = {Brownian paths and random polynomials » Chebfun:C\:\\Users\\isido\\Zotero\\storage\\SPBD8LDT\\RandomPolynomials.html:text/html},
}

@misc{lavagnini_pricing_2021,
	title = {Pricing {Asian} {Options} with {Correlators}},
	url = {http://arxiv.org/abs/2104.11684},
	abstract = {We derive a series expansion by Hermite polynomials for the price of an arithmetic Asian option. This series requires the computation of moments and correlators of the underlying price process, but for a polynomial jump-diﬀusion, these are given in closed form, hence no numerical simulation is required to evaluate the series. This allows, for example, for the explicit computation of Greeks. The weight function deﬁning the Hermite polynomials is a Gaussian density with scale b. We ﬁnd that the rate of convergence for the series depends on b, for which we prove a lower bound to guarantee convergence. Numerical examples show that the series expansion is accurate but unstable for initial values of the underlying process far from zero, mainly due to rounding errors.},
	language = {en},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Lavagnini, Silvia},
	month = apr,
	year = {2021},
	note = {arXiv:2104.11684 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance, Quantitative Finance - Pricing of Securities, finance},
	file = {Lavagnini - 2021 - Pricing Asian Options with Correlators.pdf:C\:\\Users\\isido\\Zotero\\storage\\AJNEHDQB\\Lavagnini - 2021 - Pricing Asian Options with Correlators.pdf:application/pdf},
}

@misc{bilokon_quasi-monte_2022,
	title = {Quasi-{Monte} {Carlo} methods for calculating derivatives sensitivities on the {GPU}},
	url = {http://arxiv.org/abs/2209.11337},
	abstract = {The calculation of option Greeks is vital for risk management. Traditional pathwise and ﬁnitedifference methods work poorly for higher-order Greeks and options with discontinuous payoff functions. The Quasi-Monte Carlo-based conditional pathwise method (QMC-CPW) for options Greeks allows the payoff function of options to be effectively smoothed, allowing for increased efﬁciency when calculating sensitivities. Also demonstrated in literature is the increased computational speed gained by applying GPUs to highly parallelisable ﬁnance problems such as calculating Greeks. We pair QMC-CPW with simulation on the GPU using the CUDA platform. We estimate the delta, vega and gamma Greeks of three exotic options: arithmetic Asian, binary Asian, and lookback. Not only are the beneﬁts of QMC-CPW shown through variance reduction factors of up to 1.0 × 1018, but the increased computational speed through usage of the GPU is shown as we achieve speedups over sequential CPU implementations of more than 200x for our most accurate method.},
	language = {en},
	urldate = {2023-01-30},
	publisher = {arXiv},
	author = {Bilokon, Paul and Kucherenko, Sergei and Williams, Casey},
	month = sep,
	year = {2022},
	note = {arXiv:2209.11337 [q-fin]},
	keywords = {monte carlo, Quantitative Finance - Computational Finance},
	annote = {Comment: 26 pages, 12 figures},
	file = {Bilokon e.a. - 2022 - Quasi-Monte Carlo methods for calculating derivati.pdf:C\:\\Users\\isido\\Zotero\\storage\\6BCIKW77\\Bilokon e.a. - 2022 - Quasi-Monte Carlo methods for calculating derivati.pdf:application/pdf},
}

@article{christara_option_nodate,
	title = {Option pricing in jump diffusion models with quadratic spline collocation},
	abstract = {In this paper, we develop a robust numerical method in pricing options, when the underlying asset follows a jump diffusion model. We demonstrate that, with the quadratic spline collocation method, the integral approximation in the pricing PIDE is intuitively simple, and comes down to the evaluation of the probabilistic moments of the jump density. When combined with a Picard iteration scheme, the pricing problem can be solved efﬁciently. We present the method and the numerical results from pricing European and American options with Merton’s and Kou’s models.},
	language = {en},
	author = {Christara, Christina C and Leung, Nat Chun-Ho},
	keywords = {finance},
	file = {Christara en Leung - Option pricing in jump diffusion models with quadr.pdf:C\:\\Users\\isido\\Zotero\\storage\\Z8AA6EGL\\Christara en Leung - Option pricing in jump diffusion models with quadr.pdf:application/pdf},
}

@misc{hashemi_rectangular_2021,
	title = {Rectangular eigenvalue problems},
	url = {http://arxiv.org/abs/2112.13698},
	abstract = {Often the easiest way to discretize an ordinary or partial diﬀerential equation is by a rectangular numerical method, in which n basis functions are sampled at m ≫ n collocation points. We show how eigenvalue problems can be solved in this setting by QR reduction to square matrix generalized eigenvalue problems. The method applies equally in the limit “m = ∞” of eigenvalue problems for quasimatrices. Numerical examples are presented as well as pointers to some related literature.},
	language = {en},
	urldate = {2023-01-31},
	publisher = {arXiv},
	author = {Hashemi, Behnam and Nakatsukasa, Yuji and Trefethen, Lloyd N.},
	month = dec,
	year = {2021},
	note = {arXiv:2112.13698 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
	file = {Hashemi e.a. - 2021 - Rectangular eigenvalue problems.pdf:C\:\\Users\\isido\\Zotero\\storage\\MWM8W82T\\Hashemi e.a. - 2021 - Rectangular eigenvalue problems.pdf:application/pdf},
}

@misc{trefethen_exactness_2021,
	title = {Exactness of quadrature formulas},
	url = {http://arxiv.org/abs/2101.09501},
	abstract = {The standard design principle for quadrature formulas is that they should be exact for integrands of a given class, such as polynomials of a ﬁxed degree. We show how this principle fails to predict the actual behavior in four cases: Newton–Cotes, Clenshaw–Curtis, Gauss–Legendre, and Gauss–Hermite quadrature. Three further examples are mentioned more brieﬂy.},
	language = {en},
	urldate = {2023-01-31},
	publisher = {arXiv},
	author = {Trefethen, Lloyd N.},
	month = jan,
	year = {2021},
	note = {arXiv:2101.09501 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, integration},
	file = {Trefethen - 2021 - Exactness of quadrature formulas.pdf:C\:\\Users\\isido\\Zotero\\storage\\UVIPUHJ5\\Trefethen - 2021 - Exactness of quadrature formulas.pdf:application/pdf},
}

@misc{nakatsukasa_algorithm_2019,
	title = {An algorithm for real and complex rational minimax approximation},
	url = {http://arxiv.org/abs/1908.06001},
	abstract = {Rational minimax approximation of real functions on real intervals is an established topic, but when it comes to complex functions or domains, there appear to be no algorithms currently in use. Such a method is introduced here, the AAA-Lawson algorithm, available in Chebfun. The new algorithm solves a wide range of problems on arbitrary domains in a fraction of a second of laptop time by a procedure consisting of two steps. First, the standard AAA algorithm is run to obtain a near-best approximation and a set of support points for a barycentric representation of the rational approximant. Then a “Lawson phase” of iteratively reweighted least-squares adjustment of the barycentric coeﬃcients is carried out to improve the approximation to minimax.},
	language = {en},
	urldate = {2023-01-31},
	publisher = {arXiv},
	author = {Nakatsukasa, Yuji and Trefethen, Lloyd N.},
	month = aug,
	year = {2019},
	note = {arXiv:1908.06001 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
	file = {Nakatsukasa en Trefethen - 2019 - An algorithm for real and complex rational minimax.pdf:C\:\\Users\\isido\\Zotero\\storage\\M6YJ3J87\\Nakatsukasa en Trefethen - 2019 - An algorithm for real and complex rational minimax.pdf:application/pdf},
}

@misc{trefethen_multivariate_2016,
	title = {Multivariate polynomial approximation in the hypercube},
	url = {http://arxiv.org/abs/1608.02216},
	abstract = {A theorem is proved concerning approximation of analytic functions by multivariate polynomials in the s-dimensional hypercube. The geometric convergence rate is determined not by the usual notion of degree of a multivariate polynomial, but by the Euclidean degree, deﬁned in terms of the 2-norm rather than the 1-norm of the exponent vector k of a monomial xk11 · · · xkss .},
	language = {en},
	urldate = {2023-01-31},
	publisher = {arXiv},
	author = {Trefethen, Lloyd N.},
	month = aug,
	year = {2016},
	note = {arXiv:1608.02216 [math]},
	keywords = {Mathematics - Numerical Analysis},
	file = {Trefethen - 2016 - Multivariate polynomial approximation in the hyper.pdf:C\:\\Users\\isido\\Zotero\\storage\\HSBRRHS8\\Trefethen - 2016 - Multivariate polynomial approximation in the hyper.pdf:application/pdf},
}

@misc{aurentz_chopping_2015,
	title = {Chopping a {Chebyshev} {Series}},
	url = {http://arxiv.org/abs/1512.01803},
	abstract = {Chebfun and related software projects for numerical computing with functions are based on the idea that at each step of a computation, a function f (x) deﬁned on an interval [a, b] is “rounded” to a prescribed precision by constructing a Chebyshev series and chopping it at an appropriate point. Designing a chopping algorithm with the right properties proves to be a surprisingly complex and interesting problem. We describe the chopping algorithm introduced in Chebfun Version 5.3 in 2015 after many years of discussion and the considerations that led to this design.},
	language = {en},
	urldate = {2023-01-31},
	publisher = {arXiv},
	author = {Aurentz, Jared L. and Trefethen, Lloyd N.},
	month = dec,
	year = {2015},
	note = {arXiv:1512.01803 [math]},
	keywords = {Mathematics - Numerical Analysis, chebychev},
	file = {Aurentz en Trefethen - 2015 - Chopping a Chebyshev Series.pdf:C\:\\Users\\isido\\Zotero\\storage\\Q3HLDCCX\\Aurentz en Trefethen - 2015 - Chopping a Chebyshev Series.pdf:application/pdf},
}

@article{wright_extension_2015,
	title = {Extension of {Chebfun} to periodic functions},
	volume = {37},
	issn = {1064-8275, 1095-7197},
	url = {http://arxiv.org/abs/1511.00166},
	doi = {10.1137/141001007},
	abstract = {Algorithms and underlying mathematics are presented for numerical computation with periodic functions via approximations to machine precision by trigonometric polynomials, including the solution of linear and nonlinear periodic ordinary diﬀerential equations. Diﬀerences from the nonperiodic Chebyshev case are highlighted.},
	language = {en},
	number = {5},
	urldate = {2023-01-31},
	journal = {SIAM Journal on Scientific Computing},
	author = {Wright, Grady B. and Javed, Mohsin and Montanelli, Hadrien and Trefethen, Lloyd N.},
	month = jan,
	year = {2015},
	note = {arXiv:1511.00166 [math]},
	keywords = {Mathematics - Numerical Analysis, chebychev},
	pages = {C554--C573},
	annote = {Comment: 20 pages},
	file = {Wright e.a. - 2015 - Extension of Chebfun to periodic functions.pdf:C\:\\Users\\isido\\Zotero\\storage\\UEQFA5S5\\Wright e.a. - 2015 - Extension of Chebfun to periodic functions.pdf:application/pdf},
}

@misc{noauthor_continuous_nodate-1,
	title = {Continuous analogues of matrix factorizations},
	url = {https://royalsocietypublishing.org/doi/epdf/10.1098/rspa.2014.0585},
	language = {en},
	urldate = {2023-01-31},
	doi = {10.1098/rspa.2014.0585},
	file = {Volledige Tekst:C\:\\Users\\isido\\Zotero\\storage\\24VM589B\\Continuous analogues of matrix factorizations.pdf:application/pdf},
}

@article{townsend_extension_2013,
	title = {An {Extension} of {Chebfun} to {Two} {Dimensions}},
	volume = {35},
	issn = {1064-8275, 1095-7197},
	url = {http://epubs.siam.org/doi/10.1137/130908002},
	doi = {10.1137/130908002},
	abstract = {An object-oriented Matlab system is described that extends the capabilities of Chebfun to smooth functions of two variables deﬁned on rectangles. Functions are approximated to essentially machine precision by using iterative Gaussian elimination with complete pivoting to form “chebfun2” objects representing low rank approximations. Operations such as integration, diﬀerentiation, function evaluation, and transforms are particularly eﬃcient. Global optimization, the singular value decomposition, and rootﬁnding are also extended to chebfun2 objects. Numerical applications are presented.},
	language = {en},
	number = {6},
	urldate = {2023-01-31},
	journal = {SIAM Journal on Scientific Computing},
	author = {Townsend, Alex and Trefethen, Lloyd N.},
	month = jan,
	year = {2013},
	keywords = {chebychev},
	pages = {C495--C518},
	file = {Townsend en Trefethen - 2013 - An Extension of Chebfun to Two Dimensions.pdf:C\:\\Users\\isido\\Zotero\\storage\\X7345UNU\\Townsend en Trefethen - 2013 - An Extension of Chebfun to Two Dimensions.pdf:application/pdf},
}

@article{rivaz_two-dimensional_2015,
	title = {Two-dimensional {Chebyshev} {Polynomials} for {Solving} {Two}-dimensional {Integro}-{Differential} {Equations}},
	abstract = {In this paper, we present a new approach to obtain the numerical solution of the linear twodimensional Fredholm and Volterra integro-differential equations (2D-FIDE and 2D-VIDE). First, we introduce the two-dimensional Chebyshev polynomials and construct their operational matrices of integration. Then, both of them, two-dimensional Chebyshev polynomials and their operational matrix of integration, are used to represent the matrix form of 2D-FIDE and 2D-VIDE. The main characteristic of this approach is that it reduces 2D-FIDE and 2D-VIDE to a system of linear algebraic equations. Illustrative examples are included to demonstrate the validity and applicability of the presented technique.},
	language = {en},
	number = {2},
	author = {Rivaz, Azim},
	year = {2015},
	keywords = {chebychev},
	file = {Rivaz - 2015 - Two-dimensional Chebyshev Polynomials for Solving .pdf:C\:\\Users\\isido\\Zotero\\storage\\SPZJSJ2L\\Rivaz - 2015 - Two-dimensional Chebyshev Polynomials for Solving .pdf:application/pdf},
}

@article{sajikumar_image_nodate,
	title = {{IMAGE} {COMPRESSION} {USING} {CHEBYSHEV} {POLYNOMIAL} {SURFACE} {FIT}},
	abstract = {A lossy image compression method based on block-by-block surface fit using bivariate polynomial is proposed. Chebyshev polynomials of first kind are used to generate the surface for each block. Data compression is achieved by representing the gray level variations across any block by the parameters of the fitted surface and these parameters are stored instead of the pixel values. Compression with three coefficients and four coefficients in the fit model is proposed. In standard lossy compression techniques compression is achieved by exploiting spatial redundancies of the input data. In this proposed method compression does not depends on redundant information but depends on the block size that can be predefined. The method is best suited for high compression with reasonable reconstructed image quality. Performance was tested on number of test images using Chebyshev polynomials of different orders.},
	language = {en},
	author = {Sajikumar, S},
	keywords = {chebychev},
	file = {Sajikumar - IMAGE COMPRESSION USING CHEBYSHEV POLYNOMIAL SURFA.pdf:C\:\\Users\\isido\\Zotero\\storage\\Z7XR22R5\\Sajikumar - IMAGE COMPRESSION USING CHEBYSHEV POLYNOMIAL SURFA.pdf:application/pdf},
}

@misc{noauthor_clenshaw_2022,
	title = {Clenshaw algorithm},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Clenshaw_algorithm&oldid=1089015914},
	abstract = {In numerical analysis, the Clenshaw algorithm, also called Clenshaw summation, is a recursive method to evaluate a linear combination of Chebyshev polynomials.  The method was published by Charles William Clenshaw in 1955. It is a generalization of Horner's method for evaluating a linear combination of monomials.
It generalizes to more than just Chebyshev polynomials; it applies to any class of functions that can be defined by a three-term recurrence relation.},
	language = {en},
	urldate = {2023-01-31},
	journal = {Wikipedia},
	month = may,
	year = {2022},
	note = {Page Version ID: 1089015914},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\HTUIQKE9\\Clenshaw_algorithm.html:text/html},
}

@misc{kirill_answer_2017,
	title = {Answer to "{Clenshaw}-type recurrence for derivative of {Chebyshev} series"},
	url = {https://scicomp.stackexchange.com/a/27866},
	urldate = {2023-01-31},
	journal = {Computational Science Stack Exchange},
	author = {Kirill},
	month = sep,
	year = {2017},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\PMJIJDW5\\clenshaw-type-recurrence-for-derivative-of-chebyshev-series.html:text/html},
}

@misc{pedro_answer_2012,
	title = {Answer to "{Fast} (approximate) evaluation of {Chebyshev} polynomial"},
	url = {https://scicomp.stackexchange.com/a/3416},
	urldate = {2023-01-31},
	journal = {Computational Science Stack Exchange},
	author = {Pedro},
	month = oct,
	year = {2012},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\FX7IQLE7\\fast-approximate-evaluation-of-chebyshev-polynomial.html:text/html},
}

@misc{pan_fast_2022,
	title = {Fast {Approximation} of {Polynomial} {Zeros} and {Matrix} {Eigenvalues}},
	url = {http://arxiv.org/abs/2301.11268},
	abstract = {Given a black box (oracle) for the evaluation of a univariate polynomial p(x) of a degree d, we seek its zeros, that is, the roots of the equation p(x)=0. At FOCS 2016 Louis and Vempala approximated a largest zero of such a real-rooted polynomial within \$1/2{\textasciicircum}b\$, by performing at NR cost of the evaluation of Newton's ratio p(x)/p'(x) at O(b{\textbackslash}log(d)) points x. They readily extended this root-finder to record fast approximation of a largest eigenvalue of a symmetric matrix under the Boolean complexity model. We apply distinct approach and techniques to obtain more general results at the same computational cost.},
	language = {en},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Pan, Victor Y. and Go, Soo and Luan, Qi and Zhao, Liang},
	month = dec,
	year = {2022},
	note = {arXiv:2301.11268 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, Computer Science - Symbolic Computation},
	annote = {Comment: 19 pages, 5 figures},
	file = {Pan e.a. - 2022 - Fast Approximation of Polynomial Zeros and Matrix .pdf:C\:\\Users\\isido\\Zotero\\storage\\WNNTR8QA\\Pan e.a. - 2022 - Fast Approximation of Polynomial Zeros and Matrix .pdf:application/pdf},
}

@misc{dumas_fast_2016,
	title = {Fast {Matrix} {Multiplication} and {Symbolic} {Computation}},
	url = {http://arxiv.org/abs/1612.05766},
	abstract = {The complexity of matrix multiplication (hereafter MM) has been intensively studied since 1969, when Strassen surprisingly decreased the exponent 3 in the cubic cost of the straightforward classical MM to log2(7) ≈ 2.8074. Applications to some fundamental problems of Linear Algebra and Computer Science have been immediately recognized, but the researchers in Computer Algebra keep discovering more and more applications even today, with no sign of slowdown. We survey the unﬁnished history of decreasing the exponent towards its information lower bound 2, recall some important techniques discovered in this process and linked to other ﬁelds of computing, reveal sample surprising applications to fast computation of the inner products of two vectors and summation of integers, and discuss the curse of recursion, which separates the progress in fast MM into its most acclaimed and purely theoretical part and into valuable acceleration of MM of feasible sizes. Then, in the second part of our paper, we cover fast MM in realistic symbolic computations and discuss applications and implementation of fast exact matrix multiplication. We ﬁrst review how most of exact linear algebra can be reduced to matrix multiplication over small ﬁnite ﬁelds. Then we highlight the diﬀerences in the design of approximate and exact implementations of fast MM, taking into account nowadays processor and memory hierarchies. In the concluding section we comment on current perspectives of the study of fast MM.},
	language = {en},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Dumas, Jean-Guillaume and Pan, Victor},
	month = dec,
	year = {2016},
	note = {arXiv:1612.05766 [cs]},
	keywords = {Computer Science - Symbolic Computation},
	file = {Dumas en Pan - 2016 - Fast Matrix Multiplication and Symbolic Computatio.pdf:C\:\\Users\\isido\\Zotero\\storage\\SJLTRMVL\\Dumas en Pan - 2016 - Fast Matrix Multiplication and Symbolic Computatio.pdf:application/pdf},
}

@misc{noauthor_stochastic_2023,
	title = {Stochastic gradient descent},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Stochastic_gradient_descent&oldid=1136419553},
	abstract = {Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.While the basic idea behind stochastic approximation can be traced back to the Robbins–Monro algorithm of the 1950s, stochastic gradient descent has become an important optimization method in machine learning.},
	language = {en},
	urldate = {2023-02-01},
	journal = {Wikipedia},
	month = jan,
	year = {2023},
	note = {Page Version ID: 1136419553},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\C8B3CMLS\\Stochastic_gradient_descent.html:text/html},
}

@misc{noauthor_augmented_2022,
	title = {Augmented {Lagrangian} method},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Augmented_Lagrangian_method&oldid=1130093461#Alternating_direction_method_of_multipliers},
	abstract = {Augmented Lagrangian methods are a certain class of algorithms for solving constrained optimization problems.  They have similarities to penalty methods in that they replace a constrained optimization problem by a series of unconstrained problems and add a penalty term to the objective; the difference is that the augmented Lagrangian method adds yet another term, designed to mimic a Lagrange multiplier.  The augmented Lagrangian is related to, but not identical with the method of Lagrange multipliers.
Viewed differently, the unconstrained objective is the Lagrangian of the constrained problem, with an additional penalty term (the augmentation).
The method was originally known as the method of multipliers, and was studied much in the 1970 and 1980s as a good alternative to penalty methods. It was first discussed by Magnus Hestenes, and by Michael Powell in 1969. The method was studied by R. Tyrrell Rockafellar in relation to Fenchel duality, particularly in relation to proximal-point methods, Moreau–Yosida regularization, and maximal monotone operators: These methods were used in structural optimization.  The method was also studied by Dimitri Bertsekas, notably in his 1982 book, together with extensions involving nonquadratic regularization functions, such as entropic regularization, which gives rise to the "exponential method of multipliers," a method that handles inequality constraints with a twice differentiable augmented Lagrangian function.
Since the 1970s, sequential quadratic programming (SQP) and interior point methods (IPM) have had increasing attention, in part because they more easily use sparse matrix subroutines from numerical software libraries, and in part because IPMs have proven complexity results via the theory of self-concordant functions. The augmented Lagrangian method was rejuvenated by the optimization systems LANCELOT, ALGENCAN and AMPL, which allowed sparse matrix techniques to be used on seemingly dense but "partially separable" problems. The method is still useful for some problems.
Around 2007, there was a resurgence of augmented Lagrangian methods in fields such as total-variation denoising and compressed sensing. In particular, a variant of the standard augmented Lagrangian method that uses partial updates (similar to the Gauss–Seidel method for solving linear equations) known as the alternating direction method of multipliers or ADMM gained some attention.},
	language = {en},
	urldate = {2023-02-01},
	journal = {Wikipedia},
	month = dec,
	year = {2022},
	note = {Page Version ID: 1130093461},
}

@article{yuan_overview_2019,
	title = {An overview of numerical methods for the first kind {Fredholm} integral equation},
	volume = {1},
	issn = {2523-3963, 2523-3971},
	url = {http://link.springer.com/10.1007/s42452-019-1228-3},
	doi = {10.1007/s42452-019-1228-3},
	abstract = {In the field of engineering technology, many problems can be transformed into the first kind Fredholm integral equation, which has a prominent feature called “ill-posedness”. This property makes it difficult to find the analytical solution of first kind Fredholm integral equation. Therefore, how to find the numerical solution of first kind Fredholm integral equation has been a common concern of domestic and overseas scholars in recent years. In this article, various numerical solution methods of first kind Fredholm integral equation are introduced in detail. First, the existence and convergence of the solution of the integral equation are given. Second, the current mainstream numerical methods, such as regularization method, wavelet analysis method and multilevel iteration method are introduced in detail. Finally, we presented a concise overview of the numerical method of first kind Fredholm integral equation.},
	language = {en},
	number = {10},
	urldate = {2023-02-01},
	journal = {SN Applied Sciences},
	author = {Yuan, Di and Zhang, Xinming},
	month = oct,
	year = {2019},
	keywords = {integral equations},
	pages = {1178},
	file = {Yuan en Zhang - 2019 - An overview of numerical methods for the first kin.pdf:C\:\\Users\\isido\\Zotero\\storage\\FTSQCFRP\\Yuan en Zhang - 2019 - An overview of numerical methods for the first kin.pdf:application/pdf},
}

@article{almousa_cme_2022,
	title = {The {CME} method: {Efficient} numerical inverse {Laplace} transformation with {Concentrated} {Matrix} {Exponential} distribution},
	volume = {49},
	issn = {0163-5999},
	shorttitle = {The {CME} method},
	url = {https://dl.acm.org/doi/10.1145/3543146.3543155},
	doi = {10.1145/3543146.3543155},
	abstract = {Numerical inverse Laplace transformation (NILT) is an important tool in the ﬁeld of system modelling and performance analysis. The recently introduced CME method has many important advantages over the alternative numerical inverse Laplace transformation (NILT) methods. It avoids Gibbs oscillation (i.e., does not generate overshoot and undershoot), preserves the monotonicity of functions, its accuracy is gradually improving with the order, and it is numerically more stable than the alternative methods. In this paper we demonstrate these advantages and introduce our tool which implements the CME method and other popular NILT methods.},
	language = {en},
	number = {4},
	urldate = {2023-02-02},
	journal = {ACM SIGMETRICS Performance Evaluation Review},
	author = {Almousa, Salah Al-Deen and Horv´ath, G´abor and Horv´ath, Ill ´es and M´esz´aros, Andr´as and Telek, Mikl ´os},
	month = jun,
	year = {2022},
	pages = {29--34},
	file = {Almousa e.a. - 2022 - The CME method Efficient numerical inverse Laplac.pdf:C\:\\Users\\isido\\Zotero\\storage\\BXPJ7LAI\\Almousa e.a. - 2022 - The CME method Efficient numerical inverse Laplac.pdf:application/pdf},
}

@misc{noauthor_fredholm_nodate,
	title = {fredholm theory of elliptic problems},
	file = {[Monographs in Mathematics №101] Vitaly Volpert (auth.) - Elliptic Partial Differential Equations_ Volume 1_ Fredholm Theory of Elliptic Problems in Unbounded Domains (2011, Birkhäuser) [10.1007_978-3-0346-0537-3] - libge.pdf:C\:\\Users\\isido\\Zotero\\storage\\78DYTQZ7\\[Monographs in Mathematics №101] Vitaly Volpert (auth.) - Elliptic Partial Differential Equations_ Volume 1_ Fredholm Theory of Elliptic Problems in Unbounded Domains (2011, Birkhäuser) [10.1007_978-3-0346-0.pdf:application/pdf},
}

@misc{noauthor_spectral_2023-1,
	title = {Spectral density estimation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Spectral_density_estimation&oldid=1136326946},
	abstract = {In statistical signal processing, the goal of spectral density estimation (SDE) or simply spectral estimation  is to estimate the spectral density (also known as the power spectral density) of a signal from a sequence of time samples of the signal. Intuitively speaking, the spectral density characterizes the frequency content of the signal. One purpose of estimating the spectral density is to detect any periodicities in the data, by observing peaks at the frequencies corresponding to these periodicities.
Some SDE techniques assume that a signal is composed of a limited (usually small) number of generating frequencies plus noise and seek to find the location and intensity of the generated frequencies.  Others make no assumption on the number of components and seek to estimate the whole generating spectrum.},
	language = {en},
	urldate = {2023-02-02},
	journal = {Wikipedia},
	month = jan,
	year = {2023},
	note = {Page Version ID: 1136326946},
}

@misc{noauthor_finite_nodate,
	title = {{FINITE} {DIFFERENCE} {AND} {SPECTRAL} {METHODS} {FOR} {ORDINARY} {AND} {PARTIAL} {DIFFERENTIAL} {EQUATIONS}},
	url = {https://www.math.hmc.edu/~dyong/math165/trefethenbook.pdf},
	urldate = {2023-02-08},
	keywords = {ODE},
	file = {trefethenbook.pdf:C\:\\Users\\isido\\Zotero\\storage\\GEQEN89I\\trefethenbook.pdf:application/pdf},
}

@article{chatterjee_monte_nodate,
	title = {A {Monte} {Carlo} {Algorithm} for the {Solution} of the {One}-{Dimensional} {Wave} {Equation}},
	abstract = {This paper presents a 1D Monte Carlo (MC) algorithm for the solution of the wave equation. Historically, the MC method has not been applied successfully to the solution of wave problems. This can primarily be attributed to the problem of resonance in the frequency-domain Green’s function for finite geometries at length scales greater than half a wavelength. In our previously published work, we have been successful in obtaining a frequency-domain solution at multiple-wavelength length scales through the use of infinite-domain Green’s functions. In this work, we extend the algorithm to problems in the time-domain. The MC method does not require any discretization, and hence the memory requirements are lower than approaches based on discretization. Another advantage of the MC method is that the computational procedure is inherently parallelizable and an almost linear increase in computational speed can be obtained with an increase in the number of processors. The application area of our interest is in the full-wave analysis of IC interconnect structures at multi-GHz frequencies.},
	language = {en},
	author = {Chatterjee, K and Yu, C},
	keywords = {monte carlo, PDE, hyperbolic},
	file = {Chatterjee en Yu - A Monte Carlo Algorithm for the Solution of the On.pdf:C\:\\Users\\isido\\Zotero\\storage\\Z59LHDC5\\Chatterjee en Yu - A Monte Carlo Algorithm for the Solution of the On.pdf:application/pdf},
}

@article{barnett_greens_nodate,
	title = {Greens {Functions} for the {Wave} {Equation}},
	abstract = {I gather together known results on fundamental solutions to the wave equation in free space, and Greens functions in tori, boxes, and other domains. From this the corresponding fundamental solutions for the Helmholtz equation are derived, and, for the 2D case the semiclassical approximation interpreted back in the time-domain. Utility: scarring via time-dependent propagation in cavities; Math 46 course ideas.},
	language = {en},
	author = {Barnett, Alex H},
	keywords = {PDE, hyperbolic},
	file = {Barnett - Greens Functions for the Wave Equation.pdf:C\:\\Users\\isido\\Zotero\\storage\\P38YMD3D\\Barnett - Greens Functions for the Wave Equation.pdf:application/pdf},
}

@article{rath_ears_2022,
	title = {{EARS}: efficiency-aware russian roulette and splitting},
	volume = {41},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{EARS}},
	url = {https://dl.acm.org/doi/10.1145/3528223.3530168},
	doi = {10.1145/3528223.3530168},
	abstract = {Russian roulette and splitting are widely used techniques to increase the efficiency of Monte Carlo estimators. But, despite their popularity, there is little work on how to best apply them. Most existing approaches rely on simple heuristics based on, e.g., surface albedo and roughness. Their efficiency often hinges on user-controlled parameters. We instead iteratively learn optimal Russian roulette and splitting factors during rendering, using a simple and lightweight data structure. Given perfect estimates of variance and cost, our fixed-point iteration provably converges to the optimal Russian roulette and splitting factors that maximize the rendering efficiency. In our application to unidirectional path tracing, we achieve consistent and significant speed-ups over the state of the art.},
	language = {en},
	number = {4},
	urldate = {2023-02-10},
	journal = {ACM Transactions on Graphics},
	author = {Rath, Alexander and Grittmann, Pascal and Herholz, Sebastian and Weier, Philippe and Slusallek, Philipp},
	month = jul,
	year = {2022},
	keywords = {rendering},
	pages = {1--14},
	annote = {has a youtube video
},
	file = {Rath e.a. - 2022 - EARS efficiency-aware russian roulette and splitt.pdf:C\:\\Users\\isido\\Zotero\\storage\\SX8TXAMP\\Rath e.a. - 2022 - EARS efficiency-aware russian roulette and splitt.pdf:application/pdf},
}

@article{west_continuous_2020,
	title = {Continuous multiple importance sampling},
	volume = {39},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3386569.3392436},
	doi = {10.1145/3386569.3392436},
	abstract = {Multiple importance sampling (MIS) is a provably good way to combine a finite set of sampling techniques to reduce variance in Monte Carlo integral estimation. However, there exist integration problems for which a continuum of sampling techniques is available. To handle such cases we establish a continuous MIS (CMIS) formulation as a generalization of MIS to uncountably infinite sets of techniques. Our formulation is equipped with a base estimator that is coupled with a provably optimal balance heuristic and a practical stochastic MIS (SMIS) estimator that makes CMIS accessible to a broad range of problems. To illustrate the effectiveness and utility of our framework, we apply it to three different light transport applications, showing improved performance over the prior state-of-the-art techniques.},
	language = {en},
	number = {4},
	urldate = {2023-02-10},
	journal = {ACM Transactions on Graphics},
	author = {West, Rex and Georgiev, Iliyan and Gruson, Adrien and Hachisuka, Toshiya},
	month = aug,
	year = {2020},
	keywords = {monte carlo, rendering, importance sampling},
	file = {West e.a. - 2020 - Continuous multiple importance sampling.pdf:C\:\\Users\\isido\\Zotero\\storage\\T968PF33\\West e.a. - 2020 - Continuous multiple importance sampling.pdf:application/pdf},
}

@misc{he_adaptive_2021,
	title = {Adaptive {Importance} {Sampling} for {Efficient} {Stochastic} {Root} {Finding} and {Quantile} {Estimation}},
	url = {http://arxiv.org/abs/2102.10631},
	abstract = {In solving simulation-based stochastic root-ﬁnding or optimization problems that involve rare events, such as in extreme quantile estimation, running crude Monte Carlo can be prohibitively ineﬃcient. To address this issue, importance sampling can be employed to drive down the sampling error to a desirable level. However, selecting a good importance sampler requires knowledge of the solution to the problem at hand, which is the goal to begin with and thus forms a circular challenge. We investigate the use of adaptive importance sampling to untie this circularity. Our procedure sequentially updates the importance sampler to reach the optimal sampler and the optimal solution simultaneously, and can be embedded in both sample average approximation and stochastic approximation-type algorithms. Our theoretical analysis establishes strong consistency and asymptotic normality of the resulting estimators. We also demonstrate, via a minimax perspective, the key role of using adaptivity in controlling asymptotic errors. Finally, we illustrate the eﬀectiveness of our approach via numerical experiments.},
	language = {en},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {He, Shengyi and Jiang, Guangxin and Lam, Henry and Fu, Michael C.},
	month = feb,
	year = {2021},
	note = {arXiv:2102.10631 [math, stat]},
	keywords = {monte carlo, Mathematics - Probability, Mathematics - Optimization and Control, Statistics - Methodology, rendering},
	file = {He e.a. - 2021 - Adaptive Importance Sampling for Efficient Stochas.pdf:C\:\\Users\\isido\\Zotero\\storage\\AJ5J334J\\He e.a. - 2021 - Adaptive Importance Sampling for Efficient Stochas.pdf:application/pdf},
}

@inproceedings{west_marginal_2022,
	address = {Daegu Republic of Korea},
	title = {Marginal {Multiple} {Importance} {Sampling}},
	isbn = {978-1-4503-9470-3},
	url = {https://dl.acm.org/doi/10.1145/3550469.3555388},
	doi = {10.1145/3550469.3555388},
	abstract = {Multiple importance sampling (MIS) is a powerful tool to combine different sampling techniques in a provably good manner. MIS requires that the techniques’ probability density functions (PDFs) are readily evaluable point-wise. However, this requirement may not be satisfied when (some of) those PDFs are marginals, i.e., integrals of other PDFs. We generalize MIS to combine samples from such marginal PDFs. The key idea is to consider each marginalization domain as a continuous space of sampling techniques with readily evaluable (conditional) PDFs. We stochastically select techniques from these spaces and combine the samples drawn from them into an unbiased estimator. Prior work has dealt with the special cases of multiple classical techniques or a single marginal one. Our formulation can handle mixtures of those. We apply our marginal MIS formulation to light-transport simulation to demonstrate its utility. We devise a marginal path sampling framework that makes previously intractable sampling techniques practical and significantly broadens the path-sampling choices beyond what is presently possible. We highlight results from two algorithms based on marginal MIS: a novel formulation of path-space filtering at multiple vertices along a camera path and a similar filtering method for photon-density estimation. CCS Concepts: • Computing methodologies → Rendering; Ray tracing.},
	language = {en},
	urldate = {2023-02-10},
	booktitle = {{SIGGRAPH} {Asia} 2022 {Conference} {Papers}},
	publisher = {ACM},
	author = {West, Rex and Georgiev, Iliyan and Hachisuka, Toshiya},
	month = nov,
	year = {2022},
	keywords = {monte carlo, rendering, importance sampling, integration},
	pages = {1--8},
	file = {West e.a. - 2022 - Marginal Multiple Importance Sampling.pdf:C\:\\Users\\isido\\Zotero\\storage\\HGCCANTP\\West e.a. - 2022 - Marginal Multiple Importance Sampling.pdf:application/pdf},
}

@misc{crespo_primary-space_2020,
	title = {Primary-{Space} {Adaptive} {Control} {Variates} using {Piecewise}-{Polynomial} {Approximations}},
	url = {http://arxiv.org/abs/2008.06722},
	abstract = {We present an unbiased numerical integration algorithm that handles both low-frequency regions and high frequency details of multidimensional integrals. It combines quadrature and Monte Carlo integration, by using a quadrature-base approximation as a control variate of the signal. We adaptively build the control variate constructed as a piecewise polynomial, which can be analytically integrated, and accurately reconstructs the low frequency regions of the integrand. We then recover the high-frequency details missed by the control variate by using Monte Carlo integration of the residual. Our work leverages importance sampling techniques by working in primary space, allowing the combination of multiple mappings; this enables multiple importance sampling in quadrature-based integration. Our algorithm is generic, and can be applied to any complex multidimensional integral. We demonstrate its effectiveness with four applications with low dimensionality: transmittance estimation in heterogeneous participating media, low-order scattering in homogeneous media, direct illumination computation, and rendering of distributed effects. Finally, we show how our technique is extensible to integrands of higher dimensionality, by computing the control variate on Monte Carlo estimates of the high-dimensional signal, and accounting for such additional dimensionality on the residual as well. In all cases, we show accurate results and faster convergence compared to previous approaches.},
	language = {en},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Crespo, Miguel and Bernal, Felix and Jarabo, Adrian and Muñoz, Adolfo},
	month = aug,
	year = {2020},
	note = {arXiv:2008.06722 [cs]},
	keywords = {monte carlo, Computer Science - Graphics, rendering, integration, control variates},
	annote = {Comment: 14 pages, 18 figures},
	file = {Crespo e.a. - 2020 - Primary-Space Adaptive Control Variates using Piec.pdf:C\:\\Users\\isido\\Zotero\\storage\\UYAUD2N9\\Crespo e.a. - 2020 - Primary-Space Adaptive Control Variates using Piec.pdf:application/pdf},
}

@article{lfant_numerical_nodate,
	title = {Numerical integration in complex interval arithmetic},
	language = {en},
	author = {Lfant, Fredrik Johansson and Bordeaux, Inria},
	keywords = {integration},
	file = {Lfant en Bordeaux - Numerical integration in complex interval arithmet.pdf:C\:\\Users\\isido\\Zotero\\storage\\WGXY3GTI\\Lfant en Bordeaux - Numerical integration in complex interval arithmet.pdf:application/pdf},
}

@misc{noauthor_fast_nodate,
	title = {Fast {Wavelet} {Transform} ({FWT}) {Algorithm} - {MATLAB} \& {Simulink}},
	url = {https://www.mathworks.com/help/wavelet/ug/fast-wavelet-transform-fwt-algorithm.html?ue},
	urldate = {2023-02-11},
	keywords = {wavelets},
	file = {Fast Wavelet Transform (FWT) Algorithm - MATLAB & Simulink:C\:\\Users\\isido\\Zotero\\storage\\IZRA3VBE\\fast-wavelet-transform-fwt-algorithm.html:text/html},
}

@misc{the_abel_prize_ingrid_2020,
	title = {Ingrid {Daubechies}: {Wavelet} bases: roots, surprises and applications},
	shorttitle = {Ingrid {Daubechies}},
	url = {https://www.youtube.com/watch?v=tMV61BZCrhk},
	abstract = {Yves Meyer's surprising construction of orthonormal bases consisting of dilates and translates of a single smooth function was followed soon after by the development of the Multiresolution Analysis framework in collaboration with Stephane Mallat. As already shown in the presentation by Stephane Mallat, this development was rooted in and used insights from a variety of fields -- ranging from pure harmonic analysis to statistics, quantum physics, geophysics and computer vision. The lecture will discuss some of those diverse roots in more detail, and also show how the new wavelet synthesis, sparked by Yves Meyer's seminal work, led to further progress in all those fields as well as others. Finally, hindsight shows that the new paradigm introduced by wavelet analysis was a first example of the power of  sparse decompositions -- and thus a prelude to another paradigm shift, that of Compressed Sensing, about which more will follow, in the presentation by Emmanuel Candès.},
	urldate = {2023-02-11},
	author = {{The Abel Prize}},
	month = feb,
	year = {2020},
	keywords = {wavelets},
}

@inproceedings{unser_ten_1997,
	address = {San Diego, CA},
	title = {Ten good reasons for using spline wavelets},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=930676},
	doi = {10.1117/12.292801},
	abstract = {The purpose of this note is to highlight some of the unique properties of spline wavelets. These wavelets can be classified in four categories: othogonal (Battle-Lemarié), semi-orthogonal (e.g., B-spline), shift-orthogonal, and biorthogonal (Cohen-DaubechiesFeauveau). Unlike most other wavelet bases, splines have explicit formulae in both the time and frequency domain, which greatly facilitates their manipulation. They allow for a progressive transition between the two extreme cases of a multiresolution: Haar's piecewise constant representation (spline of degree zero) versus Shannon's bandlimited model (which corresponds to a spline of infinite order). Spline wavelets are extremely regular and usually symmetric or anti-symmetric. They can be designed to have compact support and to achieve optimal time-frequency localization (B-spline wavelets). The underlying scaling functions are the B-splines, which are the shortest and most regular scaling functions of order L. Finally, splines have the best approximation properties among all known wavelets of a given order L. In other words, they are the best for approximating smooth functions.},
	language = {en},
	urldate = {2023-02-11},
	author = {Unser, Michael A.},
	editor = {Aldroubi, Akram and Laine, Andrew F. and Unser, Michael A.},
	month = oct,
	year = {1997},
	keywords = {wavelets},
	pages = {422--431},
	file = {Unser - 1997 - Ten good reasons for using spline wavelets.pdf:C\:\\Users\\isido\\Zotero\\storage\\CQW7SJ6F\\Unser - 1997 - Ten good reasons for using spline wavelets.pdf:application/pdf},
}

@misc{toulis_asymptotic_2016,
	title = {Asymptotic and finite-sample properties of estimators based on stochastic gradients},
	url = {http://arxiv.org/abs/1408.2923},
	abstract = {Stochastic optimization procedures, such as stochastic gradient descent, have gained popularity for parameter estimation from large data sets. However, standard stochastic optimization procedures cannot eﬀectively combine numerical stability with statistical and computational eﬃciency. Here, we introduce an implicit stochastic gradient descent procedure, the iterates of which are implicitly deﬁned. Intuitively, implicit iterates shrink the standard iterates. The amount of shrinkage depends on the observed Fisher information matrix, which does not need to be explicitly computed in practice, thus increasing stability without increasing the computational burden. When combined with averaging, the proposed procedure achieves statistical eﬃciency as well. We derive non-asymptotic bounds and characterize the asymptotic distribution of implicit procedures. Our analysis also reveals the asymptotic variance of a number of existing procedures. We demonstrate implicit stochastic gradient descent by further developing theory for generalized linear models, Cox proportional hazards, and M-estimation problems, and by carrying out extensive experiments. Our results suggest that the implicit stochastic gradient descent procedure is poised to become the workhorse of estimation with large data sets.},
	language = {en},
	urldate = {2023-02-11},
	publisher = {arXiv},
	author = {Toulis, Panos and Airoldi, Edoardo M.},
	month = sep,
	year = {2016},
	note = {arXiv:1408.2923 [stat]},
	keywords = {Statistics - Machine Learning, Statistics - Computation, Statistics - Methodology, optimization, gradient descent},
	annote = {Comment: Annals of Statistics, 2016, forthcoming; 71 pages, 37-page main body; 9 figures; 6 tables},
	file = {Toulis en Airoldi - 2016 - Asymptotic and finite-sample properties of estimat.pdf:C\:\\Users\\isido\\Zotero\\storage\\IUAWENHV\\Toulis en Airoldi - 2016 - Asymptotic and finite-sample properties of estimat.pdf:application/pdf},
}

@article{rath_variance-aware_2020,
	title = {Variance-aware path guiding},
	volume = {39},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3386569.3392441},
	doi = {10.1145/3386569.3392441},
	abstract = {Path guiding is a promising tool to improve the performance of path tracing algorithms. However, not much research has investigated what target densities a guiding method should strive to learn for optimal performance. Instead, most previous work pursues the zero-variance goal: The local decisions are guided under the assumption that all other decisions along the random walk will be sampled perfectly. In practice, however, many decisions are poorly guided, or not guided at all. Furthermore, learned distributions are often marginalized, e.g., by neglecting the BSDF. We present a generic procedure to derive theoretically optimal target densities for local path guiding. These densities account for variance in nested estimators, and marginalize provably well over, e.g., the BSDF. We apply our theory in two state-of-the-art rendering applications: a path guiding solution for unidirectional path tracing [Müller et al. 2017] and a guiding method for light source selection for the many lights problem [Vévoda et al. 2018]. In both cases, we observe significant improvements, especially on glossy surfaces. The implementations for both applications consist of trivial modifications to the original code base, without introducing any additional overhead.},
	language = {en},
	number = {4},
	urldate = {2023-02-11},
	journal = {ACM Transactions on Graphics},
	author = {Rath, Alexander and Grittmann, Pascal and Herholz, Sebastian and Vévoda, Petr and Slusallek, Philipp and Křivánek, Jaroslav},
	month = aug,
	year = {2020},
	keywords = {monte carlo, rendering},
	file = {Rath e.a. - 2020 - Variance-aware path guiding.pdf:C\:\\Users\\isido\\Zotero\\storage\\X79P4T7Y\\Rath e.a. - 2020 - Variance-aware path guiding.pdf:application/pdf},
}

@article{kekkonen_bayesian_nodate,
	title = {Bayesian inverse problems},
	language = {en},
	author = {Kekkonen, Hanne},
	keywords = {inverse problem},
	file = {Kekkonen - Bayesian inverse problems.pdf:C\:\\Users\\isido\\Zotero\\storage\\FC5CGCB4\\Kekkonen - Bayesian inverse problems.pdf:application/pdf},
}

@misc{dashti_bayesian_2015,
	title = {The {Bayesian} {Approach} {To} {Inverse} {Problems}},
	url = {http://arxiv.org/abs/1302.6989},
	abstract = {These lecture notes highlight the mathematical and computational structure relating to the formulation of, and development of algorithms for, the Bayesian approach to inverse problems in diﬀerential equations. This approach is fundamental in the quantiﬁcation of uncertainty within applications involving the blending of mathematical models with data. The ﬁnite dimensional situation is described ﬁrst, along with some motivational examples. Then the development of probability measures on separable Banach space is undertaken, using a random series over an inﬁnite set of functions to construct draws; these probability measures are used as priors in the Bayesian approach to inverse problems. Regularity of draws from the priors is studied in the natural Sobolev or Besov spaces implied by the choice of functions in the random series construction, and the Kolmogorov continuity theorem is used to extend regularity considerations to the space of H¨older continuous functions. Bayes’ theorem is derived in this prior setting, and here interpreted as ﬁnding conditions under which the posterior is absolutely continuous with respect to the prior, and determining a formula for the Radon-Nikodym derivative in terms of the likelihood of the data. Having established the form of the posterior, we then describe various properties common to it in the inﬁnite dimensional setting. These properties include well-posedness, approximation theory, and the existence of maximum a posteriori estimators. We then describe measure-preserving dynamics, again on the inﬁnite dimensional space, including Markov chain-Monte Carlo and sequential Monte Carlo methods, and measure-preserving reversible stochastic diﬀerential equations. By formulating the theory and algorithms on the underlying inﬁnite dimensional space, we obtain a framework suitable for rigorous analysis of the accuracy of reconstructions, of computational complexity, as well as naturally constructing algorithms which perform well under mesh reﬁnement, since they are inherently well-deﬁned in inﬁnite dimensions.},
	language = {en},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Dashti, Masoumeh and Stuart, Andrew M.},
	month = jul,
	year = {2015},
	note = {arXiv:1302.6989 [math]},
	keywords = {Mathematics - Probability, inverse problem},
	annote = {Comment: Lecture notes to appear in Handbook of Uncertainty Quantification, Editors R. Ghanem, D. Higdon and H. Owhadi, Springer, 2016},
	file = {Dashti en Stuart - 2015 - The Bayesian Approach To Inverse Problems.pdf:C\:\\Users\\isido\\Zotero\\storage\\L5WQXGWS\\Dashti en Stuart - 2015 - The Bayesian Approach To Inverse Problems.pdf:application/pdf},
}

@misc{stein_multilevel_2023,
	title = {Multilevel {Markov} {Chain} {Monte} {Carlo} for {Bayesian} {Elliptic} {Inverse} {Problems} with {Besov} {Random} {Tree} {Priors}},
	url = {http://arxiv.org/abs/2302.00678},
	abstract = {We propose a multilevel Markov chain Monte Carlo -FEM algorithm to solve elliptic Bayesian inverse problems with ”Besov random tree prior”. These priors are given by a wavelet series with stochastic coeﬃcients, and certain terms in the expansion vanishing at random, according to the law of so-called Galton-Watson trees. This allows to incorporate random fractal structures and large deviations in the log-diﬀusion, which occur naturally in many applications from geophysics or medical imaging. This framework entails two main diﬃculties: First, the associated diﬀusion coeﬃcient does not satisfy a uniform ellipticity condition, which leads to non-integrable terms and thus divergence of standard multilevel estimators. Secondly, the associated space of parameters is Polish, but not a normed linear space. We address the ﬁrst point by introducing cut-oﬀ functions in the estimator to compensate for the non-integrable terms, while the second issue is resolved by employing an independence Metropolis-Hastings sampler. The resulting algorithm converges in the mean-square sense with essentially optimal asymptotic complexity, and dimension-independent acceptance probabilities.},
	language = {en},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Stein, Andreas and Hoang, Viet Ha},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00678 [cs, math]},
	keywords = {monte carlo, Mathematics - Probability, Mathematics - Numerical Analysis},
	annote = {Comment: 31 pages. arXiv admin note: text overlap with arXiv:2302.00522},
	file = {Stein en Hoang - 2023 - Multilevel Markov Chain Monte Carlo for Bayesian E.pdf:C\:\\Users\\isido\\Zotero\\storage\\8HQLRDI8\\Stein en Hoang - 2023 - Multilevel Markov Chain Monte Carlo for Bayesian E.pdf:application/pdf},
}

@article{xiu_high-order_2005,
	title = {High-{Order} {Collocation} {Methods} for {Differential} {Equations} with {Random} {Inputs}},
	volume = {27},
	issn = {1064-8275, 1095-7197},
	url = {http://epubs.siam.org/doi/10.1137/040615201},
	doi = {10.1137/040615201},
	abstract = {Recently there has been a growing interest in designing eﬃcient methods for the solution of ordinary/partial diﬀerential equations with random inputs. To this end, stochastic Galerkin methods appear to be superior to other nonsampling methods and, in many cases, to several sampling methods. However, when the governing equations take complicated forms, numerical implementations of stochastic Galerkin methods can become nontrivial and care is needed to design robust and eﬃcient solvers for the resulting equations. On the other hand, the traditional sampling methods, e.g., Monte Carlo methods, are straightforward to implement, but they do not oﬀer convergence as fast as stochastic Galerkin methods. In this paper, a high-order stochastic collocation approach is proposed. Similar to stochastic Galerkin methods, the collocation methods take advantage of an assumption of smoothness of the solution in random space to achieve fast convergence. However, the numerical implementation of stochastic collocation is trivial, as it requires only repetitive runs of an existing deterministic solver, similar to Monte Carlo methods. The computational cost of the collocation methods depends on the choice of the collocation points, and we present several feasible constructions. One particular choice, based on sparse grids, depends weakly on the dimensionality of the random space and is more suitable for highly accurate computations of practical applications with large dimensional random inputs. Numerical examples are presented to demonstrate the accuracy and eﬃciency of the stochastic collocation methods.},
	language = {en},
	number = {3},
	urldate = {2023-02-13},
	journal = {SIAM Journal on Scientific Computing},
	author = {Xiu, Dongbin and Hesthaven, Jan S.},
	month = jan,
	year = {2005},
	keywords = {ODE},
	pages = {1118--1139},
	file = {Xiu en Hesthaven - 2005 - High-Order Collocation Methods for Differential Eq.pdf:C\:\\Users\\isido\\Zotero\\storage\\PJ6XFAQG\\Xiu en Hesthaven - 2005 - High-Order Collocation Methods for Differential Eq.pdf:application/pdf},
}

@misc{teckentrup_multilevel_2014,
	title = {A {Multilevel} {Stochastic} {Collocation} {Method} for {Partial} {Differential} {Equations} with {Random} {Input} {Data}},
	url = {http://arxiv.org/abs/1404.2647},
	abstract = {Stochastic collocation methods for approximating the solution of partial diﬀerential equations with random input data (e.g., coeﬃcients and forcing terms) suﬀer from the curse of dimensionality whereby increases in the stochastic dimension cause an explosion of the computational eﬀort. We propose and analyze a multilevel version of the stochastic collocation method that, as is the case for multilevel Monte Carlo (MLMC) methods, uses hierarchies of spatial approximations to reduce the overall computational complexity. In addition, our proposed approach utilizes, for approximation in stochastic space, a sequence of multi-dimensional interpolants of increasing ﬁdelity which can then be used for approximating statistics of the solution as well as for building highorder surrogates featuring faster convergence rates. A rigorous convergence and computational cost analysis of the new multilevel stochastic collocation method is provided, demonstrating its advantages compared to standard single-level stochastic collocation approximations as well as MLMC methods. Numerical results are provided that illustrate the theory and the eﬀectiveness of the new multilevel method.},
	language = {en},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Teckentrup, Aretha L. and Jantsch, Peter and Webster, Clayton G. and Gunzburger, Max},
	month = may,
	year = {2014},
	note = {arXiv:1404.2647 [math]},
	keywords = {Mathematics - Numerical Analysis, random ODE},
	file = {Teckentrup e.a. - 2014 - A Multilevel Stochastic Collocation Method for Par.pdf:C\:\\Users\\isido\\Zotero\\storage\\R98M8FWQ\\Teckentrup e.a. - 2014 - A Multilevel Stochastic Collocation Method for Par.pdf:application/pdf},
}

@misc{deelstra_accelerated_2022,
	title = {Accelerated {Computations} of {Sensitivities} for {xVA}},
	url = {http://arxiv.org/abs/2211.17026},
	abstract = {Exposure simulations are fundamental to many xVA calculations and are a nested expectation problem where repeated portfolio valuations create a signiﬁcant computational expense. Sensitivity calculations which require shocked and unshocked valuations in bump-and-revalue schemes exacerbate the computational load. A known reduction of the portfolio valuation cost is understood to be found in polynomial approximations, which we apply in this article to interest rate sensitivities of expected exposures. We consider a method based on the approximation of the shocked and unshocked valuation functions, as well as a novel approach in which the diﬀerence between these functions is approximated. Convergence results are shown, and we study the choice of interpolation nodes. Numerical experiments with interest rate derivatives are conducted to demonstrate the high accuracy and remarkable computational cost reduction. We further illustrate how the method can be extended to more general xVA models using the example of CVA with wrong-way risk.},
	language = {en},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Deelstra, Griselda and Grzelak, Lech A. and Wolf, Felix},
	month = nov,
	year = {2022},
	note = {arXiv:2211.17026 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance, Quantitative Finance - Risk Management, chebyhev},
	file = {Deelstra e.a. - 2022 - Accelerated Computations of Sensitivities for xVA.pdf:C\:\\Users\\isido\\Zotero\\storage\\GCGYS354\\Deelstra e.a. - 2022 - Accelerated Computations of Sensitivities for xVA.pdf:application/pdf},
}

@misc{steffes-lai_interpolation_2013,
	title = {Interpolation methods to compute statistics of a stochastic partial differential equation},
	url = {http://arxiv.org/abs/1309.3853},
	abstract = {This paper considers the analysis of partial diﬀerential equations (PDE) containing multiple random variables. Recently developed collocation methods enable the construction of high-order stochastic solutions by converting a stochastic PDE into a system of deterministic PDEs. This interpolation method requires that the probability distribution of all random input variables is known a priori, which is often not the case in industrially relevant applications. Additionally, this method suﬀers from a curse of dimensionality, i.e., the number of deterministic PDEs to be solved grows exponentially with respect to the number of random variables. This paper presents an alternative interpolation method, based on a radial basis function (RBF) metamodel, to compute statistics of the stochastic PDE. The RBF metamodel can be constructed even if the probability distribution of all random variables is not known. Then, a lot of statistic scenarios with diﬀerent probability distributions of the random variables can be computed with this single metamodel. In order to reduce the model complexity, we present a parameter screening technique which can be combined with an interpolation method to solve a reduced stochastic model. Numerical results of a model problem demonstrate that the RBF metamodel is as fast as a low order collocation approach and achieves a good accuracy. The parameter screening is able to reduce the dimension and, thus, to accelerate the computation of the stochastic solution.},
	language = {en},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Steffes-lai, Daniela and Rosseel, Eveline and Clees, Tanja},
	month = sep,
	year = {2013},
	note = {arXiv:1309.3853 [math]},
	keywords = {PDE, Mathematics - Numerical Analysis, random PDE},
	annote = {Comment: 26 pages},
	file = {Steffes-lai e.a. - 2013 - Interpolation methods to compute statistics of a s.pdf:C\:\\Users\\isido\\Zotero\\storage\\DUFXWZYZ\\Steffes-lai e.a. - 2013 - Interpolation methods to compute statistics of a s.pdf:application/pdf},
}

@misc{gas_chebyshev_2016,
	title = {Chebyshev {Interpolation} for {Parametric} {Option} {Pricing}},
	url = {http://arxiv.org/abs/1505.04648},
	abstract = {Recurrent tasks such as pricing, calibration and risk assessment need to be executed accurately and in real-time. Simultaneously we observe an increase in model sophistication on the one hand and growing demands on the quality of risk management on the other. To address the resulting computational challenges, it is natural to exploit the recurrent nature of these tasks. We concentrate on Parametric Option Pricing (POP) and show that polynomial interpolation in the parameter space promises to reduce run-times while maintaining accuracy. The attractive properties of Chebyshev interpolation and its tensorized extension enable us to identify criteria for (sub)exponential convergence and explicit error bounds. We show that these results apply to a variety of European (basket) options and aﬃne asset models. Numerical experiments conﬁrm our ﬁndings. Exploring the potential of the method further, we empirically investigate the eﬃciency of the Chebyshev method for multivariate and path-dependent options.},
	language = {en},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Gaß, Maximilian and Glau, Kathrin and Mahlstedt, Mirco and Mair, Maximilian},
	month = jul,
	year = {2016},
	note = {arXiv:1505.04648 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance, chebychev, finance},
	annote = {Comment: Multivariate Option Pricing, Complexity Reduction, (Tensorized) Chebyshev Polynomials, Polynomial Interpolation, Fourier Transform Methods, Monte Carlo, Affine Processes},
	file = {Gaß e.a. - 2016 - Chebyshev Interpolation for Parametric Option Pric.pdf:C\:\\Users\\isido\\Zotero\\storage\\3RRJ3YJB\\Gaß e.a. - 2016 - Chebyshev Interpolation for Parametric Option Pric.pdf:application/pdf},
}

@misc{glau_chebyshev_2017,
	title = {The {Chebyshev} method for the implied volatility},
	url = {http://arxiv.org/abs/1710.01797},
	abstract = {The implied volatility is a crucial element of any ﬁnancial toolbox, since it is used for quoting and the hedging of options as well as for model calibration. In contrast to the BlackScholes formula its inverse, the implied volatility, is not explicitly available and numerical approximation is required. We propose a bivariate interpolation of the implied volatility surface based on Chebyshev polynomials. This yields a closed-form approximation of the implied volatility, which is easy to implement and to maintain. We prove a subexponential error decay. This allows us to obtain an accuracy close to machine precision with polynomials of a low degree. We compare the performance of the method in terms of runtime and accuracy to the most common reference methods. In contrast to existing interpolation methods, the proposed method is able to compute the implied volatility for all relevant option data. In this context, numerical experiments conﬁrm a considerable increase in eﬃciency, especially for large data sets.},
	language = {en},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Glau, Kathrin and Herold, Paul and Madan, Dilip B. and Pötz, Christian},
	month = oct,
	year = {2017},
	note = {arXiv:1710.01797 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance, chebychev, finance},
	file = {Glau e.a. - 2017 - The Chebyshev method for the implied volatility.pdf:C\:\\Users\\isido\\Zotero\\storage\\3VZDVADR\\Glau e.a. - 2017 - The Chebyshev method for the implied volatility.pdf:application/pdf},
}

@misc{glau_deep_2020,
	title = {The {Deep} {Parametric} {PDE} {Method}: {Application} to {Option} {Pricing}},
	shorttitle = {The {Deep} {Parametric} {PDE} {Method}},
	url = {http://arxiv.org/abs/2012.06211},
	abstract = {We propose the deep parametric PDE method to solve high-dimensional parametric partial diﬀerential equations. A single neural network approximates the solution of a whole family of PDEs after being trained without the need of sample solutions. As a practical application, we compute option prices in the multivariate Black-Scholes model. After a single training phase, the prices for diﬀerent time, state and model parameters are available in milliseconds. We evaluate the accuracy in the price and a generalisation of the implied volatility with examples of up to 25 dimensions. A comparison with alternative machine learning approaches, conﬁrms the eﬀectiveness of the approach.},
	language = {en},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Glau, Kathrin and Wunderlich, Linus},
	month = dec,
	year = {2020},
	note = {arXiv:2012.06211 [q-fin]},
	keywords = {PDE, Quantitative Finance - Computational Finance, finance, neural networks},
	annote = {Comment: Some examples can be reproduced in our Jupyter Notebook: https://github.com/LWunderlich/DeepPDE/blob/main/TwoAssetsExample/DeepParametricPDEExample.ipynb},
	file = {Glau en Wunderlich - 2020 - The Deep Parametric PDE Method Application to Opt.pdf:C\:\\Users\\isido\\Zotero\\storage\\3AXRVL4J\\Glau en Wunderlich - 2020 - The Deep Parametric PDE Method Application to Opt.pdf:application/pdf},
}

@misc{glau_low-rank_2019,
	title = {Low-rank tensor approximation for {Chebyshev} interpolation in parametric option pricing},
	url = {http://arxiv.org/abs/1902.04367},
	abstract = {Treating high dimensionality is one of the main challenges in the development of computational methods for solving problems arising in ﬁnance, where tasks such as pricing, calibration, and risk assessment need to be performed accurately and in real-time. Among the growing literature addressing this problem, Gass et al. [14] propose a complexity reduction technique for parametric option pricing based on Chebyshev interpolation. As the number of parameters increases, however, this method is aﬀected by the curse of dimensionality. In this article, we extend this approach to treat high-dimensional problems: Additionally exploiting low-rank structures allows us to consider parameter spaces of high dimensions. The core of our method is to express the tensorized interpolation in tensor train (TT) format and to develop an eﬃcient way, based on tensor completion, to approximate the interpolation coeﬃcients. We apply the new method to two model problems: American option pricing in the Heston model and European basket option pricing in the multi-dimensional Black-Scholes model. In these examples we treat parameter spaces of dimensions up to 25. The numerical results conﬁrm the low-rank structure of these problems and the eﬀectiveness of our method compared to advanced techniques.},
	language = {en},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Glau, Kathrin and Kressner, Daniel and Statti, Francesco},
	month = feb,
	year = {2019},
	note = {arXiv:1902.04367 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance, chebyhev, finance},
	file = {Glau e.a. - 2019 - Low-rank tensor approximation for Chebyshev interp.pdf:C\:\\Users\\isido\\Zotero\\storage\\5FSQELEL\\Glau e.a. - 2019 - Low-rank tensor approximation for Chebyshev interp.pdf:application/pdf},
}

@misc{glau_speed-up_2019,
	title = {Speed-up credit exposure calculations for pricing and risk management},
	url = {http://arxiv.org/abs/1912.01280},
	abstract = {We introduce a new method to calculate the credit exposure of European and path-dependent options. The proposed method is able to calculate accurate expected exposure and potential future exposure proﬁles under the riskneutral and the real-world measure. Key advantage of is that it delivers an accuracy comparable to a full re-evaluation and at the same time it is faster than a regression-based method. Core of the approach is solving a dynamic programming problem by function approximation. This yields a closed form approximation along the paths together with the option’s delta and gamma. The simple structure allows for highly eﬃcient evaluation of the exposures, even for a large number of simulated paths. The approach is ﬂexible in the model choice, payoﬀ proﬁles and asset classes. We validate the accuracy of the method numerically for three diﬀerent equity products and a Bermudan interest rate swaption. Benchmarking against the popular least-squares Monte Carlo approach shows that our method is able to deliver a higher accuracy in a faster runtime.},
	language = {en},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Glau, Kathrin and Pachon, Ricardo and Pötz, Christian},
	month = dec,
	year = {2019},
	note = {arXiv:1912.01280 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance, Quantitative Finance - Risk Management, finance},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:1905.00238},
	file = {Glau e.a. - 2019 - Speed-up credit exposure calculations for pricing .pdf:C\:\\Users\\isido\\Zotero\\storage\\5CIFA7XE\\Glau e.a. - 2019 - Speed-up credit exposure calculations for pricing .pdf:application/pdf},
}

@misc{glau_new_2018,
	title = {A new approach for {American} option pricing: {The} {Dynamic} {Chebyshev} method},
	shorttitle = {A new approach for {American} option pricing},
	url = {http://arxiv.org/abs/1806.05579},
	abstract = {We introduce a new method to price American options based on Chebyshev interpolation. In each step of a dynamic programming time-stepping we approximate the value function with Chebyshev polynomials. The key advantage of this approach is that it allows to shift the model-dependent computations into an oﬄine phase prior to the time-stepping. In the oﬄine part a family of generalised (conditional) moments is computed by an appropriate numerical technique such as a Monte Carlo, PDE or Fourier transform based method. Thanks to this methodological ﬂexibility the approach applies to a large variety of models. Online, the backward induction is solved on a discrete Chebyshev grid, and no (conditional) expectations need to be computed. For each time step the method delivers a closed form approximation of the price function along with the options’ delta and gamma. Moreover, the same family of (conditional) moments yield multiple outputs including the option prices for diﬀerent strikes, maturities and diﬀerent payoﬀ proﬁles. We provide a theoretical error analysis and ﬁnd conditions that imply explicit error bounds for a variety of stock price models. Numerical experiments conﬁrm the fast convergence of prices and sensitivities. An empirical investigation of accuracy and runtime also shows an eﬃciency gain compared with the least-square Monte-Carlo method introduced by Longstaﬀ and Schwartz (2001).},
	language = {en},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Glau, Kathrin and Mahlstedt, Mirco and Pötz, Christian},
	month = jun,
	year = {2018},
	note = {arXiv:1806.05579 [q-fin]},
	keywords = {Quantitative Finance - Computational Finance, chebychev, finance},
	file = {Glau e.a. - 2018 - A new approach for American option pricing The Dy.pdf:C\:\\Users\\isido\\Zotero\\storage\\5CD69T7N\\Glau e.a. - 2018 - A new approach for American option pricing The Dy.pdf:application/pdf},
}

@article{nesterov_gradient_2022,
	title = {Gradient {Methods} with {Memory}},
	volume = {37},
	issn = {1055-6788, 1029-4937},
	url = {http://arxiv.org/abs/2105.09241},
	doi = {10.1080/10556788.2020.1858831},
	abstract = {In this paper, we consider gradient methods for minimizing smooth convex functions, which employ the information obtained at the previous iterations in order to accelerate the convergence towards the optimal solution. This information is used in the form of a piece-wise linear model of the objective function, which provides us with much better prediction abilities as compared with the standard linear model. To the best of our knowledge, this approach was never really applied in Convex Minimization to diﬀerentiable functions in view of the high complexity of the corresponding auxiliary problems. However, we show that all necessary computations can be done very eﬃciently. Consequently, we get new optimization methods, which are better than the usual Gradient Methods both in the number of oracle calls and in the computational time. Our theoretical conclusions are conﬁrmed by preliminary computational experiments.},
	language = {en},
	number = {3},
	urldate = {2023-02-14},
	journal = {Optimization Methods and Software},
	author = {Nesterov, Yurii and Florea, Mihai I.},
	month = may,
	year = {2022},
	note = {arXiv:2105.09241 [math]},
	keywords = {Mathematics - Optimization and Control, optimization, gradient descent},
	pages = {936--953},
	annote = {Comment: This is an Accepted Manuscript of an article published by Taylor {\textbackslash}\& Francis in Optimization Methods and Software on 13 Jan 2021, available at https://www.tandfonline.com/doi/10.1080/10556788.2020.1858831},
	file = {Nesterov en Florea - 2022 - Gradient Methods with Memory.pdf:C\:\\Users\\isido\\Zotero\\storage\\EZKMSTTQ\\Nesterov en Florea - 2022 - Gradient Methods with Memory.pdf:application/pdf},
}

@article{anikin_efficient_2022,
	title = {Efficient {Numerical} {Methods} to {Solve} {Sparse} {Linear} {Equations} with {Application} to {PageRank}},
	volume = {37},
	issn = {1055-6788, 1029-4937},
	url = {http://arxiv.org/abs/1508.07607},
	doi = {10.1080/10556788.2020.1858297},
	abstract = {Over the last two decades, the PageRank problem has received increased interest from the academic community as an efficient tool to estimate web-page importance in information retrieval. Despite numerous developments, the design of efficient optimization algorithms for the PageRank problem is still a challenge. This paper proposes three new algorithms with a linear-time complexity for solving the problem over a bounded-degree graph. The idea behind them is to set up the PageRank as a convex minimization problem over a unit simplex, and then solve it using iterative methods with small iteration complexity. Our theoretical results are supported by an extensive empirical justification using real-world and simulated data.},
	language = {en},
	number = {3},
	urldate = {2023-02-14},
	journal = {Optimization Methods and Software},
	author = {Anikin, Anton and Gasnikov, Alexander and Gornov, Alexander and Kamzolov, Dmitry and Maximov, Yury and Nesterov, Yurii},
	month = may,
	year = {2022},
	note = {arXiv:1508.07607 [math]},
	keywords = {linear systems, Mathematics - Optimization and Control, page rank},
	pages = {907--935},
	annote = {Comment: 26 pages; Accepted to Optimization Methods and Software},
	file = {Anikin e.a. - 2022 - Efficient Numerical Methods to Solve Sparse Linear.pdf:C\:\\Users\\isido\\Zotero\\storage\\7AQ4PXSZ\\Anikin e.a. - 2022 - Efficient Numerical Methods to Solve Sparse Linear.pdf:application/pdf},
}

@article{vicini_path_2021,
	title = {Path replay backpropagation: differentiating light paths using constant memory and linear time},
	volume = {40},
	issn = {0730-0301, 1557-7368},
	shorttitle = {Path replay backpropagation},
	url = {https://dl.acm.org/doi/10.1145/3450626.3459804},
	doi = {10.1145/3450626.3459804},
	abstract = {Differentiable physically-based rendering has become an indispensable tool for solving inverse problems involving light. Most applications in this area jointly optimize a large set of scene parameters to minimize an objective function, in which case reverse-mode differentiation is the method of choice for obtaining parameter gradients.
            However, existing techniques that perform the necessary differentiation step suffer from either statistical bias or a prohibitive cost in terms of memory and computation time. For example, standard techniques for automatic differentiation based on program transformation or Wengert tapes lead to impracticably large memory usage when applied to physically-based rendering algorithms. A recently proposed adjoint method by Nimier-David et al. [2020] reduces this to a constant memory footprint, but the computation time for unbiased gradient estimates then becomes quadratic in the number of scattering events along a light path. This is problematic when the scene contains highly scattering materials like participating media.
            In this paper, we propose a new unbiased backpropagation algorithm for rendering that only requires constant memory, and whose computation time is linear in the number of scattering events (i.e., just like path tracing). Our approach builds on the invertibility of the local Jacobian at scattering interactions to recover the various quantities needed for reverse-mode differentiation. Our method also extends to specular materials such as smooth dielectrics and conductors that cannot be handled by prior work.},
	language = {en},
	number = {4},
	urldate = {2023-02-17},
	journal = {ACM Transactions on Graphics},
	author = {Vicini, Delio and Speierer, Sébastien and Jakob, Wenzel},
	month = aug,
	year = {2021},
	keywords = {monte carlo, rendering, inverse problem},
	pages = {1--14},
	file = {Vicini e.a. - 2021 - Path replay backpropagation differentiating light.pdf:C\:\\Users\\isido\\Zotero\\storage\\CYDBTJMN\\Vicini e.a. - 2021 - Path replay backpropagation differentiating light.pdf:application/pdf},
}

@misc{sinha_multilevel_2022,
	title = {Multilevel {Monte} {Carlo} and its {Applications} in {Financial} {Engineering}},
	url = {http://arxiv.org/abs/2209.14549},
	abstract = {In this article, we present a review of the recent developments on the topic of Multilevel Monte Carlo (MLMC) algorithm, in the paradigm of applications in ﬁnancial engineering. We speciﬁcally focus on the recent studies conducted in two subareas, namely, option pricing and ﬁnancial risk management. For the former, the discussion involves incorporation of the importance sampling algorithm, in conjunction with the MLMC estimator, thereby constructing a hybrid algorithm in order to achieve reduction for the overall variance of the estimator. In case of the latter, we discuss the studies carried out in order to construct an efﬁcient algorithm in order to estimate the risk measures of Value-at-Risk (VaR) and Conditional Var (CVaR), in an efﬁcient manner. In this regard, we brieﬂy discuss the motivation and the construction of an adaptive sampling algorithm with an aim to efﬁciently estimate the nested expectation, which, in general is computationally expensive.},
	language = {en},
	urldate = {2023-02-18},
	publisher = {arXiv},
	author = {Sinha, Devang and Chakrabarty, Siddhartha P.},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14549 [q-fin]},
	keywords = {monte carlo, Quantitative Finance - Computational Finance, finance},
	file = {Sinha en Chakrabarty - 2022 - Multilevel Monte Carlo and its Applications in Fin.pdf:C\:\\Users\\isido\\Zotero\\storage\\YZXZQ439\\Sinha en Chakrabarty - 2022 - Multilevel Monte Carlo and its Applications in Fin.pdf:application/pdf},
}

@misc{beck_nonlinear_2020,
	title = {Nonlinear {Monte} {Carlo} methods with polynomial runtime for high-dimensional iterated nested expectations},
	url = {http://arxiv.org/abs/2009.13989},
	abstract = {The approximative calculation of iterated nested expectations is a recurring challenging problem in applications. Nested expectations appear, for example, in the numerical approximation of solutions of backward stochastic diﬀerential equations (BSDEs), in the numerical approximation of solutions of semilinear parabolic partial diﬀerential equations (PDEs), in statistical physics, in optimal stopping problems such as the approximative pricing of American or Bermudan options, in risk measure estimation in mathematical ﬁnance, or in decisionmaking under uncertainty. Nested expectations which arise in the above named applications often consist of a large number of nestings. However, the computational eﬀort of standard nested Monte Carlo approximations for iterated nested expectations grows exponentially in the number of nestings and it remained an open question whether it is possible to approximately calculate multiply iterated high-dimensional nested expectations in polynomial time. In this article we tackle this problem by proposing and studying a new class of full-history recursive multilevel Picard (MLP) approximation schemes for iterated nested expectations. In particular, we prove under suitable assumptions that these MLP approximation schemes can approximately calculate multiply iterated nested expectations with a computational effort growing at most polynomially in the number of nestings K P N “ t1, 2, 3, . . .u, in the problem dimension d P N, and in the reciprocal 1\{\vphantom{\}}ε of the desired approximation accuracy ε P p0, 8q.},
	language = {en},
	urldate = {2023-02-18},
	publisher = {arXiv},
	author = {Beck, Christian and Jentzen, Arnulf and Kruse, Thomas},
	month = sep,
	year = {2020},
	note = {arXiv:2009.13989 [cs, math]},
	keywords = {monte carlo, Mathematics - Probability, Mathematics - Numerical Analysis, Computer Science - Computational Complexity},
	annote = {Comment: 47 pages},
	file = {Beck e.a. - 2020 - Nonlinear Monte Carlo methods with polynomial runt.pdf:C\:\\Users\\isido\\Zotero\\storage\\DERMR4VB\\Beck e.a. - 2020 - Nonlinear Monte Carlo methods with polynomial runt.pdf:application/pdf},
}

@misc{jacquier_stacked_2019,
	title = {Stacked {Monte} {Carlo} for option pricing},
	url = {http://arxiv.org/abs/1903.10795},
	abstract = {We introduce a stacking version of the Monte Carlo algorithm in the context of option pricing. Introduced recently for aeronautic computations, this simple technique, in the spirit of current machine learning ideas, learns control variates by approximating Monte Carlo draws with some speciﬁed function. We describe the method from ﬁrst principles and suggest appropriate ﬁts, and show its eﬃciency to evaluate European and Asian Call options in constant and stochastic volatility models.},
	language = {en},
	urldate = {2023-02-18},
	publisher = {arXiv},
	author = {Jacquier, Antoine and Malone, Emma R. and Oumgari, Mugad},
	month = mar,
	year = {2019},
	note = {arXiv:1903.10795 [math, q-fin]},
	keywords = {monte carlo, Mathematics - Probability, Quantitative Finance - Computational Finance},
	annote = {Comment: 12 pages, 13 Figures},
	file = {Jacquier e.a. - 2019 - Stacked Monte Carlo for option pricing.pdf:C\:\\Users\\isido\\Zotero\\storage\\XMLJ4K7G\\Jacquier e.a. - 2019 - Stacked Monte Carlo for option pricing.pdf:application/pdf},
}

@misc{noauthor_malliavin_2022,
	title = {Malliavin calculus},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Malliavin_calculus&oldid=1115107254},
	abstract = {In probability theory and related fields, Malliavin calculus is a set of mathematical techniques and ideas that extend the mathematical field of calculus of variations from  deterministic functions to stochastic processes. In particular, it allows the computation of derivatives of random variables. Malliavin calculus is also called the stochastic calculus of variations. P. Malliavin first initiated the calculus on infinite dimensional space. Then, the significant contributors such as S. Kusuoka, D. Stroock, Bismut, S. Watanabe, I. Shigekawa, and so on finally completed the foundations.
Malliavin calculus is named after Paul Malliavin whose ideas led to a proof that Hörmander's condition implies the existence and smoothness of a density for the solution of a stochastic differential equation; Hörmander's original proof was based on the theory of  partial differential equations. The calculus has been applied to stochastic partial differential equations as well.
The calculus allows integration by parts with random variables; this operation is used in mathematical finance to compute the sensitivities of financial derivatives. The calculus has applications in, for example, stochastic filtering.},
	language = {en},
	urldate = {2023-02-20},
	journal = {Wikipedia},
	month = oct,
	year = {2022},
	note = {Page Version ID: 1115107254},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\SJFH7GTJ\\Malliavin_calculus.html:text/html},
}

@misc{pentland_error_2022,
	title = {Error bound analysis of the stochastic parareal algorithm},
	url = {http://arxiv.org/abs/2211.05496},
	abstract = {Stochastic parareal (SParareal) is a probabilistic variant of the popular parallel-intime algorithm known as parareal. Similarly to parareal, it combines ﬁne- and coarse-grained solutions to an ordinary diﬀerential equation (ODE) using a predictor-corrector (PC) scheme. The key diﬀerence is that carefully chosen random perturbations are added to the PC to try to accelerate the location of a stochastic solution to the ODE. In this paper, we derive superlinear and linear mean-square error bounds for SParareal applied to nonlinear systems of ODEs using diﬀerent types of perturbations. We illustrate these bounds numerically on a linear system of ODEs and a scalar nonlinear ODE, showing a good match between theory and numerics.},
	language = {en},
	urldate = {2023-02-25},
	publisher = {arXiv},
	author = {Pentland, Kamran and Tamborrino, Massimiliano and Sullivan, T. J.},
	month = nov,
	year = {2022},
	note = {arXiv:2211.05496 [cs, math, stat]},
	keywords = {Mathematics - Numerical Analysis, ODE, Statistics - Computation, parareal},
	file = {Pentland e.a. - 2022 - Error bound analysis of the stochastic parareal al.pdf:C\:\\Users\\isido\\Zotero\\storage\\STV8JTHP\\Pentland e.a. - 2022 - Error bound analysis of the stochastic parareal al.pdf:application/pdf},
}

@misc{carrel_low-rank_2022,
	title = {Low-rank {Parareal}: a low-rank parallel-in-time integrator},
	shorttitle = {Low-rank {Parareal}},
	url = {http://arxiv.org/abs/2203.08455},
	abstract = {In this work, the Parareal algorithm is applied to evolution problems that admit good low-rank approximations and for which the dynamical low-rank approximation (DLRA) can be used as time stepper. Many discrete integrators for DLRA have recently been proposed, based on splitting the projected vector ﬁeld or by applying projected Runge–Kutta methods. The cost and accuracy of these methods are mostly governed by the rank chosen for the approximation. These properties are used in a new method, called low-rank Parareal, in order to obtain a time-parallel DLRA solver for evolution problems. The algorithm is analyzed on aﬃne linear problems and the results are illustrated numerically.},
	language = {en},
	urldate = {2023-02-25},
	publisher = {arXiv},
	author = {Carrel, Benjamin and Gander, Martin J. and Vandereycken, Bart},
	month = sep,
	year = {2022},
	note = {arXiv:2203.08455 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, ODE, parareal},
	file = {Carrel e.a. - 2022 - Low-rank Parareal a low-rank parallel-in-time int.pdf:C\:\\Users\\isido\\Zotero\\storage\\L5268KDM\\Carrel e.a. - 2022 - Low-rank Parareal a low-rank parallel-in-time int.pdf:application/pdf},
}

@article{gander_analysis_2008,
	title = {Analysis of a {Krylov} subspace enhanced parareal algorithm for linear problems},
	volume = {25},
	issn = {1270-900X},
	url = {http://www.esaim-proc.org/10.1051/proc:082508},
	doi = {10.1051/proc:082508},
	abstract = {The parareal algorithm is a numerical method to integrate evolution problems on parallel computers. The performance of the algorithm is well understood for diﬀusive problems, and it can have spectacular performance when applied to certain non-linear problems. Its convergence properties are however less favorable for hyperbolic problems. We present and analyze in this paper a variant of the parareal algorithm, recently proposed in the PITA framework for systems of second order ordinary diﬀerential equations.},
	language = {en},
	urldate = {2023-02-25},
	journal = {ESAIM: Proceedings},
	author = {Gander, M. and Petcu, M.},
	editor = {Cancès, E. and Faure, S. and Graille, B.},
	year = {2008},
	keywords = {ODE, parareal},
	pages = {114--129},
	file = {Gander en Petcu - 2008 - Analysis of a Krylov subspace enhanced parareal al.pdf:C\:\\Users\\isido\\Zotero\\storage\\Q9UAI328\\Gander en Petcu - 2008 - Analysis of a Krylov subspace enhanced parareal al.pdf:application/pdf},
}

@misc{gander_unified_2023,
	title = {A unified analysis framework for iterative parallel-in-time algorithms},
	url = {http://arxiv.org/abs/2203.16069},
	abstract = {Parallel-in-time integration has been the focus of intensive research eﬀorts over the past two decades due to the advent of massively parallel computer architectures and the scaling limits of purely spatial parallelization. Various iterative parallel-in-time (PinT) algorithms have been proposed, like Parareal, PFASST, MGRIT, and Space-Time Multi-Grid (STMG). These methods have been described using diﬀerent notations, and the convergence estimates that are available are diﬃcult to compare. We describe Parareal, PFASST, MGRIT and STMG for the Dahlquist model problem using a common notation and give precise convergence estimates using generating functions. This allows us, for the ﬁrst time, to directly compare their convergence. We prove that all four methods eventually converge super-linearly, and also compare them numerically. The generating function framework provides further opportunities to explore and analyze existing and new methods.},
	language = {en},
	urldate = {2023-02-25},
	publisher = {arXiv},
	author = {Gander, M. J. and Lunet, T. and Ruprecht, D. and Speck, R.},
	month = feb,
	year = {2023},
	note = {arXiv:2203.16069 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, ODE, Computer Science - Computational Engineering, Finance, and Science, parareal},
	file = {2203.16069.pdf:C\:\\Users\\isido\\Zotero\\storage\\ARYQ6U3W\\2203.16069.pdf:application/pdf},
}

@misc{maday_adaptive_2020,
	title = {An {Adaptive} {Parareal} {Algorithm}},
	url = {http://arxiv.org/abs/1909.08333},
	abstract = {In this paper, we consider the problem of accelerating the numerical simulation of time dependent problems by time domain decomposition. The available algorithms enabling such decompositions present severe eﬃciency limitations and are an obstacle for the solution of large scale and high dimensional problems. Our main contribution is the improvement of the parallel eﬃciency of the parareal in time method. The parareal method is based on combining predictions made by a numerically inexpensive solver (with coarse physics and/or coarse resolution) with corrections coming from an expensive solver (with highﬁdelity physics and high resolution). At convergence, the algorithm provides a solution that has the ﬁne solver’s high-ﬁdelity physics and high resolution. In the classical version, the ﬁne solver has a ﬁxed high accuracy which is the major obstacle to achieve a competitive parallel eﬃciency. In this paper, we develop an adaptive variant that overcomes this obstacle by dynamically increasing the accuracy of the ﬁne solver across the parareal iterations. We theoretically show that the parallel eﬃciency becomes very competitive in the ideal case where the cost of the coarse solver is small, thus proving that the only remaining factors impeding full scalability become the cost of the coarse solver and communication time. The developed theory has also the merit of setting a general framework to understand the success of several extensions of parareal based on iteratively improving the quality of the ﬁne solver and re-using information from previous parareal steps. We illustrate the actual performance of the method in stiﬀ ODEs, which are a challenging family of problems since the only mechanism for adaptivity is time and eﬃciency is aﬀected by the cost of the coarse solver.},
	language = {en},
	urldate = {2023-02-25},
	publisher = {arXiv},
	author = {Maday, Y. and Mula, O.},
	month = mar,
	year = {2020},
	note = {arXiv:1909.08333 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, ODE, parareal},
	file = {Maday en Mula - 2020 - An Adaptive Parareal Algorithm.pdf:C\:\\Users\\isido\\Zotero\\storage\\3JZWT34D\\Maday en Mula - 2020 - An Adaptive Parareal Algorithm.pdf:application/pdf},
}

@misc{noauthor_monte_2023,
	title = {Monte {Carlo} integration},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Monte_Carlo_integration&oldid=1135560956},
	abstract = {In mathematics, Monte Carlo integration is a technique for numerical integration using random numbers. It is a particular Monte Carlo method that numerically computes a definite integral. While other algorithms usually evaluate the integrand at a regular grid, Monte Carlo randomly chooses points at which the integrand is evaluated. This method is particularly useful for higher-dimensional integrals.There are different methods to perform a Monte Carlo integration, such as uniform sampling, stratified sampling, importance sampling, sequential Monte Carlo (also known as a particle filter), and mean-field particle methods.},
	language = {en},
	urldate = {2023-03-01},
	journal = {Wikipedia},
	month = jan,
	year = {2023},
	note = {Page Version ID: 1135560956},
	keywords = {monte carlo, integration},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\GELHMB8Z\\Monte_Carlo_integration.html:text/html},
}

@article{lepage_adaptive_2021,
	title = {Adaptive {Multidimensional} {Integration}: {VEGAS} {Enhanced}},
	volume = {439},
	issn = {00219991},
	shorttitle = {Adaptive {Multidimensional} {Integration}},
	url = {http://arxiv.org/abs/2009.05112},
	doi = {10.1016/j.jcp.2021.110386},
	abstract = {We describe a new algorithm, VEGAS+, for adaptive multidimensional Monte Carlo integration. The new algorithm adds a second adaptive strategy, adaptive stratiﬁed sampling, to the adaptive importance sampling that is the basis for its widely used predecessor VEGAS. Both VEGAS and VEGAS+ are effective for integrands with large peaks, but VEGAS+ can be much more effective for integrands with multiple peaks or other signiﬁcant structures aligned with diagonals of the integration volume. We give examples where VEGAS+ is 2–19× more accurate than VEGAS. We also show how to combine VEGAS+ with other integrators, such as the widely available MISER algorithm, to make new hybrid integrators. For a different kind of hybrid, we show how to use integrand samples, generated using MCMC or other methods, to optimize VEGAS+ before integrating. We give an example where preconditioned VEGAS+ is more than 100× as efﬁcient as VEGAS+ without preconditioning. Finally, we give examples where VEGAS+ is more than 10× as efﬁcient as MCMC for Bayesian integrals with D = 3 and 21 parameters. We explain why VEGAS+ will often outperform MCMC for small and moderate sized problems.},
	language = {en},
	urldate = {2023-03-01},
	journal = {Journal of Computational Physics},
	author = {Lepage, G. Peter},
	month = aug,
	year = {2021},
	note = {arXiv:2009.05112 [hep-ph, physics:physics]},
	keywords = {monte carlo, Physics - Computational Physics, integration, High Energy Physics - Phenomenology},
	pages = {110386},
	annote = {Comment: 23 pages, 11 figures},
	file = {Lepage - 2021 - Adaptive Multidimensional Integration VEGAS Enhan.pdf:C\:\\Users\\isido\\Zotero\\storage\\HQJAQY43\\Lepage - 2021 - Adaptive Multidimensional Integration VEGAS Enhan.pdf:application/pdf},
}

@article{mousavi_hamiltonian_2021,
	title = {Hamiltonian {Adaptive} {Importance} {Sampling}},
	volume = {28},
	issn = {1070-9908, 1558-2361},
	url = {http://arxiv.org/abs/2209.13716},
	doi = {10.1109/LSP.2021.3068616},
	abstract = {Importance sampling (IS) is a powerful Monte Carlo (MC) methodology for approximating integrals, for instance in the context of Bayesian inference. In IS, the samples are simulated from the so-called proposal distribution, and the choice of this proposal is key for achieving a high performance. In adaptive IS (AIS) methods, a set of proposals is iteratively improved. AIS is a relevant and timely methodology although many limitations remain yet to be overcome, e.g., the curse of dimensionality in high-dimensional and multi-modal problems. Moreover, the Hamiltonian Monte Carlo (HMC) algorithm has become increasingly popular in machine learning and statistics. HMC has several appealing features such as its exploratory behavior, especially in high-dimensional targets, when other methods suffer. In this paper, we introduce the novel Hamiltonian adaptive importance sampling (HAIS) method. HAIS implements a two-step adaptive process with parallel HMC chains that cooperate at each iteration. The proposed HAIS efﬁciently adapts a population of proposals, extracting the advantages of HMC. HAIS can be understood as a particular instance of the generic layered AIS family with an additional resampling step. HAIS achieves a signiﬁcant performance improvement in high-dimensional problems w.r.t. state-of-the-art algorithms. We discuss the statistical properties of HAIS and show its high performance in two challenging examples.},
	language = {en},
	urldate = {2023-03-01},
	journal = {IEEE Signal Processing Letters},
	author = {Mousavi, Ali and Monsefi, Reza and Elvira, Víctor},
	year = {2021},
	note = {arXiv:2209.13716 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, monte carlo},
	pages = {713--717},
	file = {Mousavi e.a. - 2021 - Hamiltonian Adaptive Importance Sampling.pdf:C\:\\Users\\isido\\Zotero\\storage\\CKVBTMH7\\Mousavi e.a. - 2021 - Hamiltonian Adaptive Importance Sampling.pdf:application/pdf},
}

@article{carrazza_vegasflow_2020,
	title = {{VegasFlow}: accelerating {Monte} {Carlo} simulation across multiple hardware platforms},
	volume = {254},
	issn = {00104655},
	shorttitle = {{VegasFlow}},
	url = {http://arxiv.org/abs/2002.12921},
	doi = {10.1016/j.cpc.2020.107376},
	abstract = {We present VegasFlow , a new software for fast evaluation of high dimensional integrals based on Monte Carlo integration techniques designed for platforms with hardware accelerators. The growing complexity of calculations and simulations in many areas of science have been accompanied by advances in the computational tools which have helped their developments. VegasFlow enables developers to delegate all complicated aspects of hardware or platform implementation to the library so they can focus on the problem at hand. This software is inspired on the Vegas algorithm, ubiquitous in the particle physics community as the driver of cross section integration, and based on Google’s powerful TensorFlow library. We benchmark the performance of this library on many diﬀerent consumer and professional grade GPUs and CPUs.},
	language = {en},
	urldate = {2023-03-01},
	journal = {Computer Physics Communications},
	author = {Carrazza, Stefano and Cruz-Martinez, Juan M.},
	month = sep,
	year = {2020},
	note = {arXiv:2002.12921 [hep-ex, physics:hep-ph, physics:physics, stat]},
	keywords = {Statistics - Machine Learning, monte carlo, Physics - Computational Physics, High Energy Physics - Phenomenology, High Energy Physics - Experiment},
	pages = {107376},
	annote = {Comment: 6 pages, 5 figures, final version published in CPC},
	file = {Carrazza en Cruz-Martinez - 2020 - VegasFlow accelerating Monte Carlo simulation acr.pdf:C\:\\Users\\isido\\Zotero\\storage\\DVF3BGFQ\\Carrazza en Cruz-Martinez - 2020 - VegasFlow accelerating Monte Carlo simulation acr.pdf:application/pdf},
}

@misc{bendavid_efficient_2017,
	title = {Efficient {Monte} {Carlo} {Integration} {Using} {Boosted} {Decision} {Trees} and {Generative} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1707.00028},
	abstract = {New machine learning based algorithms have been developed and tested for Monte Carlo integration based on generative Boosted Decision Trees and Deep Neural Networks. Both of these algorithms exhibit substantial improvements compared to existing algorithms for non-factorizable integrands in terms of the achievable integration precision for a given number of target function evaluations. Large scale Monte Carlo generation of complex collider physics processes with improved eﬃciency can be achieved by implementing these algorithms into commonly used matrix element Monte Carlo generators once their robustness is demonstrated and performance validated for the relevant classes of matrix elements.},
	language = {en},
	urldate = {2023-03-01},
	publisher = {arXiv},
	author = {Bendavid, Joshua},
	month = jun,
	year = {2017},
	note = {arXiv:1707.00028 [hep-ph, physics:physics]},
	keywords = {monte carlo, Physics - Computational Physics, integration, High Energy Physics - Phenomenology, machine learning},
	file = {Bendavid - 2017 - Efficient Monte Carlo Integration Using Boosted De.pdf:C\:\\Users\\isido\\Zotero\\storage\\VAWIJ2CM\\Bendavid - 2017 - Efficient Monte Carlo Integration Using Boosted De.pdf:application/pdf},
}

@article{huang_evaluation_nodate,
	title = {Evaluation of an {Analog} {Accelerator} for {Linear} {Algebra}},
	abstract = {Due to the end of supply voltage scaling and the increasing percentage of dark silicon in modern integrated circuits, researchers are looking for new scalable ways to get useful computation from existing silicon technology. In this paper we present a reconﬁgurable analog accelerator for solving systems of linear equations. Commonly perceived downsides of analog computing, such as low precision and accuracy, limited problem sizes, and difﬁculty in programming are all compensated for using methods we discuss. Based on a prototyped analog accelerator chip we compare the performance and energy consumption of the analog solver against an efﬁcient digital algorithm running on a CPU, and ﬁnd that the analog accelerator approach may be an order of magnitude faster and provide one third energy savings, depending on the accelerator design. Due to the speed and efﬁciency of linear algebra algorithms running on digital computers, an analog accelerator that matches digital performance needs a large silicon footprint. Finally, we conclude that problem classes outside of systems of linear equations may hold more promise for analog acceleration.},
	language = {en},
	author = {Huang, Yipeng and Guo, Ning and Seok, Mingoo and Tsividis, Yannis and Sethumadhavan, Simha},
	keywords = {analog},
	file = {Huang e.a. - Evaluation of an Analog Accelerator for Linear Alg.pdf:C\:\\Users\\isido\\Zotero\\storage\\TF7SI6TR\\Huang e.a. - Evaluation of an Analog Accelerator for Linear Alg.pdf:application/pdf},
}

@misc{noauthor_analog_2023,
	title = {Analog computer},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Analog_computer&oldid=1140592830},
	abstract = {An analog computer or analogue computer is a type of computer that uses the continuous variation aspect of physical phenomena such as electrical, mechanical, or hydraulic quantities (analog signals) to model the problem being solved. In contrast, digital computers represent varying quantities symbolically and by discrete values of both time and amplitude (digital signals).
Analog computers can have a very wide range of complexity. Slide rules and nomograms are the simplest, while naval gunfire control computers and large hybrid digital/analog computers were among the most complicated. Complex mechanisms for process control and protective relays used analog computation to perform control and protective functions.
Analog computers were widely used in scientific and industrial applications even after the advent of digital computers, because at the time they were typically much faster, but they started to become obsolete as early as the 1950s and 1960s, although they remained in use in some specific applications, such as aircraft flight simulators, the flight computer in aircraft, and for teaching control systems in universities. Perhaps the most relatable example of analog computers are mechanical watches where the continuous and periodic rotation of interlinked gears drives the seconds, minutes and hours needles in the clock. More complex applications, such as aircraft flight simulators and synthetic-aperture radar, remained the domain of analog computing (and hybrid computing) well into the 1980s, since digital computers were insufficient for the task.},
	language = {en},
	urldate = {2023-03-01},
	journal = {Wikipedia},
	month = feb,
	year = {2023},
	note = {Page Version ID: 1140592830},
	keywords = {analog},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\78SJPMU4\\Analog_computer.html:text/html},
}

@article{guo_investigation_nodate,
	title = {Investigation of {Energy}-{Efficient} {Hybrid} {Analog}/{Digital} {Approximate} {Computation} in {Continuous} {Time}},
	abstract = {This work investigates energy-efficient approximate computation for solving differential equations. It extends the analog computing techniques to a new paradigm: continuous-time hybrid computation, where both analog and digital circuits operate in continuous time. In this approach, the time intervals in the digital signals contain important information. Unlike conventional synchronous digital circuits, continuous-time digital signals offer the benefits of adaptive power dissipation and no quantization noise. Two prototype chips have been fabricated in 65 nm CMOS technology and tested successfully. The first chip is capable of solving nonlinear differential equations up to 4th order, and the second chip scales up to 16th order based on the first chip. Nonlinear functions are generated by a programmable, clockless, continuous-time 8-bit hybrid architecture (ADC+SRAM+DAC). Digitally-assisted calibration is used in all analog/mixed-signal blocks. Compared to the prior art [1], our chips makes possible arbitrary nonlinearities and achieves 16× lower power dissipation, thanks to technology scaling and extensive use of class-AB analog blocks

Typically, the unit achieves a computational accuracy of about 0.5\% to 5\% RMS, solution times from a fraction of 1 μs to several hundred μs, and total computational energy from a fraction of 1 nJ to hundreds of nJ, depending on equation details. Very significant advantages are observed in computational speed and energy (over two orders of magnitude and over one order of magnitude, respectively) compared to those obtained with a modern MSP430 microcontroller for the same RMS error.},
	language = {en},
	author = {Guo, Ning},
	keywords = {ODE, analog},
	file = {Guo - Investigation of Energy-Efficient Hybrid AnalogDi.pdf:C\:\\Users\\isido\\Zotero\\storage\\DZW3H37R\\Guo - Investigation of Energy-Efficient Hybrid AnalogDi.pdf:application/pdf},
}

@inproceedings{huang_case_2018,
	address = {Shanghai, China},
	title = {A {Case} {Study} in {Analog} {Co}-{Processing} for {Solving} {Stochastic} {Differential} {Equations}},
	isbn = {978-1-5386-6811-5},
	url = {https://ieeexplore.ieee.org/document/8631831/},
	doi = {10.1109/ICDSP.2018.8631831},
	abstract = {Stochastic differential equations (SDEs) are an important class of mathematical models for areas such as physics and ﬁnance. Usually the model outputs are in the form of statistics of the dependent variables, generated from many solutions of the SDE using different samples of the random variables. Challenges in using existing conventional digital computer architectures for solving SDEs include: rapidly generating the random input variables for the SDE solutions, and having to use numerical integration to solve the differential equations. Recent work by our group has explored using hybrid analog-digital computing to solve differential equations. In the hybrid computing model, we solve differential equations by encoding variables as continuous values, which evolve in continuous time. In this paper we review the prior work, and study using the architecture, in conjunction with analog noise, to solve a canonical SDE, the Black-Scholes SDE.},
	language = {en},
	urldate = {2023-03-01},
	booktitle = {2018 {IEEE} 23rd {International} {Conference} on {Digital} {Signal} {Processing} ({DSP})},
	publisher = {IEEE},
	author = {Huang, Yipeng and Guo, Ning and Sethumadhavan, Simha and Seok, Mingoo and Tsividis, Yannis},
	month = nov,
	year = {2018},
	keywords = {analog},
	pages = {1--5},
	file = {Huang e.a. - 2018 - A Case Study in Analog Co-Processing for Solving S.pdf:C\:\\Users\\isido\\Zotero\\storage\\DVZPDWCW\\Huang e.a. - 2018 - A Case Study in Analog Co-Processing for Solving S.pdf:application/pdf},
}

@inproceedings{huang_hybrid_2017,
	address = {Cambridge Massachusetts},
	title = {Hybrid analog-digital solution of nonlinear partial differential equations},
	isbn = {978-1-4503-4952-9},
	url = {https://dl.acm.org/doi/10.1145/3123939.3124550},
	doi = {10.1145/3123939.3124550},
	abstract = {We tackle the important problem class of solving nonlinear partial di↵erential equations. While nonlinear PDEs are typically solved in high-performance supercomputers, they are increasingly used in graphics and embedded systems, where e ciency is important.},
	language = {en},
	urldate = {2023-03-01},
	booktitle = {Proceedings of the 50th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {ACM},
	author = {Huang, Yipeng and Guo, Ning and Seok, Mingoo and Tsividis, Yannis and Mandli, Kyle and Sethumadhavan, Simha},
	month = oct,
	year = {2017},
	keywords = {PDE, analog, gradient descent},
	pages = {665--678},
	file = {Huang e.a. - 2017 - Hybrid analog-digital solution of nonlinear partia.pdf:C\:\\Users\\isido\\Zotero\\storage\\WZ7SMKY8\\Huang e.a. - 2017 - Hybrid analog-digital solution of nonlinear partia.pdf:application/pdf},
}

@article{georgiev_integral_2019,
	title = {Integral formulations of volumetric transmittance},
	volume = {38},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3355089.3356559},
	doi = {10.1145/3355089.3356559},
	abstract = {Computing the light attenuation between two given points is an essential yet expensive task in volumetric light transport simulation. Existing unbiased transmittance estimators are all based on "null-scattering" random walks enabled by augmenting the media with fictitious matter. This formulation prevents the use of traditional Monte Carlo estimator variance analysis, thus the efficiency of such methods is understood from a mostly empirical perspective. In this paper, we present several novel integral formulations of volumetric transmittance in which existing estimators arise as direct Monte Carlo estimators. Breaking from physical intuition, we show that the null-scattering concept is not strictly required for unbiased transmittance estimation, but is a form of control variates for effectively reducing variance. Our formulations bring new insight into the problem and the efficiency of existing estimators. They also provide a framework for devising new types of transmittance estimators with distinct and complementary performance tradeoffs, as well as a clear recipe for applying sample stratification.},
	language = {en},
	number = {6},
	urldate = {2023-03-02},
	journal = {ACM Transactions on Graphics},
	author = {Georgiev, Iliyan and Misso, Zackary and Hachisuka, Toshiya and Nowrouzezahrai, Derek and Křivánek, Jaroslav and Jarosz, Wojciech},
	month = dec,
	year = {2019},
	keywords = {monte carlo, rendering},
	pages = {1--17},
	file = {3355089.pdf:C\:\\Users\\isido\\Zotero\\storage\\J48ZZ86P\\3355089.pdf:application/pdf},
}

@misc{goda_randomizing_2022,
	title = {Randomizing the trapezoidal rule gives the optimal {RMSE} rate in {Gaussian} {Sobolev} spaces},
	url = {http://arxiv.org/abs/2212.11476},
	abstract = {Randomized quadratures for integrating functions in Sobolev spaces of order α ≥ 1, where the integrability condition is with respect to the Gaussian measure, are considered. In this function space, the optimal rate for the worst-case root-mean-squared error (RMSE) is established. Here, optimality is for a general class of quadratures, in which adaptive non-linear algorithms with a possibly varying number of function evaluations are also allowed. The optimal rate is given by showing matching bounds. First, a lower bound on the worst-case RMSE of O(n−α−1/2) is proven, where n denotes an upper bound on the expected number of function evaluations. It turns out that a suitably randomized trapezoidal rule attains this rate, up to a logarithmic factor. A practical error estimator for this trapezoidal rule is also presented. Numerical results support our theory.},
	language = {en},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Goda, Takashi and Kazashi, Yoshihito and Suzuki, Yuya},
	month = dec,
	year = {2022},
	note = {arXiv:2212.11476 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, integration, information complexity},
	annote = {Comment: 21 pages},
	file = {Goda e.a. - 2022 - Randomizing the trapezoidal rule gives the optimal.pdf:C\:\\Users\\isido\\Zotero\\storage\\E7GZXZDN\\Goda e.a. - 2022 - Randomizing the trapezoidal rule gives the optimal.pdf:application/pdf},
}

@misc{mitchell_decomposition_2022,
	title = {Decomposition and conformal mapping techniques for the quadrature of nearly singular integrals},
	url = {http://arxiv.org/abs/2210.09954},
	abstract = {Gauss-Legendre quadrature and the trapezoidal rule are powerful tools for numerical integration of analytic functions. For nearly singular problems, however, these standard methods become unacceptably slow. We discuss and generalize some existing methods for improving on these schemes when the location of the nearby singularity is known. We conclude with an application to some nearly singular surface integrals of viscous flow.},
	language = {en},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Mitchell, William and Natkin, Abbie and Robertson, Paige and Sullivan, Marika and Yu, Xuechen and Zhu, Chenxin},
	month = oct,
	year = {2022},
	note = {arXiv:2210.09954 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, integration},
	file = {2210.09954.pdf:C\:\\Users\\isido\\Zotero\\storage\\I4A6MUMA\\2210.09954.pdf:application/pdf},
}

@misc{izzo_convergence_2022,
	title = {Convergence of a class of high order corrected trapezoidal rules},
	url = {http://arxiv.org/abs/2208.08216},
	abstract = {We present convergence theory for corrected quadrature rules on uniform Cartesian grids for functions with a point singularity. We begin by deriving an error estimate for the punctured trapezoidal rule, and then derive error expansions. We deﬁne the corrected trapezoidal rules, based on the punctured trapezoidal rule, where the weights for the nodes close to the singularity are judiciously corrected based on these expansions. Then we deﬁne the composite corrected trapezoidal rules for a larger family of functions using series expansions around the point singularity and applying corrected trapezoidal rules appropriately. We prove that we can achieve high order accuracy by using a suﬃcient number of correction nodes around the point singularity and of expansion terms.},
	language = {en},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Izzo, Federico and Runborg, Olof and Tsai, Richard},
	month = aug,
	year = {2022},
	note = {arXiv:2208.08216 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, integration},
	annote = {Comment: 23 pages, 1 figure},
	file = {2208.08216.pdf:C\:\\Users\\isido\\Zotero\\storage\\7BMQ6MUX\\2208.08216.pdf:application/pdf},
}

@article{song_high-order_2022,
	title = {High-order implicit time integration scheme based on {Pad}{\textbackslash}'e expansions},
	volume = {390},
	issn = {00457825},
	url = {http://arxiv.org/abs/2103.12282},
	doi = {10.1016/j.cma.2021.114436},
	abstract = {A single-step high-order implicit time integration scheme for the solution of transient and wave propagation problems is presented. It is constructed from the Pad´e expansions of the matrix exponential solution of a system of ﬁrst-order ordinary diﬀerential equations formulated in the state-space. A computationally eﬃcient scheme is developed exploiting the techniques of polynomial factorization and partial fractions of rational functions, and by decoupling the solution for the displacement and velocity vectors. An important feature of the novel algorithm is that no direct inversion of the mass matrix is required. From the diagonal Pad´e expansion of order M a time-stepping scheme of order 2M is developed. Here, each elevation of the accuracy by two orders results in an additional system of real or complex sparse equations to be solved. These systems are comparable in complexity to the standard Newmark method, i.e., the eﬀective system matrix is a linear combination of the static stiﬀness, damping, and mass matrices. It is shown that the second-order scheme is equivalent to Newmark’s constant average acceleration method, often also referred to as trapezoidal rule. The proposed time integrator has been implemented in MATLAB using the built-in direct linear equation solvers. In this article, numerical examples featuring nearly one million degrees of freedom are presented. High-accuracy and eﬃciency in comparison with common second-order time integration schemes are observed. The MATLAB-implementation is available from the authors upon request or from the GitHub repository (to be added).},
	language = {en},
	urldate = {2023-03-06},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Song, Chongmin and Eisenträger, Sascha},
	month = feb,
	year = {2022},
	note = {arXiv:2103.12282 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, ODE, G.1.3, G.1.8, J.2},
	pages = {114436},
	annote = {Comment: 43 pages, 19 figures},
	file = {2103.12282.pdf:C\:\\Users\\isido\\Zotero\\storage\\NTJ24PYR\\2103.12282.pdf:application/pdf},
}

@article{kruse_error_2017,
	title = {Error analysis of randomized {Runge}-{Kutta} methods for differential equations with time-irregular coefficients},
	volume = {17},
	issn = {1609-9389, 1609-4840},
	url = {http://arxiv.org/abs/1701.03444},
	doi = {10.1515/cmam-2016-0048},
	abstract = {This paper contains an error analysis of two randomized explicit Runge-Kutta schemes for ordinary diﬀerential equations (ODEs) with timeirregular coeﬃcient functions. In particular, the methods are applicable to ODEs of Carath´eodory type, whose coeﬃcient functions are only integrable with respect to the time variable but are not assumed to be continuous. A further ﬁeld of application are ODEs with coeﬃcient functions that contain weak singularities with respect to the time variable.},
	language = {en},
	number = {3},
	urldate = {2023-03-06},
	journal = {Computational Methods in Applied Mathematics},
	author = {Kruse, Raphael and Wu, Yue},
	month = jul,
	year = {2017},
	note = {arXiv:1701.03444 [math]},
	keywords = {Mathematics - Numerical Analysis, ODE, information complexity},
	pages = {479--498},
	annote = {Comment: 24 pages, 3 figures},
	file = {1701.03444.pdf:C\:\\Users\\isido\\Zotero\\storage\\WF8T4VEL\\1701.03444.pdf:application/pdf},
}

@misc{wu_randomised_2020,
	title = {A randomised trapezoidal quadrature},
	url = {http://arxiv.org/abs/2011.15086},
	abstract = {A randomised trapezoidal quadrature rule is proposed for continuous functions which enjoys less regularity than commonly required. Indeed, we consider functions in some fractional Sobolev space. Various error bounds for this randomised rule are established while an error bound for classical trapezoidal quadrature is obtained for comparison. The randomised trapezoidal quadrature rule is shown to improve the order of convergence by half.},
	language = {en},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Wu, Yue},
	month = dec,
	year = {2020},
	note = {arXiv:2011.15086 [cs, math]},
	keywords = {Mathematics - Probability, Mathematics - Numerical Analysis, integration, information complexity},
	file = {Wu - 2020 - A randomised trapezoidal quadrature.pdf:C\:\\Users\\isido\\Zotero\\storage\\ZZ4A2GLQ\\Wu - 2020 - A randomised trapezoidal quadrature.pdf:application/pdf},
}

@misc{grant_elementary_2019,
	title = {Elementary numerical methods for double integrals},
	url = {http://arxiv.org/abs/1905.05805},
	abstract = {Approximations to the integral \${\textbackslash}int\_a{\textasciicircum}b{\textbackslash}int\_c{\textasciicircum}d f(x,y){\textbackslash},dy{\textbackslash},dx\$ are obtained under the assumption that the partial derivatives of the integrand are in an \$L{\textasciicircum}p\$ space, for some \$1{\textbackslash}leq p{\textbackslash}leq{\textbackslash}infty\$. We assume \$\{{\textbackslash}lVert f\_\{xy\}{\textbackslash}rVert\}\_p\$ is bounded (integration over \$[a,b]{\textbackslash}times[c,d]\$), assume \$\{{\textbackslash}lVert f\_x({\textbackslash}cdot,c){\textbackslash}rVert\}\_p\$ and \$\{{\textbackslash}lVert f\_x({\textbackslash}cdot,d){\textbackslash}rVert\}\_p\$ are bounded (integration over \$[a,b]\$), and assume \$\{{\textbackslash}lVert f\_y(a,{\textbackslash}cdot){\textbackslash}rVert\}\_p\$ and \$\{{\textbackslash}lVert f\_y(b,{\textbackslash}cdot){\textbackslash}rVert\}\_p\$ are bounded (integration over \$[c,d]\$). The methods are elementary, using only integration by parts and H{\textbackslash}"older's inequality. Versions of the trapezoidal rule, composite trapezoidal rule, midpoint rule and composite midpoint rule are given, with error estimates in terms of the above norms.},
	language = {en},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Grant, Cameron and Talvila, Erik},
	month = may,
	year = {2019},
	note = {arXiv:1905.05805 [math]},
	keywords = {Mathematics - Numerical Analysis, integration, Primary 41A55, 65D30. Secondary 26D15},
	annote = {Comment: To appear in Minnesota Journal of Undergraduate Mathematics},
	file = {Grant en Talvila - 2019 - Elementary numerical methods for double integrals.pdf:C\:\\Users\\isido\\Zotero\\storage\\C2T9FPFP\\Grant en Talvila - 2019 - Elementary numerical methods for double integrals.pdf:application/pdf},
}

@misc{brune_derivative_2018,
	title = {Derivative {Corrections} to the {Trapezoidal} {Rule}},
	url = {http://arxiv.org/abs/1808.04743},
	abstract = {Extensions to the trapezoidal rule using derivative information are studied for periodic integrands and integrals along the entire real line. Integrands which are analytic within a half plane or within a strip containing the path of integration are considered. Derivative-free error bounds are obtained. Alternative approaches to including derivative information are discussed.},
	language = {en},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Brune, Carl R.},
	month = aug,
	year = {2018},
	note = {arXiv:1808.04743 [math]},
	keywords = {Mathematics - Numerical Analysis, integration},
	annote = {Comment: 19 pages, 2 figures},
	file = {Brune - 2018 - Derivative Corrections to the Trapezoidal Rule.pdf:C\:\\Users\\isido\\Zotero\\storage\\ADTJQB7R\\Brune - 2018 - Derivative Corrections to the Trapezoidal Rule.pdf:application/pdf},
}

@misc{talvila_simple_2012,
	title = {Simple derivation of basic quadrature formulas},
	url = {http://arxiv.org/abs/1202.0249},
	abstract = {Simple proofs of the midpoint, trapezoidal and Simpson’s rules are proved for numerical integration on a compact interval. The integrand is assumed to be twice continuously diﬀerentiable for the midpoint and trapezoidal rules, and to be four times continuously diﬀerentiable for Simpson’s rule. Errors are estimated in terms of the uniform norm of second or fourth derivatives of the integrand. The proof uses only integration by parts, applied to the second or fourth derivative of the integrand, multiplied by an appropriate polynomial or piecewise polynomial function. A corrected trapezoidal rule that includes the ﬁrst derivative of the integrand at the endpoints of the integration interval is also proved in this manner, the coeﬃcient in the error estimate being smaller than for the midpoint and trapezoidal rules. The proofs are suitable for presentation in a calculus or elementary numerical analysis class. Several student projects are suggested.},
	language = {en},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Talvila, Erik and Wiersma, Matthew},
	month = feb,
	year = {2012},
	note = {arXiv:1202.0249 [math]},
	keywords = {Mathematics - Numerical Analysis, integration, Mathematics - Classical Analysis and ODEs},
	annote = {Comment: To appear in Atlantic Electronic Journal of Mathematics},
	file = {Talvila en Wiersma - 2012 - Simple derivation of basic quadrature formulas.pdf:C\:\\Users\\isido\\Zotero\\storage\\XSLY6YKE\\Talvila en Wiersma - 2012 - Simple derivation of basic quadrature formulas.pdf:application/pdf},
}

@article{khan_analytical_2019,
	title = {Analytical {Solution} of {Van} {Der} {Pol}’s {Differential} {Equation} {Using} {Homotopy} {Perturbation} {Method}},
	volume = {07},
	issn = {2327-4352, 2327-4379},
	url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/jamp.2019.71001},
	doi = {10.4236/jamp.2019.71001},
	abstract = {In this research work, Homotopy perturbation method (HPM) is applied to find the approximate solution of the Van der Pol Differential equation (VDPDE), which is a well-known nonlinear ODE. Firstly, the approximate solution of Van Der Pol equation is developed using Dirichlet boundary conditions. Then a comparison between the present results and previously published results is presented and a good agreement is observed. Finally, HPM method is applied to find the approximate solution of VDPDE with Robin and Neumann boundary conditions.},
	language = {en},
	number = {01},
	urldate = {2023-03-07},
	journal = {Journal of Applied Mathematics and Physics},
	author = {Khan, Md. Mamun-Ur-Rashid},
	year = {2019},
	keywords = {ODE},
	pages = {1--12},
	file = {Khan - 2019 - Analytical Solution of Van Der Pol’s Differential .pdf:C\:\\Users\\isido\\Zotero\\storage\\S442NHWI\\Khan - 2019 - Analytical Solution of Van Der Pol’s Differential .pdf:application/pdf},
}

@article{soomro_comparison_2013,
	title = {A {Comparison} of {Numerical} {Methods} for {Solving} the {Unforced} {Van} {Der} {Pol}’s {Equation}},
	abstract = {Due to the advancements in the field of computational mathematics, numerical methods are most widely being utilized to solve the equations arising in the fields of applied medical sciences, engineering and technology. In this paper, the numerical solutions of an important equation of applied dynamics: namely, the Unforced Van der Pol’s Equation (UFVDP) are obtained by reducing it to a system of two first order differential equations. The objective of this work is to investigate the efficiency of improved Heun’s (IH) method against the classical Runge-Kutta (RK4) and Mid-point (MP) methods for UFVDP equation. For analysis of accuracy, the Poincare-Lindstedt method has been used as a comparison criterion and respective error bounds are obtained. The results show that the popular RK4 method retains its better accuracy than other methods used for comparison.},
	language = {en},
	author = {Soomro, Abdul Sattar and Tularam, Gurudeo Anand and Shaikh, Muhammad Mujtaba},
	year = {2013},
	keywords = {ODE, perturbation techniques},
	file = {Soomro e.a. - 2013 - A Comparison of Numerical Methods for Solving the .pdf:C\:\\Users\\isido\\Zotero\\storage\\G8ZFGQ6S\\Soomro e.a. - 2013 - A Comparison of Numerical Methods for Solving the .pdf:application/pdf},
}

@article{kritzer_lattice_2019,
	title = {Lattice rules with random \$n\$ achieve nearly the optimal \${\textbackslash}mathcal\{{O}\}(n{\textasciicircum}\{-{\textbackslash}alpha-1/2\})\$ error independently of the dimension},
	volume = {240},
	issn = {00219045},
	url = {http://arxiv.org/abs/1706.04502},
	doi = {10.1016/j.jat.2018.09.011},
	abstract = {We analyze a new random algorithm for numerical integration of \$d\$-variate functions over \$[0,1]{\textasciicircum}d\$ from a weighted Sobolev space with dominating mixed smoothness \${\textbackslash}alpha{\textbackslash}ge 0\$ and product weights \$1{\textbackslash}ge{\textbackslash}gamma\_1{\textbackslash}ge{\textbackslash}gamma\_2{\textbackslash}ge{\textbackslash}cdots{\textgreater}0\$, where the functions are continuous and periodic when \${\textbackslash}alpha{\textgreater}1/2\$. The algorithm is based on rank-\$1\$ lattice rules with a random number of points{\textasciitilde}\$n\$. For the case \${\textbackslash}alpha{\textgreater}1/2\$, we prove that the algorithm achieves almost the optimal order of convergence of \${\textbackslash}mathcal\{O\}(n{\textasciicircum}\{-{\textbackslash}alpha-1/2\})\$, where the implied constant is independent of the dimension{\textasciitilde}\$d\$ if the weights satisfy \${\textbackslash}sum\_\{j=1\}{\textasciicircum}{\textbackslash}infty {\textbackslash}gamma\_j{\textasciicircum}\{1/{\textbackslash}alpha\}{\textless}{\textbackslash}infty\$. The same rate of convergence holds for the more general case \${\textbackslash}alpha{\textgreater}0\$ by adding a random shift to the lattice rule with random \$n\$. This shows, in particular, that the exponent of strong tractability in the randomized setting equals \$1/({\textbackslash}alpha+1/2)\$, if the weights decay fast enough. We obtain a lower bound to indicate that our results are essentially optimal. This paper is a significant advancement over previous related works with respect to the potential for implementation and the independence of error bounds on the problem dimension. Other known algorithms which achieve the optimal error bounds, such as those based on Frolov's method, are very difficult to implement especially in high dimensions. Here we adapt a lesser-known randomization technique introduced by Bakhvalov in 1961. This algorithm is based on rank-\$1\$ lattice rules which are very easy to implement given the integer generating vectors. A simple probabilistic approach can be used to obtain suitable generating vectors.},
	language = {en},
	urldate = {2023-03-14},
	journal = {Journal of Approximation Theory},
	author = {Kritzer, Peter and Kuo, Frances Y. and Nuyens, Dirk and Ullrich, Mario},
	month = apr,
	year = {2019},
	note = {arXiv:1706.04502 [math]},
	keywords = {Mathematics - Numerical Analysis, integration, information complexity},
	pages = {96--113},
	annote = {Comment: 17 pages},
	file = {Kritzer e.a. - 2019 - Lattice rules with random \$n\$ achieve nearly the o.pdf:C\:\\Users\\isido\\Zotero\\storage\\85WBUZK2\\Kritzer e.a. - 2019 - Lattice rules with random \$n\$ achieve nearly the o.pdf:application/pdf},
}

@misc{kacewicz_almost_2006,
	title = {Almost {Optimal} {Solution} of {Initial}-{Value} {Problems} by {Randomized} and {Quantum} {Algorithms}},
	url = {http://arxiv.org/abs/quant-ph/0510045},
	abstract = {We establish essentially optimal bounds on the complexity of initial-value problems in the randomized and quantum settings. For this purpose we define a sequence of new algorithms whose error/cost properties improve from step to step. These algorithms yield new upper complexity bounds, which differ from known lower bounds by only an arbitrarily small positive parameter in the exponent, and a logarithmic factor. In both the randomized and quantum settings, initial-value problems turn out to be essentially as difficult as scalar integration.},
	language = {en},
	urldate = {2023-03-18},
	publisher = {arXiv},
	author = {Kacewicz, Boleslaw},
	month = oct,
	year = {2006},
	note = {arXiv:quant-ph/0510045},
	keywords = {ODE, Quantum Physics, information complexity},
	annote = {Comment: 16 pages, minor presentation changes},
	file = {Kacewicz - 2006 - Almost Optimal Solution of Initial-Value Problems .pdf:C\:\\Users\\isido\\Zotero\\storage\\5U78U57J\\Kacewicz - 2006 - Almost Optimal Solution of Initial-Value Problems .pdf:application/pdf},
}

@article{coulibaly_quasi-randomized_1999,
	title = {A quasi-randomized {Runge}-{Kutta} method},
	volume = {68},
	issn = {0025-5718},
	url = {http://www.ams.org/journal-getitem?pii=S0025-5718-99-01056-X},
	doi = {10.1090/S0025-5718-99-01056-X},
	abstract = {We analyze a quasi-Monte Carlo method to solve the initial-value problem for a system of diﬀerential equations y (t) = f (t, y(t)). The function f is smooth in y and we suppose that f and Dy1f are of bounded variation in t and that Dy2f is bounded in a neighborhood of the graph of the solution. The method is akin to the second order Heun method of the Runge-Kutta family. It uses a quasi-Monte Carlo estimate of integrals. The error bound involves the square of the step size as well as the discrepancy of the point set used for quasi-Monte Carlo approximation. Numerical experiments show that the quasi-randomized method outperforms a recently proposed randomized numerical method.},
	language = {en},
	number = {226},
	urldate = {2023-03-06},
	journal = {Mathematics of Computation},
	author = {Coulibaly, Ibrahim and Lécot, Christian},
	month = apr,
	year = {1999},
	keywords = {ODE, information complexity},
	pages = {651--660},
	file = {Coulibaly en Lécot - 1999 - A quasi-randomized Runge-Kutta method.pdf:C\:\\Users\\isido\\Zotero\\storage\\LQ4QYKR2\\Coulibaly en Lécot - 1999 - A quasi-randomized Runge-Kutta method.pdf:application/pdf},
}

@misc{noauthor_stiff_nodate,
	title = {Stiff {Differential} {Equations}},
	url = {https://www.mathworks.com/company/newsletters/articles/stiff-differential-equations.html},
	abstract = {Stiffness is a subtle, difficult, and important - concept in the numerical solution of ordinary differential equations.},
	language = {en},
	urldate = {2023-03-07},
	keywords = {ODE, stiff ODE},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\674H6FG6\\stiff-differential-equations.html:text/html},
}

@misc{noauthor_perturbation_2023,
	title = {Perturbation theory},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Perturbation_theory&oldid=1131120602},
	abstract = {In mathematics and applied mathematics, perturbation theory comprises methods for finding an approximate solution to a problem, by starting from the exact solution of a related, simpler problem. A critical feature of the technique is a middle step that breaks the problem into "solvable" and "perturbative" parts.  In perturbation theory, the solution is expressed as a power series in a small parameter 
  
    
      
        ε
      
    
    \{{\textbackslash}displaystyle {\textbackslash}varepsilon \}
  . The first term is the known solution to the solvable problem.  Successive terms in the series at higher powers of 
  
    
      
        ε
      
    
    \{{\textbackslash}displaystyle {\textbackslash}varepsilon \}
   usually become smaller.  An approximate 'perturbation solution' is obtained by truncating the series, usually by keeping only the first two terms, the solution to the known problem and the 'first order' perturbation correction.
Perturbation theory is used in a wide range of fields, and reaches its most sophisticated and advanced forms in quantum field theory. Perturbation theory (quantum mechanics) describes the use of this method in quantum mechanics. The field in general remains actively and heavily researched across multiple disciplines.},
	language = {en},
	urldate = {2023-03-07},
	journal = {Wikipedia},
	month = jan,
	year = {2023},
	note = {Page Version ID: 1131120602},
	keywords = {perturbation techniques},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\Y4XGJLUT\\Perturbation_theory.html:text/html},
}

@article{tzitzouris_notes_nodate,
	title = {Notes on {Perturbation} {Techniques} for {ODEs}},
	language = {en},
	author = {Tzitzouris, James A},
	keywords = {perturbation techniques},
	file = {Tzitzouris - Notes on Perturbation Techniques for ODEs.pdf:C\:\\Users\\isido\\Zotero\\storage\\DYKBRWX6\\Tzitzouris - Notes on Perturbation Techniques for ODEs.pdf:application/pdf},
}

@book{kuehn_multiple_2015,
	address = {Cham},
	series = {Applied {Mathematical} {Sciences}},
	title = {Multiple {Time} {Scale} {Dynamics}},
	volume = {191},
	isbn = {978-3-319-12315-8 978-3-319-12316-5},
	url = {https://link.springer.com/10.1007/978-3-319-12316-5},
	abstract = {This book aims to provide an introduction to dynamical systems with multiple time scales. As in any overview book, several topics are covered only quite briefly. My aim was to focus on topics that seem to be less available in introductory form. However, I try to give a global view of the subject by covering a broad spectrum of ideas and tools. The detailed bibliography aims to direct the reader to further topics. To explain it with a simple metaphor: using this book should make you more familiar with a country’s map, culture, and main attractions rather than imparting details of every street in just one city. Both things are useful at times. The term “multiple time scale dynamics” is rather modern. The subject and many of its core ideas are much older. For example, “singular perturbation theory” or “multiscale systems” encompass a larger variety of topics than what I present here. On the one hand, no serious multidimensional spatial problems are considered in this book. Furthermore, there are many singularly perturbed problems that have very little to do with dynamical systems. On the other hand, ordinary differential equations (ODEs) with multiple time scales already contain motivation, technique, and intuition for more complicated scenarios. Classical singular perturbation theory for multiple time scale systems provides many asymptotic techniques centered on series expansions, matching, and averaging. These methods are still indispensable today, and this book gives an overview of them. However, the details are not covered, since many excellent introductory texts are available. The last two decades have brought major additional progress with a particular focus on geometric ideas as well as powerful numerical algorithms. A major goal of this book was to merge several viewpoints with a wide variety of different techniques into a unified framework. Another reason for the broad choice of topics was to make it easier for students and researchers new to the field to get a much quicker overview. Again, I would like to warn the reader that this book is obviously not a mathematical monograph aiming at a complete treatment of the entire field of multiple time scales. Some readers, particularly students, may wonder how a book of over 700 pages can be only an “introduction,” but let me point out that most chapters, and even many five-page sections, in this book in fact deserve their own mathematical monograph of 300 pages or more. A few such books have

vi PREFACE been written, while many exist only in a distant, happier future. I encourage my colleagues working in the field—you know who you are—to begin work on such projects and fill in the missing mathematical details that I decided to leave out in order to make the subject much more accessible to beginners. Despite the simplifications, there seem to be several advantages of the style of presentation. The great diversity of the subject, ranging from mathematical theory in dynamics, analysis, geometry, topology, stochastics, and numerics to virtually all fields in science and engineering applications, easily becomes visible. The unity and interconnections between different approaches to multiple time scale problems can be identified much more readily. Also, scientists with particular applications in mind should find it easier to spot many potential tools right away, while a “purer” mathematician can use this text as a source book of open mathematical problems. The target audience of the book is senior undergraduates, graduate students, as well as researchers interested in using the theory of multiple time scale dynamics in nonlinear science, either from a theoretical or a mathematical modeling perspective. Section 1.1provides a more detailed guide to the book. Now I have the pleasure of thanking several colleagues, collaborators, and institutions that have helped to get this book started, keep it on track, and eventually push it over the finish line. First and foremost, I would like to thank my thesis adviser, John Guckenheimer, for introducing me to the field during my time as a graduate student. Undoubtedly, he shaped my view of the field, and without his support and encouragement, I would never have attempted to undertake a book project on multiple time scale systems. Important influences on this book during my postdoctoral years have come from my colleagues Thilo Gross, Peter Szmolyan, Nils Berglund, and Barbara Gentz. Thilo helped me to form bridges from multiscale dynamics to such seemingly distant areas as ecology, networks, systems biology, and statistical physics. I would like to thank Peter for sharing his tremendous insights into all aspects of geometric multiscale dynamics. Nils and Barbara have been constant sources of inspiration on everything stochastic. Although it is clear that I am responsible for all potential errors that may remain within this version, I would like to thank several colleagues who responded with valuable feedback—alerting me to anything from tiny typos to blatant blunders—in various draft versions of this book: Nils Berglund, Alan Champneys, Hayato Chiba, Mike Cortez, Peter De Maesschalck, John Guckenheimer, Pavel Gurevich, Annalisa Iuorio, Mike Jeffrey, Hans Kaper, Daniel Karrasch, Chris Jones, Ilona Kosiuk, Steven Lade, Gabriel Lord, Anatoly Neishtadt, Clare Perryman, Sofia Piltz, Nikola Popovic, Jens Rademacher, Martin Rasmussen, Martin Riedler, Stephen Schecter, Jan Sieber, Eric Siero, Peter Szmolyan, Frits Veerman, Martin Wechselberger, and Antonios Zagaris. Furthermore, I would like to thank the production staff at Springer for the handling of my manuscript. In particular, Achi Dosanjh has been extremely important and tremendously helpful in leading the entire editorial process. Regarding the formatting of the book, I would also like to thank Yuri Kuznetsov

PREFACE vii who shared his LATEX book formatting preamble with me, from which I took some inspiration for the format of this book. Several anonymous referees also provided very valuable feedback, which helped to improve the book. During the writing of this book I have also benefited from the generous hospitality and financial support of various institutions, including Cornell University, the Max Planck Institute for Physics of Complex Systems, and the Vienna University of Technology. Furthermore, I would like to thank the Austrian Academy of Sciences for support via the “Austrian Programme for Advanced Research and Technology” and the European Commission for support via a “Marie Curie International Reintegration Grant.” The final push of this project has been supported through the program “Oberwolfach Leibniz Fellows” by the Mathematisches Forschungsinstitut Oberwolfach. Although it is obvious for an overview book on a topic, let me stress that I do not make any claims to novelty of its content. I have tried to summarize and condense the extensive literature on multiple time scale dynamics into a more accessible expository format. However, I can certainly say that during the writing of this book, several very natural new ideas arose. I hope that the research-oriented reader will have a similar experience and that this book will provide a starting point for new ideas in multiscale dynamics.},
	language = {en},
	urldate = {2023-03-08},
	publisher = {Springer International Publishing},
	author = {Kuehn, Christian},
	year = {2015},
	doi = {10.1007/978-3-319-12316-5},
	keywords = {perturbation techniques},
	file = {Kuehn - 2015 - Multiple Time Scale Dynamics.pdf:C\:\\Users\\isido\\Zotero\\storage\\KEKJNI5S\\Kuehn - 2015 - Multiple Time Scale Dynamics.pdf:application/pdf},
}

@misc{noauthor_intro_nodate,
	title = {intro perturbation theory},
	abstract = {First, let me say hello and welcome to the subject of perturbation methods. For those who may be unfamiliar with the topic, the title can be confusing. The first time I became aware of this was during a family reunion when someone asked what I did as a mathematician. This is not an easy question to answer, but I started by describing how a certain segment of the applied mathematics community was interested in problems that arise from physical problems. Examples such as water waves, sound propagation, and the aerodynamics of airplanes were discussed. The difficulty of solving such problems was also described in exaggerated detail. Next came the part about how one generally ends up using a computer to actually find the solution. At this point I editorialized on the limitations of computer solutions and why it is important to derive, if at all possible, accurate approximations of the solution. This lead naturally to the mentioning of asymptotics and perturbation methods. These terms ended the conversation because I was unprepared for their reactions. They were not sure exactly what asymptotics meant, but they were quite perplexed about perturbation methods. I tried, unsuccessfully, to explain what it means, but it was not until sometime later that I realized the difficulty. For them, as in Webster's Collegiate Dictionary, the first two meanings for the word perturb are "to disturb greatly in mind (disquiet); to throw into confusion (disorder)." Although a cynic might suggest this is indeed appropriate for the subject, the intent is exactly the opposite. (For a related comment, see Exercise 3.4.1(d).) In a nutshell, this book serves as an introduction into how to systematically construct an approximation of the solution of a problem that i

viii Preface otherwise intractable. The methods all rely on there being a parameter in the problem that is relatively small. Such a situation is relatively common in applications, and this is one of the reasons that perturbation methods are a cornerstone of applied mathematics. One of the other cornerstones is scientific computing, and it is interesting that the two subjects have grown up together. However, this is not unexpected given their respective capabilities. When using a computer, one is capable of solving problems that are nonlinear, inhomogeneous, and multidimensional. Moreover, it is possible to achieve very high accuracy. The drawbacks are that computer solutions do not provide much insight into the physics of the problem (particularly for those who do not have access to the appropriate software or computer), and there is always the question as to whether or not the computed solution is correct. On the other hand, perturbation methods are also capable of dealing with nonlinear, inhomogeneous, and multidimensional problems (although not to the same extent as computer-generated solutions). The principal objective when using perturbation methods, at least as far as the author is concerned, is to provide a reasonably accurate expression for the solution. By doing this one is able to derive an understanding of the physics of the problem. Also, one can use the result, in conjunction with the original problem, to obtain more efficient numerical procedures for computing the solution. The methods covered in the text vary widely in their applicability. The first chapter introduces the fundamental ideas underlying asymptotic approximations. This includes their use in constructing approximate solutions of transcendental equations as well as differential equations. In the second chapter, matched asymptotic expansions are used to analyze problems with layers. Chapter 3 describes a method for dealing with problems with more than one time scale. In Chapter 4, the WKB method for analyzing linear singular perturbation problems is developed, while in Chapter 5 a method for dealing with materials containing disparate spatial scales (e.g., microscopic versus macroscopic) is discussed. The last chapter examines the topics of multiple solutions and stability. The mathematical prerequisites for this text include a basic background in differential equations and advanced calculus. In terms of difficulty, the chapters are written so that the first sections are either elementary or intermediate, while the later sections are somewhat more advanced. Also, the ideas developed in each chapter are applied to a spectrum of problems, including ordinary differential equations, partial differential equations, and difference equations. Scattered through the exercises are applications to integral equations, integra-differential equations, differential-difference equations, and delay equations. What will not be found is an in-depth discussion of the theory underlying the methods. This aspect of the subject is important, and references to the more theoretical work in the area are given in each chapter

Preface ix The exercises in each section vary in their complexity. In addition to the more standard textbook problems, an attempt has been made to include problems from the research literature. The latter are intended to provide a window into the wide range of areas that use perturbation methods. Solutions to some of the exercises are available from the author's home page located at http://www.math.rpi.edu/"'holmes. Also located there is an errata list. Those who may want to make a contribution to one of these files, or have suggestions about the text, can reach the author at holmes@rpi.edu. I would like to express my gratitude to the many students who took my course in perturbation methods at Rensselaer. They helped me immeasurably in understanding the subject and provided much needed encouragement to write this book. It is a pleasure to acknowledge the suggestions of Jon Bell, Ash Kapila, and Bob O'Malley, who read early versions of the manuscript. I would also like to thank Julian Cole, who first introduced me to perturbation methods and is still, to this day, showing me what the subject is about.},
	keywords = {perturbation techniques},
	file = {[Texts in Applied Mathematics №20] Mark H. Holmes (auth.) - Introduction to Perturbation Methods (1995, Springer) [10.1007_978-1-4612-5347-1] - libgen.li.pdf:C\:\\Users\\isido\\Zotero\\storage\\A7D8KXBJ\\[Texts in Applied Mathematics №20] Mark H. Holmes (auth.) - Introduction to Perturbation Methods (1995, Springer) [10.1007_978-1-4612-5347-1] - libgen.li.pdf:application/pdf},
}

@misc{wijgerden_tail_2022,
	title = {Tail recursion for branching problems},
	url = {https://jeroenvanwijgerden.me/post/recursion-1/},
	abstract = {Recently one of my clients asked me for help in learning about graphs. As graphs are a particular favorite topic of mine I took on this challenge with zeal and alacrity. In our ensuing sessions we touched upon some interesting topics regarding recursion, which inspired me to write this article.

I assume you, dear reader, are already somewhat familiar with recursion. Nevertheless, I begin this article with a brief recap of a classic recursive solution: calculating a factorial.

Then I introduce the what and why of tail recursion and how to modify regular recursion to tail recursion.

The meat of this article is a discussion of how to use regular recursion and tail recursion to solve branching problems. The particular branching problem I use as an example is finding paths in a tree.

I discuss not one but two different approaches to finding paths in trees. The first is more flexible, the second is more performant.},
	language = {en},
	urldate = {2023-03-15},
	author = {Wijgerden, Jeroen van},
	month = feb,
	year = {2022},
	note = {Section: post},
	keywords = {tail recursion},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\4BTFZQHH\\recursion-1.html:text/html},
}

@misc{christopoulos_polynomial_2013,
	title = {Polynomial regression using trapezoidal rule for computing {Legendre} coefficients},
	url = {http://arxiv.org/abs/1311.7525},
	abstract = {We are presenting a method for computing the Fourier coeﬃcients of a given polynomial regression by using the trapezoidal rule for numerical integration. As function basis we use the orthogonal Legendre polynomials. The results are accurate and stable compared to Forsythe’s method.},
	language = {en},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Christopoulos, Demetris T.},
	month = nov,
	year = {2013},
	note = {arXiv:1311.7525 [math, stat]},
	keywords = {Mathematics - Numerical Analysis, Statistics - Computation, chebychev, Primary 62J05, Secondary 65D99},
	annote = {Comment: 13 pages, 2 figures, 4 tables},
	file = {Christopoulos - 2013 - Polynomial regression using trapezoidal rule for c.pdf:C\:\\Users\\isido\\Zotero\\storage\\MNCK7RST\\Christopoulos - 2013 - Polynomial regression using trapezoidal rule for c.pdf:application/pdf},
}

@misc{noauthor_hardware_2022,
	title = {Hardware acceleration},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Hardware_acceleration&oldid=1098931508},
	abstract = {Hardware acceleration is the use of computer hardware designed to perform specific functions more efficiently when compared to software running on a general-purpose central processing unit (CPU). Any transformation of data that can be calculated in software running on a generic CPU can also be calculated in custom-made hardware, or in some mix of both.
To perform computing tasks more quickly (or better in some other way), generally one can invest time and money in improving the software, improving the hardware, or both. There are various approaches with advantages and disadvantages in terms of decreased latency, increased throughput and reduced energy consumption. Typical advantages of focusing on software may include more rapid development, lower non-recurring engineering costs, heightened portability, and ease of updating features or patching bugs, at the cost of overhead to compute general operations. Advantages of focusing on hardware may include speedup, reduced power consumption, lower latency, increased parallelism and bandwidth, and better utilization of area and functional components available on an integrated circuit; at the cost of lower ability to update designs once etched onto silicon and higher costs of functional verification, and times to market. In the hierarchy of digital computing systems ranging from general-purpose processors to fully customized hardware, there is a tradeoff between flexibility and efficiency, with efficiency increasing by orders of magnitude when any given application is implemented higher up that hierarchy. This hierarchy includes general-purpose processors such as CPUs, more specialized processors such as GPUs, fixed-function implemented on field-programmable gate arrays (FPGAs), and fixed-function implemented on application-specific integrated circuits (ASICs).Hardware acceleration is advantageous for performance, and practical when the functions are fixed so updates are not as needed as in software solutions. With the advent of reprogrammable logic devices such as FPGAs, the restriction of hardware acceleration to fully fixed algorithms has eased since 2010, allowing hardware acceleration to be applied to problem domains requiring modification to algorithms and processing control flow. The disadvantage however, is that in many open source projects, it requires proprietary libraries that not all vendors are keen to distribute or expose, making it difficult to integrate in such projects.},
	language = {en},
	urldate = {2023-03-01},
	journal = {Wikipedia},
	month = jul,
	year = {2022},
	note = {Page Version ID: 1098931508},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\DYFFG8BP\\Hardware_acceleration.html:text/html},
}

@misc{noauthor_parallelodespdf_nodate,
	title = {{parallelODEs}.pdf},
	url = {https://www.cs.usask.ca/~spiteri/M314/notes/parallelODEs.pdf},
	urldate = {2023-01-25},
	keywords = {ODE},
	file = {parallelODEs.pdf:C\:\\Users\\isido\\Zotero\\storage\\NW56ESQ7\\parallelODEs.pdf:application/pdf},
}

@article{ullrich_monte_2017,
	title = {A {Monte} {Carlo} method for integration of multivariate smooth functions},
	volume = {55},
	issn = {0036-1429, 1095-7170},
	url = {http://arxiv.org/abs/1604.06008},
	doi = {10.1137/16M1075557},
	abstract = {We study a Monte Carlo algorithm that is based on a speciﬁc (randomly shifted and dilated) lattice point set. The main result of this paper is that the mean squared error for a given compactly supported, square-integrable function is bounded by n−1/2 times the L2-norm of the Fourier transform outside a region around the origin, where n is the expected number of function evaluations. As corollaries we obtain the optimal order of convergence for functions from the Sobolev spaces Hps with isotropic, anisotropic or mixed smoothness with given compact support for all values of the parameters. If the region of integration is the unit cube, we obtain the same optimal orders for functions without boundary conditions. This proves, in particular, that the optimal order of convergence in the latter case is n−s−1/2 for p ≥ 2, which is, in contrast to the case of deterministic algorithms, independent of the dimension. This shows that Monte Carlo algorithms can improve the order by more than n−1/2 for a whole class of natural function spaces. Note that a similar result (for a diﬀerent class) was obtained by Heinrich et al. [13].},
	language = {en},
	number = {3},
	urldate = {2023-05-25},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Ullrich, Mario},
	month = jan,
	year = {2017},
	note = {arXiv:1604.06008 [math]},
	keywords = {monte carlo, Mathematics - Numerical Analysis, integration},
	pages = {1188--1200},
	annote = {Comment: The numbering of the theorems differs from the published version},
	file = {Ullrich - 2017 - A Monte Carlo method for integration of multivaria.pdf:C\:\\Users\\isido\\Zotero\\storage\\WBZMRYM7\\Ullrich - 2017 - A Monte Carlo method for integration of multivaria.pdf:application/pdf},
}

@article{daluiso_second_2018,
	title = {Second {Order} {Sensitivities} in {Linear} or {Constant} {Time}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3201559},
	doi = {10.2139/ssrn.3201559},
	abstract = {We analyse and compare methods to compute the full set of second order sensitivities of a Monte Carlo price in a time which is at most O(N · T ) where N is the number of inputs and T is the time required to compute the price. The new ones include the ﬁrst algorithm which achieves a complexity O(T ) and has acceptable (in fact very low) statistical uncertainties at least in one relevant test case.
Keywords: Greeks; Gamma; Vanna; Volga; Vomma; algorithmic differentiation; adjoints; pathwise differentiation; derivatives pricing; discontinuous payoffs; digital options.},
	language = {en},
	urldate = {2023-05-24},
	journal = {SSRN Electronic Journal},
	author = {Daluiso, Roberto},
	year = {2018},
	keywords = {finance},
	file = {Daluiso - 2018 - Second Order Sensitivities in Linear or Constant T.pdf:C\:\\Users\\isido\\Zotero\\storage\\T5E6FRLG\\Daluiso - 2018 - Second Order Sensitivities in Linear or Constant T.pdf:application/pdf},
}

@misc{elliott_simple_2018,
	title = {The simple essence of automatic differentiation},
	url = {http://arxiv.org/abs/1804.00746},
	abstract = {Automatic diﬀerentiation (AD) in reverse mode (RAD) is a central component of deep learning and other uses of large-scale optimization. Commonly used RAD algorithms such as backpropagation, however, are complex and stateful, hindering deep understanding, improvement, and parallel execution. This paper develops a simple, generalized AD algorithm calculated from a simple, natural speciﬁcation. The general algorithm is then specialized by varying the representation of derivatives. In particular, applying well-known constructions to a naive representation yields two RAD algorithms that are far simpler than previously known. In contrast to commonly used RAD implementations, the algorithms deﬁned here involve no graphs, tapes, variables, partial derivatives, or mutation. They are inherently parallel-friendly, correct by construction, and usable directly from an existing programming language with no need for new data types or programming style, thanks to use of an AD-agnostic compiler plugin.},
	language = {en},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Elliott, Conal},
	month = oct,
	year = {2018},
	note = {arXiv:1804.00746 [cs]},
	keywords = {optimization, Computer Science - Programming Languages},
	annote = {Comment: 37 pages with proof appendices and 15 figures. Extended version of a paper appearing at ICFP 2018. More info at http://conal.net/papers/essence-of-ad/},
	file = {1804.00746.pdf:C\:\\Users\\isido\\Zotero\\storage\\XCWM9PGQ\\1804.00746.pdf:application/pdf},
}

@misc{phd_how_2023,
	title = {How {I} {Turned} {My} {Company}’s {Docs} into a {Searchable} {Database} with {OpenAI}},
	url = {https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736},
	abstract = {For the past six months, I’ve been working at series A startup Voxel51, a and creator of the open source computer vision toolkit FiftyOne. As a machine learning engineer and developer evangelist, my job is to listen to our open source community and bring them what they need — new features, integrations, tutorials, workshops, you name it.

A few weeks ago, we added native support for vector search engines and text similarity queries to FiftyOne, so that users can find the most relevant images in their (often massive — containing millions or tens of millions of samples) datasets, via simple natural language queries.

This put us in a curious position: it was now possible for people using open source FiftyOne to readily search datasets with natural language queries, but using our documentation still required traditional keyword search.

We have a lot of documentation, which has its pros and cons. As a user myself, I sometimes find that given the sheer quantity of documentation, finding precisely what I’m looking for requires more time than I’d like.},
	language = {en},
	urldate = {2023-05-23},
	journal = {Medium},
	author = {Ph.D, Jacob Marks},
	month = apr,
	year = {2023},
	keywords = {machine learning, nlp},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\CPBYHIF6\\how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736.html:text/html},
}

@misc{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	language = {en},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	note = {arXiv:1411.1784 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, machine learning, neural networks, GANS},
	file = {Mirza en Osindero - 2014 - Conditional Generative Adversarial Nets.pdf:C\:\\Users\\isido\\Zotero\\storage\\C933QUW2\\Mirza en Osindero - 2014 - Conditional Generative Adversarial Nets.pdf:application/pdf},
}

@misc{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	language = {en},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = jan,
	year = {2016},
	note = {arXiv:1511.06434 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, machine learning, neural networks, GANS},
	annote = {Comment: Under review as a conference paper at ICLR 2016},
	file = {Radford e.a. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:C\:\\Users\\isido\\Zotero\\storage\\8NCTCL8G\\Radford e.a. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf},
}

@misc{yang_diffusion_2023,
	title = {Diffusion {Models}: {A} {Comprehensive} {Survey} of {Methods} and {Applications}},
	shorttitle = {Diffusion {Models}},
	url = {http://arxiv.org/abs/2209.00796},
	abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.},
	language = {en},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
	month = mar,
	year = {2023},
	note = {arXiv:2209.00796 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, machine learning, neural networks, diffusion model},
	annote = {Comment: 49 pages, 17 figures, citing 337 (up-to-date) papers, project: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy},
	file = {Yang e.a. - 2023 - Diffusion Models A Comprehensive Survey of Method.pdf:C\:\\Users\\isido\\Zotero\\storage\\8VW927IA\\Yang e.a. - 2023 - Diffusion Models A Comprehensive Survey of Method.pdf:application/pdf},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, machine learning, Computer Science - Computation and Language, neural networks},
	annote = {Comment: 15 pages, 5 figures},
	file = {1706.03762.pdf:C\:\\Users\\isido\\Zotero\\storage\\I5AYFTK6\\1706.03762.pdf:application/pdf},
}

@article{pu_solving_2023,
	title = {Solving {Schrodinger} equations using physically constrained neural network},
	volume = {47},
	issn = {1674-1137, 2058-6132},
	url = {http://arxiv.org/abs/2303.03934},
	doi = {10.1088/1674-1137/acc518},
	abstract = {Deep neural network (DNN) and auto differentiation have been widely used in computational physics to solve variational problems. When DNN is used to represent the wave function to solve quantum many-body problems using variational optimization, various physical constraints have to be injected into the neural network by construction, to increase the data and learning efficiency. We build the unitary constraint to the variational wave function using a monotonic neural network to represent the Cumulative Distribution Function (CDF) \$F(x) = {\textbackslash}int\_\{-{\textbackslash}infty\}{\textasciicircum}\{x\} {\textbackslash}psi{\textasciicircum}*{\textbackslash}psi dx'\$. Using this constrained neural network to represent the variational wave function, we solve Schrodinger equations using auto-differentiation and stochastic gradient descent (SGD), by minimizing the violation of the trial wave function \${\textbackslash}psi(x)\$ to the Schrodinger equation. For several classical problems in quantum mechanics, we obtain their ground state wave function and energy with very low errors. The method developed in the present paper may pave a new way in solving nuclear many body problems in the future.},
	language = {en},
	number = {5},
	urldate = {2023-05-22},
	journal = {Chinese Physics C},
	author = {Pu, Kai-Fang and Li, Hanlin and Lu, Hong-Liang and Pang, Long-Gang},
	month = may,
	year = {2023},
	note = {arXiv:2303.03934 [nucl-th]},
	keywords = {PDE, Nuclear Theory, neural networks},
	pages = {054104},
	file = {Pu e.a. - 2023 - Solving Schrodinger equations using physically con.pdf:C\:\\Users\\isido\\Zotero\\storage\\WD3BPTM7\\Pu e.a. - 2023 - Solving Schrodinger equations using physically con.pdf:application/pdf},
}

@misc{yang_fastsgd_2021,
	title = {{FastSGD}: {A} {Fast} {Compressed} {SGD} {Framework} for {Distributed} {Machine} {Learning}},
	shorttitle = {{FastSGD}},
	url = {http://arxiv.org/abs/2112.04291},
	abstract = {With the rapid increase of big data, distributed Machine Learning (ML) has been widely applied in training large-scale models. Stochastic Gradient Descent (SGD) is arguably the workhorse algorithm of ML. Distributed ML models trained by SGD involve large amounts of gradient communication, which limits the scalability of distributed ML. Thus, it is important to compress the gradients for reducing communication. In this paper, we propose FastSGD, a Fast compressed SGD framework for distributed ML. To achieve a high compression ratio at a low cost, FastSGD represents the gradients as key-value pairs, and compresses both the gradient keys and values in linear time complexity. For the gradient value compression, FastSGD ﬁrst uses a reciprocal mapper to transform original values into reciprocal values, and then, it utilizes a logarithm quantization to further reduce reciprocal values to small integers. Finally, FastSGD ﬁlters reduced gradient integers by a given threshold. For the gradient key compression, FastSGD provides an adaptive ﬁne-grained delta encoding method to store gradient keys with fewer bits. Extensive experiments on practical ML models and datasets demonstrate that FastSGD achieves the compression ratio up to 4 orders of magnitude, and accelerates the convergence time up to 8×, compared with state-of-the-art methods.},
	language = {en},
	urldate = {2023-05-22},
	publisher = {arXiv},
	author = {Yang, Keyu and Chen, Lu and Zeng, Zhihao and Gao, Yunjun},
	month = dec,
	year = {2021},
	note = {arXiv:2112.04291 [cs]},
	keywords = {Computer Science - Machine Learning, optimization, Computer Science - Distributed, Parallel, and Cluster Computing, gradient descent},
	file = {2112.04291.pdf:C\:\\Users\\isido\\Zotero\\storage\\S8Q4SDYG\\2112.04291.pdf:application/pdf},
}

@misc{ghaffari_is_2023,
	title = {Is {Integer} {Arithmetic} {Enough} for {Deep} {Learning} {Training}?},
	url = {http://arxiv.org/abs/2207.08822},
	abstract = {The ever-increasing computational complexity of deep learning models makes their training and deployment difﬁcult on various cloud and edge platforms. Replacing ﬂoating-point arithmetic with low-bit integer arithmetic is a promising approach to save energy, memory footprint, and latency of deep learning models. As such, quantization has attracted the attention of researchers in recent years. However, using integer numbers to form a fully functional integer training pipeline including forward pass, back-propagation, and stochastic gradient descent is not studied in detail. Our empirical and mathematical results reveal that integer arithmetic seems to be enough to train deep learning models. Unlike recent proposals, instead of quantization, we directly switch the number representation of computations. Our novel training method forms a fully integer training pipeline that does not change the trajectory of the loss and accuracy compared to ﬂoating-point, nor does it need any special hyper-parameter tuning, distribution adjustment, or gradient clipping. Our experimental results show that our proposed method is effective in a wide variety of tasks such as classiﬁcation (including vision transformers), object detection, and semantic segmentation.},
	language = {en},
	urldate = {2023-05-22},
	publisher = {arXiv},
	author = {Ghaffari, Alireza and Tahaei, Marzieh S. and Tayaranian, Mohammadreza and Asgharian, Masoud and Nia, Vahid Partovi},
	month = jan,
	year = {2023},
	note = {arXiv:2207.08822 [cs]},
	keywords = {Computer Science - Machine Learning, optimization, Computer Science - Computational Complexity, machine learning, gradient descent},
	annote = {Comment: final camera ready submitted to NeurIPS},
	file = {2207.08822.pdf:C\:\\Users\\isido\\Zotero\\storage\\3P85UYGA\\2207.08822.pdf:application/pdf},
}

@misc{yan_killing_2023,
	title = {Killing {Two} {Birds} with {One} {Stone}: {Quantization} {Achieves} {Privacy} in {Distributed} {Learning}},
	shorttitle = {Killing {Two} {Birds} with {One} {Stone}},
	url = {http://arxiv.org/abs/2304.13545},
	abstract = {Communication efﬁciency and privacy protection are two critical issues in distributed machine learning. Existing methods tackle these two issues separately and may have a high implementation complexity that constrains their application in a resource-limited environment. We propose a comprehensive quantization-based solution that could simultaneously achieve communication efﬁciency and privacy protection, providing new insights into the correlated nature of communication and privacy. Speciﬁcally, we demonstrate the effectiveness of our proposed solutions in the distributed stochastic gradient descent (SGD) framework by adding binomial noise to the uniformly quantized gradients to reach the desired differential privacy level but with a minor sacriﬁce in communication efﬁciency. We theoretically capture the new trade-offs between communication, privacy, and learning performance.},
	language = {en},
	urldate = {2023-05-22},
	publisher = {arXiv},
	author = {Yan, Guangfeng and Li, Tan and Wu, Kui and Song, Linqi},
	month = apr,
	year = {2023},
	note = {arXiv:2304.13545 [cs]},
	keywords = {Computer Science - Machine Learning, optimization, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, machine learning, gradient descent},
	file = {Yan e.a. - 2023 - Killing Two Birds with One Stone Quantization Ach.pdf:C\:\\Users\\isido\\Zotero\\storage\\AXSWITAR\\Yan e.a. - 2023 - Killing Two Birds with One Stone Quantization Ach.pdf:application/pdf},
}

@misc{goda_randomizing_2022-1,
	title = {Randomizing the trapezoidal rule gives the optimal {RMSE} rate in {Gaussian} {Sobolev} spaces},
	url = {http://arxiv.org/abs/2212.11476},
	abstract = {Randomized quadratures for integrating functions in Sobolev spaces of order α ≥ 1, where the integrability condition is with respect to the Gaussian measure, are considered. In this function space, the optimal rate for the worst-case root-mean-squared error (RMSE) is established. Here, optimality is for a general class of quadratures, in which adaptive non-linear algorithms with a possibly varying number of function evaluations are also allowed. The optimal rate is given by showing matching bounds. First, a lower bound on the worst-case RMSE of O(n−α−1/2) is proven, where n denotes an upper bound on the expected number of function evaluations. It turns out that a suitably randomized trapezoidal rule attains this rate, up to a logarithmic factor. A practical error estimator for this trapezoidal rule is also presented. Numerical results support our theory.},
	language = {en},
	urldate = {2023-05-22},
	publisher = {arXiv},
	author = {Goda, Takashi and Kazashi, Yoshihito and Suzuki, Yuya},
	month = dec,
	year = {2022},
	note = {arXiv:2212.11476 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, integration, information complexity},
	annote = {Comment: 21 pages},
	file = {Goda e.a. - 2022 - Randomizing the trapezoidal rule gives the optimal.pdf:C\:\\Users\\isido\\Zotero\\storage\\6Z96NCM6\\Goda e.a. - 2022 - Randomizing the trapezoidal rule gives the optimal.pdf:application/pdf},
}

@misc{lin_catalyst_2018,
	title = {Catalyst {Acceleration} for {First}-order {Convex} {Optimization}: from {Theory} to {Practice}},
	shorttitle = {Catalyst {Acceleration} for {First}-order {Convex} {Optimization}},
	url = {http://arxiv.org/abs/1712.05654},
	abstract = {We introduce a generic scheme for accelerating gradient-based optimization methods in the sense of Nesterov. The approach, called Catalyst, builds upon the inexact accelerated proximal point algorithm for minimizing a convex objective function, and consists of approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. One of the keys to achieve acceleration in theory and in practice is to solve these sub-problems with appropriate accuracy by using the right stopping criterion and the right warm-start strategy. We give practical guidelines to use Catalyst and present a comprehensive analysis of its global complexity. We show that Catalyst applies to a large class of algorithms, including gradient descent, block coordinate descent, incremental algorithms such as SAG, SAGA, SDCA, SVRG, MISO/Finito, and their proximal variants. For all of these methods, we establish faster rates using the Catalyst acceleration, for strongly convex and non-strongly convex objectives. We conclude with extensive experiments showing that acceleration is useful in practice, especially for ill-conditioned problems.},
	language = {en},
	urldate = {2023-05-22},
	publisher = {arXiv},
	author = {Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
	month = jun,
	year = {2018},
	note = {arXiv:1712.05654 [math, stat]},
	keywords = {Statistics - Machine Learning, Mathematics - Optimization and Control, optimization, machine learning},
	annote = {Comment: link to publisher website: http://jmlr.org/papers/volume18/17-748/17-748.pdf},
	file = {1712.05654.pdf:C\:\\Users\\isido\\Zotero\\storage\\PPKCAMMC\\1712.05654.pdf:application/pdf},
}

@article{stampfer_methods_2002,
	title = {{METHODS} {FOR} {ESTIMATING} {PRINCIPAL} {POINTS}},
	volume = {31},
	issn = {0361-0918, 1532-4141},
	url = {http://www.tandfonline.com/doi/abs/10.1081/SAC-120003338},
	doi = {10.1081/SAC-120003338},
	abstract = {Principal points of a distribution have been introduced by Flury (1) who tackled the problem of optimal grouping in multivariate data. In essence, principal points are the theoretical counterparts of cluster means obtained by a \$k\$-means clustering algorithm. There has been considerable effort to find efficient estimation procedures for principal points. It is well known that under certain conditions the \$k\$-means estimator is a consistent and asymptotically normal estimator of the population principal points. In this paper some material on principal points is reviewed and new algorithms for the estimation of principal points in univariate distributions (univariate principal points) are proposed. Additionally, the Bootstrap approach is applied to assess the variability of the suggested estimators.},
	language = {en},
	number = {2},
	urldate = {2023-05-17},
	journal = {Communications in Statistics - Simulation and Computation},
	author = {Stampfer, Erwin and Stadlober, Ernst},
	month = may,
	year = {2002},
	keywords = {machine learning},
	pages = {261--277},
	file = {Stampfer en Stadlober - 2002 - METHODS FOR ESTIMATING PRINCIPAL POINTS.pdf:C\:\\Users\\isido\\Zotero\\storage\\7XXCGFY2\\Stampfer en Stadlober - 2002 - METHODS FOR ESTIMATING PRINCIPAL POINTS.pdf:application/pdf},
}

@misc{jankowiak_pathwise_2018,
	title = {Pathwise {Derivatives} {Beyond} the {Reparameterization} {Trick}},
	url = {http://arxiv.org/abs/1806.01851},
	abstract = {We observe that gradients computed via the reparameterization trick are in direct correspondence with solutions of the transport equation in the formalism of optimal transport. We use this perspective to compute (approximate) pathwise gradients for probability distributions not directly amenable to the reparameterization trick: Gamma, Beta, and Dirichlet. We further observe that when the reparameterization trick is applied to the Choleskyfactorized multivariate Normal distribution, the resulting gradients are suboptimal in the sense of optimal transport. We derive the optimal gradients and show that they have reduced variance in a Gaussian Process regression task. We demonstrate with a variety of synthetic experiments and stochastic variational inference tasks that our pathwise gradients are competitive with other methods.},
	language = {en},
	urldate = {2023-05-11},
	publisher = {arXiv},
	author = {Jankowiak, Martin and Obermeyer, Fritz},
	month = jul,
	year = {2018},
	note = {arXiv:1806.01851 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, machine learning},
	annote = {Comment: ICML 2018},
	file = {Jankowiak en Obermeyer - 2018 - Pathwise Derivatives Beyond the Reparameterization.pdf:C\:\\Users\\isido\\Zotero\\storage\\E9PELC7L\\Jankowiak en Obermeyer - 2018 - Pathwise Derivatives Beyond the Reparameterization.pdf:application/pdf},
}

@article{xie_frozen_nodate,
	title = {Frozen {Gaussian} {Sampling}: {A} {Mesh}-free {Monte} {Carlo} {Method} {For} {Approximating} {Semiclassical} {Schr}¨odinger {Equations}},
	abstract = {In this paper, we develop a Monte Carlo algorithm named the Frozen Gaussian Sampling (FGS) to solve the semiclassical Schr¨odinger equation based on the frozen Gaussian approximation. Due to the highly oscillatory structure of the wave function, traditional mesh-based algorithms suﬀer from ”the curse of dimensionality”, which gives rise to more severe computational burden when the semiclassical parameter ε is small. The Frozen Gaussian sampling outperforms the existing algorithms in that it is mesh-free in computing the physical observables and is suitable for high dimensional problems. In this work, we provide detailed procedures to implement the FGS for both Gaussian and WKB initial data cases, where the sampling strategies on the phase space balance the need of variance reduction and sampling convenience. Moreover, we rigorously prove that, to reach a certain accuracy, the number of samples needed for the FGS is independent of the scaling parameter ε. Furthermore, the complexity of the FGS algorithm is of a sublinear scaling with respect to the microscopic degrees of freedom and, in particular, is insensitive to the dimension number. The performance of the FGS is validated through several typical numerical experiments, including simulating scattering by the barrier potential, formation of the caustics and computing the high-dimensional physical observables without mesh.},
	language = {en},
	author = {Xie, Yantong and Zhou, Zhennan},
	keywords = {monte carlo},
	file = {Xie en Zhou - Frozen Gaussian Sampling A Mesh-free Monte Carlo .pdf:C\:\\Users\\isido\\Zotero\\storage\\DR2NE79L\\Xie en Zhou - Frozen Gaussian Sampling A Mesh-free Monte Carlo .pdf:application/pdf},
}

@book{oksendal_stochastic_2003,
	address = {Berlin, Heidelberg},
	series = {Universitext},
	title = {Stochastic {Differential} {Equations}},
	isbn = {978-3-540-04758-2 978-3-642-14394-6},
	url = {http://link.springer.com/10.1007/978-3-642-14394-6},
	abstract = {These notes are based on a postgraduate course I gave on stochastic differential equations at Edinburgh University in the spring 1982. No previous knowledge about the subject was assumed, but the presentation is based on some background in measure theory. There are several reasons why one should learn more about stochastic differential equations: They have a wide range of applications outside mathematics, there are many fruitful connections to other mathematical disciplines and the subject has a rapidly developing life of its own as a fascinating research field with many interesting unanswered questions. Unfortunately most of the literature about stochastic differential equations seems to place so much emphasis on rigor and completenessthatitscares many nonexperts away. These notes are an attempt to approach the subject from the nonexpert point of view: Not knowing anything (except rumours, maybe) about a subject to start with, what would I like to know first of all? My answer would be: 1) In what situations does the subject arise? 2) What are its essential features? 3) What are the applications and the connections to other fields? I would not be so interested in the proof of the most general case, but rather in an easier proof of a special case, which may give just as much of the basic idea in the argument. And I would be willing to believe some basic results without proof (at first stage, anyway) in order to have time for some more basic applications. These notes reflect this point of view. Such an approach enables us to reach the highlights of the theory quicker and easier. Thus it is hoped that these notes may contribute to fill a gap in the existing literature. The course is meant to be an appetizer. If it succeeds in awaking further interest, the reader will have a large selection of excellent literature available for the study of the whole story. Some of this literature is listed at the back. xxvi

xxviii In the introduction we state 6 problems where stochastic differential equations play an essential role in the solution. In Chapter II we introduce the basic mathematical notions needed for the mathematical model of some of these problems, leading to the concept of Ito integrals in Chapter III. In Chapter IV we develop the stochastic calculus (the Ito formula) and in Chapter V we use this to solve some stochastic differential equations, including the first two problems in the introduction. In Chapter VI we present a solution of the linear filtering problem (of which problem 3 is an example), using the stochastic calculus. Problem 4 is the Dirichlet problem. Although this is purely deterministic we outline in Chapters VII and VIII how the introduction of an associated Ito diffusion (i.e. solution of a stochastic differential equation) leads to a simple, intuitive and useful stochastic solution, which is the cornerstone of stochastic potential theory. Problem 5 is an optimal stopping problem. In Chapter IX we represent the state of a game at time t by an Ito diffusion and solve the corresponding optimal stopping problem. The solution involves potential theoretic notions, such as the generalized harmonic extension provided by the solution of the Dirichlet problem in Chapter VIII. Problem 6 is a stochastic version of F.P. Ramsey’s classical control problem from 1928. In Chapter X we formulate the general stochastic control problem in terms of stochastic differential equations, and we apply the results of Chapters VII and VIII to show that the problem can be reduced to solving the (deterministic) Hamilton-Jacobi-Bellman equation. As an illustration we solve a problem about optimal portfolio selection. After the course was first given in Edinburgh in 1982, revised and expanded versions were presented at Agder College, Kristiansand and University of Oslo. Every time about half of the audience have come from the applied section, the others being so-called “pure” mathematicians. This fruitful combination has created a broad variety of valuable comments, for which I am very grateful. I particularly wish to express my gratitude to K.K. Aase, L. Csink and A.M. Davie for many useful discussions. I wish to thank the Science and Engineering Research Council, U.K. and Norges Almenvitenskapelige Forskningsra  ̊d (NAVF), Norway for their financial support. And I am greatly indebted to Ingrid Skram, Agder College and Inger Prestbakken, University of Oslo for their excellent typing – and their patience with the innumerable changes in the manuscript during these two years.},
	language = {en},
	urldate = {2023-05-08},
	publisher = {Springer Berlin Heidelberg},
	author = {Øksendal, Bernt},
	year = {2003},
	doi = {10.1007/978-3-642-14394-6},
	file = {Øksendal - 2003 - Stochastic Differential Equations.pdf:C\:\\Users\\isido\\Zotero\\storage\\GCS3EXST\\Øksendal - 2003 - Stochastic Differential Equations.pdf:application/pdf},
}

@misc{lovbak_reversible_2023,
	title = {Reversible random number generation for adjoint {Monte} {Carlo} simulation of the heat equation},
	url = {http://arxiv.org/abs/2302.02778},
	abstract = {In PDE-constrained optimization, one aims to ﬁnd design parameters that minimize some objective, subject to the satisfaction of a partial diﬀerential equation. A major challenges is computing gradients of the objective to the design parameters, as applying the chain rule requires computing the Jacobian of the design parameters to the PDE’s state. The adjoint method avoids this Jacobian by computing partial derivatives of a Lagrangian. Evaluating these derivatives requires the solution of a second PDE with the adjoint diﬀerential operator to the constraint, resulting in a backwards-in-time simulation.},
	language = {en},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Løvbak, Emil and Blondeel, Frédéric and Lee, Adam and Vanroye, Lander and Van Barel, Andreas and Samaey, Giovanni},
	month = feb,
	year = {2023},
	note = {arXiv:2302.02778 [cs, math]},
	keywords = {monte carlo, PDE, Mathematics - Numerical Analysis},
	annote = {Comment: 17 pages, 5 figures, submitted to the proceedings of MCQMC22},
	file = {Løvbak e.a. - 2023 - Reversible random number generation for adjoint Mo.pdf:C\:\\Users\\isido\\Zotero\\storage\\I2IIHESY\\Løvbak e.a. - 2023 - Reversible random number generation for adjoint Mo.pdf:application/pdf},
}

@misc{fauskanger_what_2017,
	type = {Forum post},
	title = {What is the advantages of {Wasserstein} metric compared to {Kullback}-{Leibler} divergence?},
	url = {https://stats.stackexchange.com/q/295617},
	urldate = {2023-05-07},
	journal = {Cross Validated},
	author = {Fauskanger, Thomas},
	month = aug,
	year = {2017},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\NW2RJDMV\\what-is-the-advantages-of-wasserstein-metric-compared-to-kullback-leibler-diverg.html:text/html},
}

@article{johnson_accelerating_nodate,
	title = {Accelerating {Stochastic} {Gradient} {Descent} using {Predictive} {Variance} {Reduction}},
	abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is signiﬁcantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
	language = {en},
	author = {Johnson, Rie and Zhang, Tong},
	keywords = {optimization, gradient descent},
	file = {Johnson en Zhang - Accelerating Stochastic Gradient Descent using Pre.pdf:C\:\\Users\\isido\\Zotero\\storage\\NTPY2NIB\\Johnson en Zhang - Accelerating Stochastic Gradient Descent using Pre.pdf:application/pdf},
}

@article{jentzen_random_2009,
	title = {A random {Euler} scheme for {Carathéodory} differential equations},
	volume = {224},
	issn = {03770427},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377042708002136},
	doi = {10.1016/j.cam.2008.05.060},
	abstract = {We study a random Euler scheme for the approximation of Carathéodory differential equations and give a precise error analysis. In particular, we show that under weak assumptions, this approximation scheme obtains the same rate of convergence as the classical Monte–Carlo method for integration problems.},
	language = {en},
	number = {1},
	urldate = {2023-03-06},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Jentzen, A. and Neuenkirch, A.},
	month = feb,
	year = {2009},
	keywords = {monte carlo, ODE},
	pages = {346--359},
	file = {Jentzen en Neuenkirch - 2009 - A random Euler scheme for Carathéodory differentia.pdf:C\:\\Users\\isido\\Zotero\\storage\\LFZQS9Q7\\Jentzen en Neuenkirch - 2009 - A random Euler scheme for Carathéodory differentia.pdf:application/pdf},
}

@misc{miller_boundary_2023,
	title = {Boundary {Value} {Caching} for {Walk} on {Spheres}},
	url = {http://arxiv.org/abs/2302.11825},
	abstract = {Grid-free Monte Carlo methods such as {\textbackslash}emph\{walk on spheres\} can be used to solve elliptic partial differential equations without mesh generation or global solves. However, such methods independently estimate the solution at every point, and hence do not take advantage of the high spatial regularity of solutions to elliptic problems. We propose a fast caching strategy which first estimates solution values and derivatives at randomly sampled points along the boundary of the domain (or a local region of interest). These cached values then provide cheap, output-sensitive evaluation of the solution (or its gradient) at interior points, via a boundary integral formulation. Unlike classic boundary integral methods, our caching scheme introduces zero statistical bias and does not require a dense global solve. Moreover we can handle imperfect geometry (e.g., with self-intersections) and detailed boundary/source terms without repairing or resampling the boundary representation. Overall, our scheme is similar in spirit to {\textbackslash}emph\{virtual point light\} methods from photorealistic rendering: it suppresses the typical salt-and-pepper noise characteristic of independent Monte Carlo estimates, while still retaining the many advantages of Monte Carlo solvers: progressive evaluation, trivial parallelization, geometric robustness, {\textbackslash}etc\{\}{\textbackslash} We validate our approach using test problems from visual and geometric computing.},
	language = {en},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Miller, Bailey and Sawhney, Rohan and Crane, Keenan and Gkioulekas, Ioannis},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11825 [cs]},
	keywords = {PDE, walk on spheres, Computer Science - Graphics, boundary value problems},
	annote = {Comment: Added citation to concurrent submission},
	file = {Miller e.a. - 2023 - Boundary Value Caching for Walk on Spheres.pdf:C\:\\Users\\isido\\Zotero\\storage\\LX3BMITM\\Miller e.a. - 2023 - Boundary Value Caching for Walk on Spheres.pdf:application/pdf},
}

@misc{sawhney_walk_2023,
	title = {Walk on {Stars}: {A} {Grid}-{Free} {Monte} {Carlo} {Method} for {PDEs} with {Neumann} {Boundary} {Conditions}},
	shorttitle = {Walk on {Stars}},
	url = {http://arxiv.org/abs/2302.11815},
	abstract = {Grid-free Monte Carlo methods based on the {\textbackslash}emph\{walk on spheres (WoS)\} algorithm solve fundamental partial differential equations (PDEs) like the Poisson equation without discretizing the problem domain, nor approximating functions in a finite basis. Such methods hence avoid aliasing in the solution, and evade the many challenges of mesh generation. Yet for problems with complex geometry, practical grid-free methods have been largely limited to basic Dirichlet boundary conditions. This paper introduces the {\textbackslash}emph\{walk on stars (WoSt)\} method, which solves linear elliptic PDEs with arbitrary mixed Neumann and Dirichlet boundary conditions. The key insight is that one can efficiently simulate reflecting Brownian motion (which models Neumann conditions) by replacing the balls used by WoS with {\textbackslash}emph\{star-shaped\} domains; we identify such domains by locating the closest visible point on the geometric silhouette. Overall, WoSt retains many attractive features of other grid-free Monte Carlo methods, such as progressive evaluation, trivial parallel implementation, and logarithmic scaling relative to geometric complexity.},
	language = {en},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Sawhney, Rohan and Miller, Bailey and Gkioulekas, Ioannis and Crane, Keenan},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11815 [cs]},
	keywords = {PDE, walk on spheres, Computer Science - Graphics},
	annote = {Comment: Added citation to concurrent submission},
	file = {2302.11815.pdf:C\:\\Users\\isido\\Zotero\\storage\\36DKDXFD\\2302.11815.pdf:application/pdf},
}

@article{el_filali_ech-chafiq_automatic_2021,
	title = {Automatic control variates for option pricing using neural networks},
	volume = {27},
	issn = {1569-3961, 0929-9629},
	url = {https://www.degruyter.com/document/doi/10.1515/mcma-2020-2081/html},
	doi = {10.1515/mcma-2020-2081},
	abstract = {Many pricing problems boil down to the computation of a high dimensional integral, which is usually estimated using Monte Carlo. In fact, the accuracy of a Monte Carlo estimator with M simulations is given by √σ . Meaning that its convergence is immune to the dimension of the problem. However, M this convergence can be relatively slow depending on the variance σ of the function to be integrated. To resolve such a problem, one would perform some variance reduction techniques such as importance sampling, stratiﬁcation, or control variates. In this paper, we will study two approaches for improving the convergence of Monte Carlo using Neural Networks. The ﬁrst approach relies on the fact that many high dimensional ﬁnancial problems are of low effective dimensions[15]. We expose a method to reduce the dimension of such problems in order to keep only the necessary variables. The integration can then be done using fast numerical integration techniques such as Gaussian quadrature. The second approach consists in building an automatic control variate using neural networks. We learn the function to be integrated (which incorporates the diffusion model plus the payoff function) in order to build a network that is highly correlated to it. As the network that we use can be integrated exactly, we can use it as a control variate.},
	language = {en},
	number = {2},
	urldate = {2023-04-10},
	journal = {Monte Carlo Methods and Applications},
	author = {El Filali Ech-Chafiq, Zineb and Lelong, Jérôme and Reghai, Adil},
	month = jun,
	year = {2021},
	keywords = {control variates, neural networks},
	pages = {91--104},
	file = {El Filali Ech-Chafiq e.a. - 2021 - Automatic control variates for option pricing usin.pdf:C\:\\Users\\isido\\Zotero\\storage\\XVFWLNCP\\El Filali Ech-Chafiq e.a. - 2021 - Automatic control variates for option pricing usin.pdf:application/pdf},
}

@misc{geeraert_mini-symposium_2017,
	title = {Mini-symposium on automatic differentiation and its applications in the financial industry},
	url = {http://arxiv.org/abs/1703.02311},
	abstract = {Automatic diﬀerentiation has been involved for long in applied mathematics as an alternative to ﬁnite diﬀerence to improve the accuracy of numerical computation of derivatives. Each time a numerical minimization is involved, automatic diﬀerentiation can be used. In between formal derivation and standard numerical schemes, this approach is based on software solutions applying mechanically the chain rule formula to obtain an exact value for the desired derivative. It has a cost in memory and cpu consumption.},
	language = {en},
	urldate = {2023-04-10},
	publisher = {arXiv},
	author = {Geeraert, Sébastien and Lehalle, Charles-Albert and Pearlmutter, Barak and Pironneau, Olivier and Reghai, Adil},
	month = jun,
	year = {2017},
	note = {arXiv:1703.02311 [cs, q-fin]},
	keywords = {Quantitative Finance - Computational Finance, Mathematics - Numerical Analysis, Computer Science - Computational Engineering, Finance, and Science, MC forex course},
	file = {Geeraert e.a. - 2017 - Mini-symposium on automatic differentiation and it.pdf:C\:\\Users\\isido\\Zotero\\storage\\2CWLCKI6\\Geeraert e.a. - 2017 - Mini-symposium on automatic differentiation and it.pdf:application/pdf},
}

@misc{guo_computational_2019,
	title = {Computational {Methods} for {Martingale} {Optimal} {Transport} problems},
	url = {http://arxiv.org/abs/1710.07911},
	abstract = {We establish numerical methods for solving the martingale optimal transport problem (MOT) - a version of the classical optimal transport with an additional martingale constraint on transport's dynamics. We prove that the MOT value can be approximated using linear programming (LP) problems which result from a discretisation of the marginal distributions combined with a suitable relaxation of the martingale constraint. Specialising to dimension one, we provide bounds on the convergence rate of the above scheme. We also show a stability result under only partial specification of the marginal distributions. Finally, we specialise to a particular discretisation scheme which preserves the convex ordering and does not require the martingale relaxation. We introduce an entropic regularisation for the corresponding LP problem and detail the corresponding iterative Bregman projection. We also rewrite its dual problem as a minimisation problem without constraint and solve it by computing the concave envelope of scattered data.},
	language = {en},
	urldate = {2023-04-10},
	publisher = {arXiv},
	author = {Guo, Gaoyue and Obloj, Jan},
	month = apr,
	year = {2019},
	note = {arXiv:1710.07911 [math, q-fin]},
	keywords = {Mathematics - Probability, Quantitative Finance - Computational Finance, Mathematics - Optimization and Control, MC forex course},
	file = {Guo en Obloj - 2019 - Computational Methods for Martingale Optimal Trans.pdf:C\:\\Users\\isido\\Zotero\\storage\\WT6MVWSJ\\Guo en Obloj - 2019 - Computational Methods for Martingale Optimal Trans.pdf:application/pdf},
}

@article{belkotain_iterative_nodate,
	title = {Iterative {Bregman} {Projection} for {Monte} {Carlo} {Enhancement}},
	language = {en},
	author = {Belkotain, M and Reghai, A},
	keywords = {MC forex course},
	file = {Belkotain en Reghai - Iterative Bregman Projection for Monte Carlo Enhan.pdf:C\:\\Users\\isido\\Zotero\\storage\\MMDYLN8T\\Belkotain en Reghai - Iterative Bregman Projection for Monte Carlo Enhan.pdf:application/pdf},
}

@article{belkotain_iterative_nodate-1,
	title = {Iterative {Bregman} {Projection} for {Monte} {Carlo} {Enhancement}},
	abstract = {Given a set of \$N\$ trajectories, \$S\_\{t\_1\}, {\textbackslash}ldots S\_\{t\_n\}\$,
\$\$
{\textbackslash}left[{\textbackslash}begin\{array\}\{lll\}
S\_\{T\_1\}{\textbackslash}left({\textbackslash}omega\_1{\textbackslash}right) \& {\textbackslash}ldots \& S\_\{T\_n\}{\textbackslash}left({\textbackslash}omega\_1{\textbackslash}right) {\textbackslash}{\textbackslash}
S\_\{T\_1\}{\textbackslash}left({\textbackslash}omega\_2{\textbackslash}right) \& {\textbackslash}ldots \& S\_\{T\_n\}{\textbackslash}left({\textbackslash}omega\_2{\textbackslash}right) {\textbackslash}{\textbackslash}
{\textbackslash}cdot \& {\textbackslash}ldots \& {\textbackslash}{\textbackslash}
S\_\{T\_1\}{\textbackslash}left({\textbackslash}omega\_N{\textbackslash}right) \& {\textbackslash}ldots \& S\_\{T\_n\}{\textbackslash}left({\textbackslash}omega\_N{\textbackslash}right)
{\textbackslash}end\{array\}{\textbackslash}right]
\$\$
sampled at \$t\_1, {\textbackslash}ldots, t\_n\$, the empirical density is, \${\textbackslash}mu={\textbackslash}frac\{1\}\{N\} {\textbackslash}sum\_\{i=1\}{\textasciicircum}N {\textbackslash}delta\_\{S\_\{T\_1\}, {\textbackslash}ldots, S\_\{T\_n\}\}\$. This distribution is used to price and hedge derivative products. This set of scenariosaccumulates many types of discretisation errors. These can be linked to data quality input, to calibration algorithms, discretisation in time, interpolation in space or even dependent on the sample’s size N. All these errors contribute in errors that are detrimental to the quality of the final result. The classical approach to mitigate these errors is to work separately on each step of the sequence before the generation of the trajectories. Most of the time this increases significantly the computation time and notably the size sample in order to improve convergence and the final result. In the context of dynamic markets, where real time management is essential for trading desks, Also, there are important needs for financial engineers to perform back tests and forward tests to design the appropriate financial products. For the risk department and with regard to important set of risk metrics such those requested by the FRTB (Fundamental review of the trading Desk) it becomes essential to obtain good convergence with a minimum computation budget, in this case very little set of trajectories. It is also essential to dispose of a methodology that is agnostic to the designed model which can be different from one bank to the other and even sometimes from one department to another. The contribution of this paper is to introduce a feedback loop on the Monte Carlo paths. We do not revise all steps of the pricing algorithms, we rather concentrate on the resulting set of trajectories and smoothly transport them into a new version of trajectories which is more appropriate to fit the target.The proposed method acts directly on the generated sample and proposes a way to modify them in a smooth way in order to fit the market tradeables and importantly meet the non arbitrage condition i.e.  the preservation of the martingale property. It is a re-sampling approach which ensures that the finite set of scenarios fits perfectly the pricing constraints. We achieve this through an application of the iterative Bregman Projection algorithm.},
	language = {en},
	author = {Belkotain, Mehdi and Reghai, Adil},
	keywords = {MC forex course},
	file = {Belkotain en Reghai - Iterative Bregman Projection for Monte Carlo Enhan.pdf:C\:\\Users\\isido\\Zotero\\storage\\WF3R3AZN\\Belkotain en Reghai - Iterative Bregman Projection for Monte Carlo Enhan.pdf:application/pdf},
}

@article{south_russian_state_polytechnic_university_novocherkassk_polytechnic_institute_calculation_2021,
	title = {The calculation of the solution of multidimensional integral equations with methods {Monte} {Carlo} and quasi-{Monte} {Carlo}},
	volume = {15},
	issn = {20728735, 20728743},
	url = {http://media-publisher.ru/wp-content/uploads/8-10-2021.pdf},
	doi = {10.36724/2072-8735-2021-15-10-55-63},
	abstract = {The article considers an approach based on the random cubature method for solving both single and multidimensional singular integral equations, Volterra and Fredholm equations of the 1st kind, for ill-posed problems in the theory of integral equations, etc. A variant of the quasi-Monte Carlo method is studied. The integral in an integral equation is approximated using the traditional Monte Carlo method for calculating integrals. Multidimensional interpolation is applied on an arbitrary set of points. Examples of applying the method to a one-dimensional integral equation with a smooth kernel using both random and low-dispersed pseudo-random nodes are considered. A multidimensional linear integral equation with a polynomial kernel and a multidimensional nonlinear problem – the Hammerstein integral equation – are solved using the Newton method. The existence of several solutions is shown. Multidimensional integral equations of the first kind and their solution using regularization are considered. The Monte Carlo and quasi-Monte Carlo methods have not been used to solve such problems in the studied literature. The Lavrentiev regularization method was used, as well as random and pseudo-random nodes obtained using the Halton sequence. The problem of eigenvalues is solved. It is established that one of the best methods considered is the Leverrier-Faddeev method. The results of solving the problem for a different number of quadrature nodes are presented in the table. An approach based on parametric regularization of the core, an interpolation-projection method, and averaged adaptive densities are studied. The considered methods can be successfully applied in solving spatial boundary value problems for areas of complex shape. These approaches allow us to expand the range of problems in the theory of integral equations solved by Monte Carlo and quasi-Monte Carlo methods, since there are no restrictions on the value of the norm of the integral operator. A series of examples demonstrating the effectiveness of the method under study is considered.},
	language = {en},
	number = {10},
	urldate = {2023-03-26},
	journal = {T-Comm},
	author = {{South Russian State Polytechnic University (Novocherkassk Polytechnic Institute)} and Abas, Abas Wisam Mahdi},
	year = {2021},
	keywords = {monte carlo, integral equations, integration},
	pages = {55--63},
	file = {South Russian State Polytechnic University (Novocherkassk Polytechnic Institute) en Abas - 2021 - The calculation of the solution of multidimensiona.pdf:C\:\\Users\\isido\\Zotero\\storage\\AI7SS2YK\\South Russian State Polytechnic University (Novocherkassk Polytechnic Institute) en Abas - 2021 - The calculation of the solution of multidimensiona.pdf:application/pdf},
}

@article{zhimin_monte_2012,
	title = {Monte {Carlo} {Method} for {Solving} the {Fredholm} {Integral} {Equations} of the {Second} {Kind}},
	volume = {41},
	issn = {0041-1450, 1532-2424},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00411450.2012.695317},
	doi = {10.1080/00411450.2012.695317},
	abstract = {This article is concerned with a numerical algorithm for solving approximate solutions of Fredholm integral equations of the second kind with random sampling. We use Simpson’s rule for solving integral equations, which yields a linear system. The Monte Carlo method, based on the simulation of a finite discrete Markov chain, is employed to solve this linear system. To show the efficiency of the method, we use numerical examples. Results obtained by the present method indicate that the method is an effective alternate method. Keywords Monte Carlo algorithms; Markov chain; Simpson’s formula; Fredholm integral equation},
	language = {en},
	number = {7},
	urldate = {2023-03-26},
	journal = {Transport Theory and Statistical Physics},
	author = {ZhiMin, Hong and ZaiZai, Yan and JianRui, Chen},
	month = dec,
	year = {2012},
	keywords = {monte carlo, integral equations},
	pages = {513--528},
	file = {ZhiMin e.a. - 2012 - Monte Carlo Method for Solving the Fredholm Integr.pdf:C\:\\Users\\isido\\Zotero\\storage\\HJ9CJZP4\\ZhiMin e.a. - 2012 - Monte Carlo Method for Solving the Fredholm Integr.pdf:application/pdf},
}

@book{hackbusch_integral_2012,
	address = {Basel},
	edition = {1. softcover ed.]; [Reprint of the orig. 1st ed. 1995},
	series = {International series of numerical mathematics},
	title = {Integral equations: theory and numerical treatment},
	isbn = {978-3-0348-9947-5},
	shorttitle = {Integral equations},
	abstract = {Book on Integral equations: theory and numerical treatment},
	language = {eng},
	number = {120},
	publisher = {Birkhäuser},
	author = {Hackbusch, Wolfgang},
	year = {2012},
	keywords = {integral equations},
	file = {[International Series of Numerical Mathematics №120] Wolfgang Hackbusch (auth.) - Integral Equations_ Theory and Numerical Treatment (1995, Birkhäuser) [10.1007_978-3-0348-9215-5] - libgen.li.pdf:C\:\\Users\\isido\\Zotero\\storage\\UBYLDU35\\[International Series of Numerical Mathematics №120] Wolfgang Hackbusch (auth.) - Integral Equations_ Theory and Numerical Treatment (1995, Birkhäuser) [10.1007_978-3-0348-9215-5] - libgen.li.pdf:application/pdf},
}

@book{polianin_handbook_2008,
	address = {Boca Raton},
	edition = {2nd ed},
	series = {Handbooks of mathematical equations},
	title = {Handbook of integral equations},
	isbn = {978-1-58488-507-8},
	abstract = {Integral equations are encountered in various fields of science and numerous applications (in elasticity, plasticity, heat and mass transfer, oscillation theory, fluid dynamics, filtration theory, electrostatics, electrodynamics, biomechanics, game theory, control, queuing theory, electrical engineering, economics, medicine, etc.). Exact (closed-form) solutions of integral equations play an important role in the proper understanding of qualitative features of many phenomena and processes in various areas of natural science. Lots of equations of physics, chemistry, and biology contain functions or parameters which are obtained from experiments and hence are not strictly fixed. Therefore, it is expedient to choose the structure of these functions so that it would be easier to analyze and solve the equation. As a possible selection criterion, one may adopt the requirement that the model integral equation admits a solution in a closed form. Exact solutions can be used to verify the consistency and estimate errors of various numerical, asymptotic, and approximate methods. More than 2,100 integral equations and their solutions are given in the first part of the book (Chapters 1–6). A lot of new exact solutions to linear and nonlinear equations are included. Special attention is paid to equations of general form, which depend on arbitrary functions. The other equations contain one or more free parameters (the book actually deals with families of integral xxx

xxxii PREFACE equations); it is the reader’s option to fix these parameters. In total, the number of equations described in this handbook is an order of magnitude greater than in any other book currently available. The second part of the book (Chapters 7–14) presents exact, approximate analytical, and numerical methods for solving linear and nonlinear integral equations. Apart from the classical methods, some new methods are also described. When selecting the material, the authors have given a pronounced preference to practical aspects of the matter; that is, to methods that allow effectively “constructing” the solution. For the reader’s better understanding of the methods, each section is supplied with examples of specific equations. Some sections may be used by lecturers of colleges and universities as a basis for courses on integral equations and mathematical physics equations for graduate and postgraduate students. For the convenience of a wide audience with different mathematical backgrounds, the authors tried to do their best, wherever possible, to avoid special terminology. Therefore, some of the methods are outlined in a schematic and somewhat simplified manner, with necessary references made to books where these methods are considered in more detail. For some nonlinear equations, only solutions of the simplest form are given. The book does not cover two-, three-, and multidimensional integral equations. The handbook consists of chapters, sections, and subsections. Equations and formulas are numbered separately in each section. The equations within a section are arranged in increasing order of complexity. The extensive table of contents provides rapid access to the desired equations. For the reader’s convenience, the main material is followed by a number of supplements, where some properties of elementary and special functions are described, tables of indefinite and definite integrals are given, as well as tables of Laplace, Mellin, and other transforms, which are used in the book. The first and second parts of the book, just as many sections, were written so that they could be read independently from each other. This allows the reader to quickly get to the heart of the matter. We would like to express our deep gratitude to Rolf Sulanke and Alexei Zhurov for fruitful discussions and valuable remarks. We also appreciate the help of Vladimir Nazaikinskii and Alexander Shtern in translating the second part of this book, and are thankful to Inna Shingareva for her assistance in preparing the camera-ready copy of the book. The authors hope that the handbook will prove helpful for a wide audience of researchers, college and university teachers, engineers, and students in various fields of mathematics, mechanics, physics, chemistry, biology, economics, and engineering sciences.},
	language = {en},
	publisher = {Chapman \& Hall/CRC},
	author = {Poli︠a︡nin, A. D. and Manzhirov, A. V.},
	year = {2008},
	note = {OCLC: ocn167516078},
	keywords = {integral equations, Handbooks, manuals, etc, Integral equations},
	file = {Poli︠a︡nin en Manzhirov - 2008 - Handbook of integral equations.pdf:C\:\\Users\\isido\\Zotero\\storage\\83JNUZ6J\\Poli︠a︡nin en Manzhirov - 2008 - Handbook of integral equations.pdf:application/pdf},
}

@misc{noauthor_fredholm_nodate-1,
	title = {Fredholm {Integral} {Equations} colorado},
	url = {https://www.colorado.edu/amath/sites/default/files/attached-files/fredholm.pdf},
	urldate = {2023-03-26},
	keywords = {integral equations},
	file = {fredholm.pdf:C\:\\Users\\isido\\Zotero\\storage\\G9NULA4I\\fredholm.pdf:application/pdf},
}

@misc{heinrich_optimal_2001,
	title = {Optimal {Summation} and {Integration} by {Deterministic}, {Randomized}, and {Quantum} {Algorithms}},
	url = {http://arxiv.org/abs/quant-ph/0105114},
	abstract = {We survey old and new results about optimal algorithms for summation of finite sequences and for integration of functions from Hoelder or Sobolev spaces. First we discuss optimal deterministic and randomized algorithms. Then we add a new aspect, which has not been covered before on conferences about (quasi-) Monte Carlo methods: quantum computation. We give a short introduction into this setting and present recent results of the authors on optimal quantum algorithms for summation and integration. We discuss comparisons between the three settings. The most interesting case for Monte Carlo and quantum integration is that of moderate smoothness k and large dimension d which, in fact, occurs in a number of important applied problems. In that case the deterministic exponent is negligible, so the n{\textasciicircum}\{-1/2\} Monte Carlo and the n{\textasciicircum}\{-1\} quantum speedup essentially constitute the entire convergence rate. We observe that -- there is an exponential speed-up of quantum algorithms over deterministic (classical) algorithms, if k/d tends to zero; -- there is a (roughly) quadratic speed-up of quantum algorithms over randomized classical algorithms, if k/d is small.},
	language = {en},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Heinrich, S. and Novak, E.},
	month = may,
	year = {2001},
	note = {arXiv:quant-ph/0105114},
	keywords = {Mathematics - Numerical Analysis, Quantum Physics, integration, information complexity},
	annote = {Comment: 13 pages, contribution to the 4th International Conference on Monte Carlo and Quasi-Monte Carlo Methods, Hong Kong 2000},
	file = {Heinrich en Novak - 2001 - Optimal Summation and Integration by Deterministic.pdf:C\:\\Users\\isido\\Zotero\\storage\\LK6RKYIX\\Heinrich en Novak - 2001 - Optimal Summation and Integration by Deterministic.pdf:application/pdf},
}

@article{daun_randomized_2011,
	title = {On the randomized solution of initial value problems},
	volume = {27},
	issn = {0885064X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885064X1000066X},
	doi = {10.1016/j.jco.2010.07.002},
	abstract = {We study the randomized solution of initial value problems for systems of ordinary differential equations
\$\$
y{\textasciicircum}\{{\textbackslash}prime\}(x)=f(x, y(x)), x {\textbackslash}in[a, b], y(a)=y\_0 {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}d .
\$\$
Recently S. Heinrich and B. Milla presented an order optimal randomized algorithm solving this problem for \${\textbackslash}gamma\$-smooth input data (i.e. \${\textbackslash}gamma=r+{\textbackslash}rho\$ : the \$r\$-th derivatives of \$f\$ satisfy a \${\textbackslash}rho\$-Hölder condition). This algorithm uses function values and values of derivatives of \$f\$. In this paper we present an order optimal randomized algorithm for the class of \${\textbackslash}gamma\$-smooth functions that uses only values of \$f\$. For this purpose we show how to obtain an order optimal randomized algorithm from an order (sub)optimal deterministic one.},
	language = {en},
	number = {3-4},
	urldate = {2023-03-06},
	journal = {Journal of Complexity},
	author = {Daun, Thomas},
	month = jun,
	year = {2011},
	keywords = {monte carlo, ODE, information complexity},
	pages = {300--311},
	file = {Daun - 2011 - On the randomized solution of initial value proble.pdf:C\:\\Users\\isido\\Zotero\\storage\\NCVQFBK5\\Daun - 2011 - On the randomized solution of initial value proble.pdf:application/pdf},
}

@article{gocwin_randomized_2014,
	title = {Randomized and quantum complexity of nonlinear two-point {BVPs}},
	volume = {245},
	issn = {00963003},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S009630031401073X},
	doi = {10.1016/j.amc.2014.07.106},
	abstract = {We deal with the complexity of nonlinear BVPs with nonlinear two-point boundary conditions. We consider the randomized and quantum models of computation. We assume that the right-hand side function is r times differentiable with all derivatives bounded by a constant. We show that the e-complexity is roughly of order eÀ1=ðrþ1=2Þ in the randomized setting, and eÀ1=ðrþ1Þ in the quantum setting. We compare our results with known results in the deterministic setting. The speed-up of the randomized computations with respect to the deterministic computations is by 1=ðrð2r þ 1ÞÞ in the exponent of 1=e, and the speed-up of the quantum computations by 1=ðrðr þ 1ÞÞ in the exponent.},
	language = {en},
	urldate = {2023-03-19},
	journal = {Applied Mathematics and Computation},
	author = {Goćwin, Maciej},
	month = oct,
	year = {2014},
	keywords = {ODE, information complexity},
	pages = {357--371},
	file = {Goćwin - 2014 - Randomized and quantum complexity of nonlinear two.pdf:C\:\\Users\\isido\\Zotero\\storage\\AVIKYTNP\\Goćwin - 2014 - Randomized and quantum complexity of nonlinear two.pdf:application/pdf},
}

@misc{marcin_anforowicz_just_2022,
	title = {The "{Just} {One} {More}" {Paradox}},
	url = {https://www.youtube.com/watch?v=_FuuYSM7yOo},
	urldate = {2023-05-29},
	author = {{Marcin Anforowicz}},
	month = sep,
	year = {2022},
}

@misc{cuturi_sinkhorn_2013,
	title = {Sinkhorn {Distances}: {Lightspeed} {Computation} of {Optimal} {Transportation} {Distances}},
	shorttitle = {Sinkhorn {Distances}},
	url = {http://arxiv.org/abs/1306.0895},
	abstract = {Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms’ dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp’s matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem.},
	language = {en},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Cuturi, Marco},
	month = jun,
	year = {2013},
	note = {arXiv:1306.0895 [stat]},
	keywords = {Statistics - Machine Learning},
	file = {Cuturi - 2013 - Sinkhorn Distances Lightspeed Computation of Opti.pdf:C\:\\Users\\isido\\Zotero\\storage\\772CFQJN\\Cuturi - 2013 - Sinkhorn Distances Lightspeed Computation of Opti.pdf:application/pdf},
}

@article{hu_heat_2020,
	title = {Heat {Conduction} {Simulation} of {2D} {Moving} {Heat} {Source} {Problems} {Using} a {Moving} {Mesh} {Method}},
	volume = {2020},
	issn = {1687-9120, 1687-9139},
	url = {https://www.hindawi.com/journals/amp/2020/6067854/},
	doi = {10.1155/2020/6067854},
	abstract = {This paper focuses on efficiently numerical investigation of two-dimensional heat conduction problems of material subjected to multiple moving Gaussian point heat sources. All heat sources are imposed on the inside of material and assumed to move along some specified straight lines or curves with time-dependent velocities. A simple but efficient moving mesh method, which continuously adjusts the two-dimensional mesh dimension by dimension upon the one-dimensional moving mesh partial differential equation with an appropriate monitor function of the temperature field, has been developed. The physical model problem is then solved on this adaptive moving mesh. Numerical experiments are presented to exhibit the capability of the proposed moving mesh algorithm to efficiently and accurately simulate the moving heat source problems. The transient heat conduction phenomena due to various parameters of the moving heat sources, including the number of heat sources and the types of motion, are well simulated and investigated.},
	language = {en},
	urldate = {2023-05-29},
	journal = {Advances in Mathematical Physics},
	author = {Hu, Zhicheng and Liu, Zhihui},
	month = feb,
	year = {2020},
	keywords = {PDE},
	pages = {1--16},
	file = {Hu en Liu - 2020 - Heat Conduction Simulation of 2D Moving Heat Sourc.pdf:C\:\\Users\\isido\\Zotero\\storage\\G7ZGFDCD\\Hu en Liu - 2020 - Heat Conduction Simulation of 2D Moving Heat Sourc.pdf:application/pdf},
}

@misc{mathemaniac_weirdest_2022,
	title = {The weirdest paradox in statistics (and machine learning)},
	url = {https://www.youtube.com/watch?v=cUqoHQDinCM},
	abstract = {Stein's paradox is of fundamental importance in modern statistics, introducing concepts of shrinkage to further reduce the mean squared error, especially in higher dimensional statistics that is particularly relevant nowadays, in the world of machine learning, for example. However, this is usually ignored, because it is mostly seen as a toy problem. Precisely because it is such a simple problem that illustrates the problem of maximum likelihood estimation! This paradox is the subject of many blogposts (linked below), but not really here on YouTube, except in some lecture recordings, so I have to bring this up to YouTube.

This is not to say that maximum likelihood estimator is not useful - in most situations, especially in lower dimensional statistics, it is still good, but to hold it to such a high place, as statisticians did before 1961? That is not a healthy attitude to this theory.

One thing I did not say, but perhaps a lot of people will want me to, is that this is an emprical Bayes estimator, but again, more links below.

Video chapters:
00:00 Introduction
04:38 Chapter 1: The "best" estimator
09:48 Chapter 2: Why shrinkage works
15:51 Chapter 3: Bias-variance tradeoff
18:45 Chapter 4: Applications},
	urldate = {2023-06-05},
	author = {{Mathemaniac}},
	month = aug,
	year = {2022},
}

@misc{sugimoto_practical_2023,
	title = {A {Practical} {Walk}-on-{Boundary} {Method} for {Boundary} {Value} {Problems}},
	url = {http://arxiv.org/abs/2305.04403},
	doi = {10.1145/3592109},
	abstract = {We introduce the walk-on-boundary (WoB) method for solving boundary value problems to computer graphics. WoB is a grid-free Monte Carlo solver for certain classes of second order partial differential equations. A similar Monte Carlo solver, the walk-on-spheres (WoS) method, has been recently popularized in computer graphics due to its advantages over traditional spatial discretization-based alternatives. We show that WoB’s intrinsic properties yield further advantages beyond those of WoS. Unlike WoS, WoB naturally supports various boundary conditions (Dirichlet, Neumann, Robin, and mixed) for both interior and exterior domains. WoB builds upon boundary integral formulations, and it is mathematically more similar to light transport simulation in rendering than the random walk formulation of WoS. This similarity between WoB and rendering allows us to implement WoB on top of Monte Carlo ray tracing, and to incorporate advanced rendering techniques (e.g., bidirectional estimators with multiple importance sampling, the virtual point lights method, and Markov chain Monte Carlo) into WoB. WoB does not suffer from the intrinsic bias of WoS near the boundary and can estimate solutions precisely on the boundary. Our numerical results highlight the advantages of WoB over WoS as an attractive alternative to solve boundary value problems based on Monte Carlo. CCS Concepts: • Mathematics of computing → Integral equations; Partial differential equations; • Computing methodologies → Ray tracing.},
	language = {en},
	urldate = {2023-06-07},
	author = {Sugimoto, Ryusuke and Chen, Terry and Jiang, Yiti and Batty, Christopher and Hachisuka, Toshiya},
	month = may,
	year = {2023},
	note = {arXiv:2305.04403 [cs, math]},
	keywords = {PDE, walk on spheres, Mathematics - Numerical Analysis, Computer Science - Graphics, boundary value problems},
	annote = {Comment: Accepted to ACM SIGGRAPH North America 2023 / Transactions on Graphics. See https://rsugimoto.net/WoBforBVPsProject/ for updates, including the reference implementation},
	file = {Sugimoto e.a. - 2023 - A Practical Walk-on-Boundary Method for Boundary V.pdf:C\:\\Users\\isido\\Zotero\\storage\\7AGTX5CS\\Sugimoto e.a. - 2023 - A Practical Walk-on-Boundary Method for Boundary V.pdf:application/pdf},
}

@article{mossberg_gpu-accelerated_nodate,
	title = {{GPU}-{Accelerated} {Monte} {Carlo} {Geometry} {Processing} for {Gradient}-{Domain} {Methods}},
	abstract = {This thesis extends the utility of the Monte Carlo approach to PDE-based methods presented in the paper Monte Carlo Geometry Processing. In particular, we implement this method on the GPU using CUDA, and investigate more viable methods of estimating the source integral when solving Poisson’s equation with intricate source terms. This is the case for a large group of gradient-domain methods in computer graphics, where source terms are represented by discrete volumetric data on regular grids. We develop unbiased source integral estimators like image-based importance sampling (IBIS) and biased estimators like source integral caching (SIC), and evaluate these against existing GPU-accelerated finite difference solvers for gradient-domain applications. By decoupling the source integration step from the WoS-algorithm, we find that the SIC method can improve performance by several orders of magnitude, making it competitive with existing finite difference solvers in many cases. We further investigate the viability of distance fields for accelerated distance queries, and find that these can provide significant performance improvements compared to BVHs without meaningfully affecting bias.},
	language = {en},
	author = {Mossberg, Linus},
	keywords = {monte carlo, PDE, walk on spheres, boundary value problems},
	file = {Mossberg - GPU-Accelerated Monte Carlo Geometry Processing fo.pdf:C\:\\Users\\isido\\Zotero\\storage\\M3CIZFYI\\Mossberg - GPU-Accelerated Monte Carlo Geometry Processing fo.pdf:application/pdf},
}

@article{tibshirani_nonparametric_nodate,
	title = {Nonparametric {Regression} (and {Classiﬁcation})},
	language = {en},
	author = {Tibshirani, Ryan},
	file = {Tibshirani - Nonparametric Regression (and Classiﬁcation).pdf:C\:\\Users\\isido\\Zotero\\storage\\GSH6DL6E\\Tibshirani - Nonparametric Regression (and Classiﬁcation).pdf:application/pdf},
}

@misc{ryan_t_lecture_2015,
	title = {Lecture 11: {Nonparametric} {Bayes}},
	shorttitle = {Lecture 11},
	url = {https://www.youtube.com/watch?v=l0HQXtqnBZY},
	abstract = {Lecture Date: 02/18/15},
	urldate = {2023-06-09},
	author = {{Ryan T}},
	month = feb,
	year = {2015},
}

@article{tibshirani_sparsity_nodate,
	title = {Sparsity, the {Lasso}, and {Friends}},
	language = {en},
	author = {Tibshirani, Ryan},
	file = {Tibshirani - Sparsity, the Lasso, and Friends.pdf:C\:\\Users\\isido\\Zotero\\storage\\FMEX9G6K\\Tibshirani - Sparsity, the Lasso, and Friends.pdf:application/pdf},
}

@incollection{montavon_stochastic_2012,
	address = {Berlin, Heidelberg},
	title = {Stochastic {Gradient} {Descent} {Tricks}},
	volume = {7700},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	url = {http://link.springer.com/10.1007/978-3-642-35289-8_25},
	abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
	language = {en},
	urldate = {2023-06-09},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bottou, Léon},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_25},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {optimization, gradient descent},
	pages = {421--436},
	file = {Bottou - 2012 - Stochastic Gradient Descent Tricks.pdf:C\:\\Users\\isido\\Zotero\\storage\\38JMSAZM\\Bottou - 2012 - Stochastic Gradient Descent Tricks.pdf:application/pdf},
}

@article{tibshirani_numerical_nodate,
	title = {Numerical {Linear} {Algebra} {Primer}},
	language = {en},
	author = {Tibshirani, Ryan},
	file = {Tibshirani - Numerical Linear Algebra Primer.pdf:C\:\\Users\\isido\\Zotero\\storage\\VZ7YFNWJ\\Tibshirani - Numerical Linear Algebra Primer.pdf:application/pdf},
}

@article{blanes_magnus_2009,
	title = {The {Magnus} expansion and some of its applications},
	volume = {470},
	issn = {03701573},
	url = {http://arxiv.org/abs/0810.5488},
	doi = {10.1016/j.physrep.2008.11.001},
	abstract = {Approximate resolution of linear systems of diﬀerential equations with varying coeﬃcients is a recurrent problem shared by a number of scientiﬁc and engineering areas, ranging from Quantum Mechanics to Control Theory. When formulated in operator or matrix form, the Magnus expansion furnishes an elegant setting to built up approximate exponential representations of the solution of the system. It provides a power series expansion for the corresponding exponent and is sometimes referred to as Time-Dependent Exponential Perturbation Theory. Every Magnus approximant corresponds in Perturbation Theory to a partial re-summation of inﬁnite terms with the important additional property of preserving at any order certain symmetries of the exact solution.},
	language = {en},
	number = {5-6},
	urldate = {2023-06-12},
	journal = {Physics Reports},
	author = {Blanes, S. and Casas, F. and Oteo, J. A. and Ros, J.},
	month = jan,
	year = {2009},
	note = {arXiv:0810.5488 [math-ph]},
	keywords = {ODE, Mathematical Physics},
	pages = {151--238},
	annote = {Comment: Report on the Magnus expansion for differential equations and its applications to several physical problems},
	file = {0810.5488.pdf:C\:\\Users\\isido\\Zotero\\storage\\352XINMQ\\0810.5488.pdf:application/pdf},
}

@book{novak_deterministic_1988,
	address = {Berlin Heidelberg},
	series = {Lecture notes in mathematics},
	title = {Deterministic and stochastic error bounds in numerical analysis},
	isbn = {978-3-540-50368-2 978-0-387-50368-4},
	abstract = {In these notes we want to investigate different deterministic and stochastic error bounds of numerical analysis. For many computational problems (such as approximation, optimization, and quadrature) we have only partial information and consequently such problems can only be solved with uncertainty in the answer. The information-centered approach asks for optimal methods and optimal error bounds if only the type of information available is indicated. We begin with worst case error bounds for deterministic methods and consider relations between these error bounds and the n-widths of the class of problem elements (1.2). In 1.3 we give worst case error bounds for some special problems. We are mainly interested in the problems of approximation (App), optimization (Opt and Opt*), and quadrature or integration (Int). We consider different function classes, for both adaptive and nonadaptive methods. First of all, I explain the information-based approach by means of an example.},
	language = {en},
	number = {1349},
	publisher = {Springer},
	author = {Novak, Erich},
	year = {1988},
	keywords = {information complexity},
	file = {Novak - 1988 - Deterministic and stochastic error bounds in numer.pdf:C\:\\Users\\isido\\Zotero\\storage\\CT75RT9T\\Novak - 1988 - Deterministic and stochastic error bounds in numer.pdf:application/pdf;Novak - Deterministic and Stochastic Error Bounds In Numer.pdf:C\:\\Users\\isido\\Zotero\\storage\\3YULLQV7\\Novak - Deterministic and Stochastic Error Bounds In Numer.pdf:application/pdf},
}

@misc{noauthor_no_2023,
	title = {No free lunch theorem},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=No_free_lunch_theorem&oldid=1154448734},
	abstract = {In mathematical folklore, the "no free lunch" (NFL) theorem (sometimes pluralized) of David Wolpert and William Macready appears in the 1997 "No Free Lunch Theorems for Optimization". Wolpert had previously derived no free lunch theorems for machine learning (statistical inference). The name alludes to the saying "there ain't no such thing as a free lunch", that is, there are no easy shortcuts to success.
In 2005, Wolpert and Macready themselves indicated that the first theorem in their paper "state[s] that any two optimization algorithms are equivalent when their performance is averaged across all possible problems".The "no free lunch" (NFL) theorem is an easily stated and easily understood consequence of theorems Wolpert and Macready actually prove. It is weaker than the proven theorems, and thus does not encapsulate them. Various investigators have extended the work of Wolpert and Macready substantively. In terms of how the NFL theorem is used in the context of the research area, the no free lunch in search and optimization is a field that is dedicated for purposes of mathematically analyzing data for statistical identity, particularly search and optimization.While some scholars argue that NFL conveys important insight, others argue that NFL is of little relevance to machine learning research.},
	language = {en},
	urldate = {2023-06-13},
	journal = {Wikipedia},
	month = may,
	year = {2023},
	note = {Page Version ID: 1154448734},
	keywords = {optimization, machine learning},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\S7SXIJCH\\No_free_lunch_theorem.html:text/html},
}

@misc{noauthor_information-based_2022,
	title = {Information-based complexity},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Information-based_complexity&oldid=1116392336},
	abstract = {Information-based complexity (IBC) studies optimal algorithms and computational complexity for the continuous problems that arise in physical science, economics, engineering, and mathematical finance. IBC has studied such continuous problems as path integration, partial differential equations, systems of ordinary differential equations, nonlinear equations, integral equations, fixed points, and very-high-dimensional integration. All these problems involve functions (typically multivariate) of a real or complex variable. Since one can never obtain a closed-form solution to the problems of interest one has to settle for a numerical solution. Since a function of a real or complex variable cannot be entered into a digital computer, the solution of continuous problems involves partial information. To give a simple illustration, in the numerical approximation of an integral, only samples of the integrand at a finite number of points are available. In the numerical solution of partial differential equations the functions specifying the boundary conditions and the coefficients of the differential operator can only be sampled. Furthermore, this partial information can be expensive to obtain. Finally the information is often contaminated by noise.
The goal of information-based complexity is to create a theory of computational complexity and optimal algorithms for problems with partial, contaminated and priced information, and to apply the results to answering questions in various disciplines. Examples of such disciplines include physics, economics, mathematical finance, computer vision, control theory, geophysics, medical imaging, weather forecasting and climate prediction, and statistics. The theory is developed over abstract spaces, typically Hilbert or Banach spaces, while the applications are usually for multivariate problems.
Since the information is partial and contaminated, only approximate solutions can be obtained. IBC studies computational complexity and optimal algorithms for approximate solutions in various settings. Since the worst case setting often leads to negative results such as unsolvability and intractability, settings with weaker assurances such as average, probabilistic and randomized are also studied. A fairly new area of IBC research is continuous quantum computing.},
	language = {en},
	urldate = {2023-06-14},
	journal = {Wikipedia},
	month = oct,
	year = {2022},
	note = {Page Version ID: 1116392336},
	keywords = {information complexity},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\K3D6TU8D\\Information-based_complexity.html:text/html},
}

@misc{robbe_recycling_2018,
	title = {Recycling {Samples} in the {Multigrid} {Multilevel} ({Quasi}-){Monte} {Carlo} {Method}},
	url = {http://arxiv.org/abs/1806.05619},
	abstract = {The Multilevel Monte Carlo method is an eﬃcient variance reduction technique. It uses a sequence of coarse approximations to reduce the computational cost in uncertainty quantiﬁcation applications. The method is nowadays often considered to be the method of choice for solving PDEs with random coeﬃcients when many uncertainties are involved. When using Full Multigrid to solve the deterministic problem, coarse solutions obtained by the solver can be recycled as samples in the Multilevel Monte Carlo method, as was pointed out by Kumar, Oosterlee and Dwight [Int. J. Uncertain. Quantif., 7 (2017), pp. 57–81]. In this article, an alternative approach is considered, using Quasi-Monte Carlo points, to speed up convergence. Additionally, our method comes with an improved variance estimate which is also valid in case of the Monte Carlo based approach. The new method is illustrated on the example of an elliptic PDE with lognormal diﬀusion coeﬃcient. Numerical results for a variety of random ﬁelds with diﬀerent smoothness parameters in the Matérn covariance function show that sample recycling is more eﬃcient when the input random ﬁeld is nonsmooth.},
	language = {en},
	urldate = {2023-06-14},
	publisher = {arXiv},
	author = {Robbe, Pieterjan and Nuyens, Dirk and Vandewalle, Stefan},
	month = jun,
	year = {2018},
	note = {arXiv:1806.05619 [math]},
	keywords = {monte carlo, PDE, Mathematics - Numerical Analysis},
	file = {Robbe e.a. - 2018 - Recycling Samples in the Multigrid Multilevel (Qua.pdf:C\:\\Users\\isido\\Zotero\\storage\\PWK6DY7M\\Robbe e.a. - 2018 - Recycling Samples in the Multigrid Multilevel (Qua.pdf:application/pdf},
}

@misc{kuo_application_2017,
	title = {Application of quasi-{Monte} {Carlo} methods to {PDEs} with random coefficients -- an overview and tutorial},
	url = {http://arxiv.org/abs/1710.10984},
	abstract = {This article provides a high-level overview of some recent works on the application of quasi-Monte Carlo (QMC) methods to PDEs with random coeﬃcients. It is based on an indepth survey of a similar title by the same authors, with an accompanying software package which is also brieﬂy discussed here. Embedded in this article is a step-by-step tutorial of the required analysis for the setting known as the uniform case with ﬁrst order QMC rules. The aim of this article is to provide an easy entry point for QMC experts wanting to start research in this direction and for PDE analysts and practitioners wanting to tap into contemporary QMC theory and methods.},
	language = {en},
	urldate = {2023-06-14},
	publisher = {arXiv},
	author = {Kuo, Frances Y. and Nuyens, Dirk},
	month = oct,
	year = {2017},
	note = {arXiv:1710.10984 [math]},
	keywords = {monte carlo, PDE, Mathematics - Numerical Analysis, random PDE},
	annote = {Comment: arXiv admin note: text overlap with arXiv:1606.06613},
	file = {Kuo en Nuyens - 2017 - Application of quasi-Monte Carlo methods to PDEs w.pdf:C\:\\Users\\isido\\Zotero\\storage\\CN5A6NQ6\\Kuo en Nuyens - 2017 - Application of quasi-Monte Carlo methods to PDEs w.pdf:application/pdf},
}

@misc{heinrich_monte_2001,
	title = {From {Monte} {Carlo} to {Quantum} {Computation}},
	url = {http://arxiv.org/abs/quant-ph/0112152},
	abstract = {Quantum computing was so far mainly concerned with discrete problems. Recently, E. Novak and the author studied quantum algorithms for high dimensional integration and dealt with the question, which advantages quantum computing can bring over classical deterministic or randomized methods for this type of problem.},
	language = {en},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Heinrich, Stefan},
	month = dec,
	year = {2001},
	note = {arXiv:quant-ph/0112152},
	keywords = {monte carlo, Quantum Physics, Quantum computing},
	annote = {Comment: Paper submitted to the Proceedings of the 3rd IMACS Seminar on Monte Carlo Methods MCM2001, Salzburg. 15 pages},
	file = {Heinrich - 2001 - From Monte Carlo to Quantum Computation.pdf:C\:\\Users\\isido\\Zotero\\storage\\2BSDMTVF\\Heinrich - 2001 - From Monte Carlo to Quantum Computation.pdf:application/pdf},
}

@misc{lan_optimal_2015,
	title = {An optimal randomized incremental gradient method},
	url = {http://arxiv.org/abs/1507.02000},
	abstract = {In this paper, we consider a class of finite-sum convex optimization problems whose objective function is given by the summation of \$m\$ (\${\textbackslash}ge 1\$) smooth components together with some other relatively simple terms. We first introduce a deterministic primal-dual gradient (PDG) method that can achieve the optimal black-box iteration complexity for solving these composite optimization problems using a primal-dual termination criterion. Our major contribution is to develop a randomized primal-dual gradient (RPDG) method, which needs to compute the gradient of only one randomly selected smooth component at each iteration, but can possibly achieve better complexity than PDG in terms of the total number of gradient evaluations. More specifically, we show that the total number of gradient evaluations performed by RPDG can be \$\{{\textbackslash}cal O\} ({\textbackslash}sqrt\{m\})\$ times smaller, both in expectation and with high probability, than those performed by deterministic optimal first-order methods under favorable situations. We also show that the complexity of the RPDG method is not improvable by developing a new lower complexity bound for a general class of randomized methods for solving large-scale finite-sum convex optimization problems. Moreover, through the development of PDG and RPDG, we introduce a novel game-theoretic interpretation for these optimal methods for convex optimization.},
	language = {en},
	urldate = {2023-06-16},
	publisher = {arXiv},
	author = {Lan, Guanghui and Zhou, Yi},
	month = oct,
	year = {2015},
	note = {arXiv:1507.02000 [cs, math, stat]},
	keywords = {Statistics - Machine Learning, Mathematics - Optimization and Control, optimization, Computer Science - Computational Complexity, gradient descent},
	file = {Lan en Zhou - 2015 - An optimal randomized incremental gradient method.pdf:C\:\\Users\\isido\\Zotero\\storage\\YKXRE23K\\Lan en Zhou - 2015 - An optimal randomized incremental gradient method.pdf:application/pdf},
}

@article{driggs_accelerating_2022,
	title = {Accelerating variance-reduced stochastic gradient methods},
	volume = {191},
	issn = {0025-5610, 1436-4646},
	url = {https://link.springer.com/10.1007/s10107-020-01566-2},
	doi = {10.1007/s10107-020-01566-2},
	abstract = {Variance reduction is a crucial tool for improving the slow convergence of stochastic gradient descent. Only a few variance-reduced methods, however, have yet been shown to directly beneﬁt from Nesterov’s acceleration techniques to match the convergence rates of accelerated gradient methods. Such approaches rely on “negative momentum”, a technique for further variance reduction that is generally speciﬁc to the SVRG gradient estimator. In this work, we show for the ﬁrst time that negative momentum is unnecessary for acceleration and develop a universal acceleration framework that allows all popular variance-reduced methods to achieve accelerated convergence rates. The constants appearing in these rates, including their dependence on the number of functions n, scale with the mean-squared-error and bias of the gradient estimator. In a series of numerical experiments, we demonstrate that versions of SAGA, SVRG, SARAH, and SARGE using our framework signiﬁcantly outperform non-accelerated versions and compare favourably with algorithms using negative momentum.},
	language = {en},
	number = {2},
	urldate = {2023-06-16},
	journal = {Mathematical Programming},
	author = {Driggs, Derek and Ehrhardt, Matthias J. and Schönlieb, Carola-Bibiane},
	month = feb,
	year = {2022},
	keywords = {optimization, gradient descent},
	pages = {671--715},
	file = {Driggs e.a. - 2022 - Accelerating variance-reduced stochastic gradient .pdf:C\:\\Users\\isido\\Zotero\\storage\\HBSFVERS\\Driggs e.a. - 2022 - Accelerating variance-reduced stochastic gradient .pdf:application/pdf},
}

@article{kettunen_unbiased_2021,
	title = {An unbiased ray-marching transmittance estimator},
	volume = {40},
	issn = {0730-0301, 1557-7368},
	url = {http://arxiv.org/abs/2102.10294},
	doi = {10.1145/3450626.3459937},
	abstract = {We present an in-depth analysis of the sources of variance in state-of-the-art unbiased volumetric transmittance estimators, and propose several new methods for improving their efficiency. These combine to produce a single estimator that is universally optimal relative to prior work, with up to several orders of magnitude lower variance at the same cost, and has zero variance for any ray with non-varying extinction. We first reduce the variance of truncated power-series estimators using a novel efficient application of U-statistics. We then greatly reduce the average expansion order of the power series and redistribute density evaluations to filter the optical depth estimates with an equidistant sampling comb. Combined with the use of an online control variate built from a sampled mean density estimate, the resulting estimator effectively performs ray marching most of the time while using rarely-sampled higher order terms to correct the bias.},
	language = {en},
	number = {4},
	urldate = {2023-06-18},
	journal = {ACM Transactions on Graphics},
	author = {Kettunen, Markus and d'Eon, Eugene and Pantaleoni, Jacopo and Novak, Jan},
	month = aug,
	year = {2021},
	note = {arXiv:2102.10294 [cs]},
	keywords = {Computer Science - Graphics, rendering, exponential integrators},
	pages = {1--20},
	annote = {Comment: 20 pages},
	file = {Kettunen e.a. - 2021 - An unbiased ray-marching transmittance estimator.pdf:C\:\\Users\\isido\\Zotero\\storage\\6276WE78\\Kettunen e.a. - 2021 - An unbiased ray-marching transmittance estimator.pdf:application/pdf},
}

@article{nimier-david_unbiased_2022,
	title = {Unbiased inverse volume rendering with differential trackers},
	volume = {41},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3528223.3530073},
	doi = {10.1145/3528223.3530073},
	abstract = {Volumetric representations are popular in inverse rendering because they have a simple parameterization, are smoothly varying, and transparently handle topology changes. However, incorporating the full volumetric transport of light is costly and challenging, often leading practitioners to implement simplified models, such as purely emissive and absorbing volumes with "baked" lighting. One such challenge is the efficient estimation of the gradients of the volume's appearance with respect to its scattering and absorption parameters. We show that the straightforward approach---differentiating a volumetric free-flight sampler---can lead to biased and high-variance gradients, hindering optimization. Instead, we propose using a new sampling strategy:
              differential ratio tracking
              , which is unbiased, yields low-variance gradients, and runs in linear time. Differential ratio tracking combines ratio tracking and reservoir sampling to estimate gradients by sampling distances proportional to the unweighted transmittance rather than the usual extinction-weighted transmittance. In addition, we observe local minima when optimizing scattering parameters to reproduce dense volumes or surfaces. We show that these local minima can be overcome by bootstrapping the optimization from nonphysical emissive volumes that are easily optimized.},
	language = {en},
	number = {4},
	urldate = {2023-06-18},
	journal = {ACM Transactions on Graphics},
	author = {Nimier-David, Merlin and Müller, Thomas and Keller, Alexander and Jakob, Wenzel},
	month = jul,
	year = {2022},
	keywords = {rendering, inverse problem},
	pages = {1--20},
	file = {Nimier-David e.a. - 2022 - Unbiased inverse volume rendering with differentia.pdf:C\:\\Users\\isido\\Zotero\\storage\\4PYPVM7M\\Nimier-David e.a. - 2022 - Unbiased inverse volume rendering with differentia.pdf:application/pdf},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.},
	language = {en},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning, optimization, machine learning, gradient descent},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {Kingma en Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:C\:\\Users\\isido\\Zotero\\storage\\PZYRRCM2\\Kingma en Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf},
}

@misc{buvoli_imex_2021,
	title = {{IMEX} {Runge}-{Kutta} {Parareal} for {Non}-{Diffusive} {Equations}},
	url = {http://arxiv.org/abs/2011.01604},
	abstract = {Parareal is a widely studied parallel-in-time method that can achieve meaningful speedup on certain problems. However, it is well known that the method typically performs poorly on non-diﬀusive equations. This paper analyzes linear stability and convergence for IMEX Runge-Kutta Parareal methods on non-diﬀusive equations. By combining standard linear stability analysis with a simple convergence analysis, we ﬁnd that certain Parareal conﬁgurations can achieve parallel speedup on non-diﬀusive equations. These stable conﬁgurations all posses low iteration counts, large block sizes, and a large number of processors. Numerical examples using the nonlinear Schro¨dinger equation demonstrate the analytical conclusions.},
	language = {en},
	urldate = {2023-06-25},
	publisher = {arXiv},
	author = {Buvoli, Tommaso and Minion, Michael L.},
	month = jul,
	year = {2021},
	note = {arXiv:2011.01604 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, ODE, parareal},
	file = {Buvoli en Minion - 2021 - IMEX Runge-Kutta Parareal for Non-Diffusive Equati.pdf:C\:\\Users\\isido\\Zotero\\storage\\JJGX8M7T\\Buvoli en Minion - 2021 - IMEX Runge-Kutta Parareal for Non-Diffusive Equati.pdf:application/pdf},
}

@misc{buvoli_stability_2021,
	title = {On the {Stability} of {Exponential} {Integrators} for {Non}-{Diffusive} {Equations}},
	url = {http://arxiv.org/abs/2108.00185},
	abstract = {Exponential integrators are a well-known class of time integration methods that have been the subject of many studies and developments in the past two decades. Surprisingly, there have been limited eﬀorts to analyze their stability and eﬃciency on non-diﬀusive equations to date. In this paper we apply linear stability analysis to showcase the poor stability properties of exponential integrators on non-diﬀusive problems. We then propose a simple repartitioning approach that stabilizes the integrators and enables the eﬃcient solution of stiﬀ, non-diﬀusive equations. To validate the eﬀectiveness of our approach, we perform several numerical experiments that compare partitioned exponential integrators to unmodiﬁed ones. We also compare repartitioning to the well-known approach of adding hyperviscosity to the equation right-hand-side. Overall, we ﬁnd that the repartitioning restores convergence at large timesteps and, unlike hyperviscosity, it does not require the use of high-order spatial derivatives.},
	language = {en},
	urldate = {2023-06-25},
	publisher = {arXiv},
	author = {Buvoli, Tommaso and Minion, Michael L.},
	month = jul,
	year = {2021},
	note = {arXiv:2108.00185 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, ODE, exponential integrators},
	file = {Buvoli en Minion - 2021 - On the Stability of Exponential Integrators for No.pdf:C\:\\Users\\isido\\Zotero\\storage\\ZTRRZAV8\\Buvoli en Minion - 2021 - On the Stability of Exponential Integrators for No.pdf:application/pdf},
}

@misc{buvoli_exponential_2023,
	title = {Exponential {Runge}-{Kutta} {Parareal} for {Non}-{Diffusive} {Equations}},
	url = {http://arxiv.org/abs/2301.03764},
	abstract = {Parareal is a well-known parallel-in-time algorithm that combines a coarse and ﬁne propagator within a parallel iteration. It allows for large-scale parallelism that leads to signiﬁcantly reduced computational time compared to serial time-stepping methods. However, like many parallel-intime methods it can fail to achieve parallel speedup when applied to non-diﬀusive equations such as hyperbolic systems or dispersive nonlinear wave equations. This paper explores the use of exponential integrators within the Parareal iteration. Exponential integrators are particularly interesting candidates for Parareal because of their ability to resolve fast-moving waves, even at the large stepsizes used by coarse propagators. This work begins with an introduction to exponential Parareal integrators followed by several motivating numerical experiments involving the nonlinear Schrödinger equation. These experiments are then analyzed using linear analysis that approximates the stability and convergence properties of the exponential Parareal iteration on nonlinear problems. The paper concludes with two additional numerical experiments involving the dispersive Kadomtsev-Petviashvili equation and the hyperbolic Vlasov-Poisson equation. These experiments demonstrate that exponential Parareal methods can achieve signiﬁcant parallel speedup on diﬀerent types of non-diﬀusive equations.},
	language = {en},
	urldate = {2023-06-25},
	publisher = {arXiv},
	author = {Buvoli, Tommaso and Minion, Michael L.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.03764 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, ODE, exponential integrators, parareal},
	file = {Buvoli en Minion - 2023 - Exponential Runge-Kutta Parareal for Non-Diffusive.pdf:C\:\\Users\\isido\\Zotero\\storage\\EJTQSHKJ\\Buvoli en Minion - 2023 - Exponential Runge-Kutta Parareal for Non-Diffusive.pdf:application/pdf},
}

@misc{gander_unified_2023-1,
	title = {A unified analysis framework for iterative parallel-in-time algorithms},
	url = {http://arxiv.org/abs/2203.16069},
	abstract = {Parallel-in-time integration has been the focus of intensive research eﬀorts over the past two decades due to the advent of massively parallel computer architectures and the scaling limits of purely spatial parallelization. Various iterative parallel-in-time (PinT) algorithms have been proposed, like Parareal, PFASST, MGRIT, and Space-Time Multi-Grid (STMG). These methods have been described using diﬀerent notations, and the convergence estimates that are available are diﬃcult to compare. We describe Parareal, PFASST, MGRIT and STMG for the Dahlquist model problem using a common notation and give precise convergence estimates using generating functions. This allows us, for the ﬁrst time, to directly compare their convergence. We prove that all four methods eventually converge super-linearly, and also compare them numerically. The generating function framework provides further opportunities to explore and analyze existing and new methods.},
	language = {en},
	urldate = {2023-06-25},
	publisher = {arXiv},
	author = {Gander, M. J. and Lunet, T. and Ruprecht, D. and Speck, R.},
	month = apr,
	year = {2023},
	note = {arXiv:2203.16069 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, ODE, Computer Science - Computational Engineering, Finance, and Science, parareal},
	file = {Gander e.a. - 2023 - A unified analysis framework for iterative paralle.pdf:C\:\\Users\\isido\\Zotero\\storage\\GJFVIYAG\\Gander e.a. - 2023 - A unified analysis framework for iterative paralle.pdf:application/pdf},
}

@article{zanger_quantum_2021,
	title = {Quantum {Algorithms} for {Solving} {Ordinary} {Differential} {Equations} via {Classical} {Integration} {Methods}},
	volume = {5},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/2012.09469},
	doi = {10.22331/q-2021-07-13-502},
	abstract = {Identifying computational tasks suitable for (future) quantum computers is an active field of research. Here we explore utilizing quantum computers for the purpose of solving differential equations. We consider two approaches: (i) basis encoding and fixed-point arithmetic on a digital quantum computer, and (ii) representing and solving high-order Runge-Kutta methods as optimization problems on quantum annealers. As realizations applied to two-dimensional linear ordinary differential equations, we devise and simulate corresponding digital quantum circuits, and implement and run a 6\${\textasciicircum}\{{\textbackslash}mathrm\{th\}\}\$ order Gauss-Legendre collocation method on a D-Wave 2000Q system, showing good agreement with the reference solution. We find that the quantum annealing approach exhibits the largest potential for high-order implicit integration methods. As promising future scenario, the digital arithmetic method could be employed as an "oracle" within quantum search algorithms for inverse problems.},
	language = {en},
	urldate = {2023-06-25},
	journal = {Quantum},
	author = {Zanger, Benjamin and Mendl, Christian B. and Schulz, Martin and Schreiber, Martin},
	month = jul,
	year = {2021},
	note = {arXiv:2012.09469 [quant-ph]},
	keywords = {ODE, Quantum Physics, Quantum computing},
	pages = {502},
	file = {Zanger e.a. - 2021 - Quantum Algorithms for Solving Ordinary Differenti.pdf:C\:\\Users\\isido\\Zotero\\storage\\6LH65XLT\\Zanger e.a. - 2021 - Quantum Algorithms for Solving Ordinary Differenti.pdf:application/pdf},
}

@misc{huang_accelerating_2022,
	title = {Accelerating {Numerical} {Solvers} for {Large}-{Scale} {Simulation} of {Dynamical} {System} via {NeurVec}},
	url = {http://arxiv.org/abs/2208.03680},
	abstract = {Ensemble-based large-scale simulation of dynamical systems is essential to a wide range of science and engineering problems. Conventional numerical solvers used in the simulation are significantly limited by the step size for time integration, which hampers efficiency and feasibility especially when high accuracy is desired. To overcome this limitation, we propose a data-driven corrector method that allows using large step sizes while compensating for the integration error for high accuracy. This corrector is represented in the form of a vector-valued function and is modeled by a neural network to regress the error in the phase space. Hence we name the corrector neural vector (NeurVec). We show that NeurVec can achieve the same accuracy as traditional solvers with much larger step sizes. We empirically demonstrate that NeurVec can accelerate a variety of numerical solvers significantly and overcome the stability restriction of these solvers. Our results on benchmark problems, ranging from high-dimensional problems to chaotic systems, suggest that NeurVec is capable of capturing the leading error term and maintaining the statistics of ensemble forecasts.},
	language = {en},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Huang, Zhongzhan and Liang, Senwei and Zhang, Hong and Yang, Haizhao and Lin, Liang},
	month = aug,
	year = {2022},
	note = {arXiv:2208.03680 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, ODE, Computer Science - Computational Engineering, Finance, and Science, Computer Science - Artificial Intelligence, neural networks},
	annote = {Comment: Technical report},
	file = {Huang e.a. - 2022 - Accelerating Numerical Solvers for Large-Scale Sim.pdf:C\:\\Users\\isido\\Zotero\\storage\\V8G6BSNX\\Huang e.a. - 2022 - Accelerating Numerical Solvers for Large-Scale Sim.pdf:application/pdf},
}

@misc{huang_robust_2023,
	title = {On {Robust} {Numerical} {Solver} for {ODE} via {Self}-{Attention} {Mechanism}},
	url = {http://arxiv.org/abs/2302.10184},
	abstract = {With the development of deep learning techniques, AI-enhanced numerical solvers are expected to become a new paradigm for solving differential equations due to their versatility and effectiveness in alleviating the accuracy-speed trade-off in traditional numerical solvers. However, this paradigm still inevitably requires a large amount of high-quality data, whose acquisition is often very expensive in natural science and engineering problems. Therefore, in this paper, we explore training efﬁcient and robust AI-enhanced numerical solvers with a small data size by mitigating intrinsic noise disturbances. We ﬁrst analyze the ability of the self-attention mechanism to regulate noise in supervised learning and then propose a simple-yet-effective numerical solver, AttSolver, which introduces an additive self-attention mechanism to the numerical solution of differential equations based on the dynamical system perspective of the residual neural network. Our results on benchmarks, ranging from high-dimensional problems to chaotic systems, demonstrate the effectiveness of AttSolver in generally improving the performance of existing traditional numerical solvers without any elaborated model crafting. Finally, we analyze the convergence, generalization, and robustness of the proposed method experimentally and theoretically.},
	language = {en},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Huang, Zhongzhan and Liang, Mingfu and Lin, Liang},
	month = feb,
	year = {2023},
	note = {arXiv:2302.10184 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, ODE, Computer Science - Artificial Intelligence, neural networks},
	annote = {Comment: Work in progress. Technical report},
	file = {Huang e.a. - 2023 - On Robust Numerical Solver for ODE via Self-Attent.pdf:C\:\\Users\\isido\\Zotero\\storage\\B6JQY5GS\\Huang e.a. - 2023 - On Robust Numerical Solver for ODE via Self-Attent.pdf:application/pdf},
}

@misc{finzi_stable_2023,
	title = {A {Stable} and {Scalable} {Method} for {Solving} {Initial} {Value} {PDEs} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/2304.14994},
	abstract = {Unlike conventional grid and mesh based methods for solving partial differential equations (PDEs), neural networks have the potential to break the curse of dimensionality, providing approximate solutions to problems where using classical solvers is difﬁcult or impossible. While global minimization of the PDE residual over the network parameters works well for boundary value problems, catastrophic forgetting impairs the applicability of this approach to initial value problems (IVPs). In an alternative local-in-time approach, the optimization problem can be converted into an ordinary differential equation (ODE) on the network parameters and the solution propagated forward in time; however, we demonstrate that current methods based on this approach suffer from two key issues. First, following the ODE produces an uncontrolled growth in the conditioning of the problem, ultimately leading to unacceptably large numerical errors. Second, as the ODE methods scale cubically with the number of model parameters, they are restricted to small neural networks, signiﬁcantly limiting their ability to represent intricate PDE initial conditions and solutions. Building on these insights, we develop Neural IVP, an ODE based IVP solver which prevents the network from getting ill-conditioned and runs in time linear in the number of parameters, enabling us to evolve the dynamics of challenging PDEs with neural networks.},
	language = {en},
	urldate = {2023-07-06},
	publisher = {arXiv},
	author = {Finzi, Marc and Potapczynski, Andres and Choptuik, Matthew and Wilson, Andrew Gordon},
	month = apr,
	year = {2023},
	note = {arXiv:2304.14994 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, PDE, Mathematics - Numerical Analysis, neural networks},
	annote = {Comment: ICLR 2023. Code available at https://github.com/mfinzi/neural-ivp},
	file = {2304.14994.pdf:C\:\\Users\\isido\\Zotero\\storage\\G6V5GEWN\\2304.14994.pdf:application/pdf},
}

@article{johnson_accelerating_nodate-1,
	title = {Accelerating {Stochastic} {Gradient} {Descent} using {Predictive} {Variance} {Reduction}},
	abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is signiﬁcantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
	language = {en},
	author = {Johnson, Rie and Zhang, Tong},
	keywords = {optimization, gradient descent},
	file = {Johnson en Zhang - Accelerating Stochastic Gradient Descent using Pre.pdf:C\:\\Users\\isido\\Zotero\\storage\\V5D4E8K9\\Johnson en Zhang - Accelerating Stochastic Gradient Descent using Pre.pdf:application/pdf},
}

@article{mukhopadhyay_stochastic_2020,
	title = {Stochastic {Gradient} {Descent} {For} {Linear} {Systems} {With} {Sequential} {Matrix} {Entry} {Accumulation}},
	volume = {171},
	doi = {10.1016/j.sigpro.2020.107494},
	abstract = {Conventional stochastic iterative methods are often employed for solving linear systems of equations involving large matrix sizes using low memory footprint. However, their performances are often limited by the unavailability of all the matrix entries, which is often termed as the problem of missing data. Although Ma and Needell [1] have recently proposed a method, termed as mSGD, assuming a model for data missing that results in improved convergence, their result is also affected by constant large variance of the stochastic gradient. In this paper we propose a SGD type method termed as cumulative information SGD (CISGD) for solving a linear system with missing data with an additional provision to accumulate a very small number of matrix entries sequentially per iteration, termed as the sequential matrix entry accumulation (SEMEA) mechanism. CISGD uses the data collected by SEMEA mechanism along with the prior model for data missing mechanism of [1] to gradually reduce variance of the stochastic gradient. The convergence of the proposed CISGD is theoretically analyzed and some interesting implications of the result are investigated under a specific SEMEA mechanism. Finally, numerical experiments are performed along with simulations that corroborate the theoretical findings regarding the efficacy of the proposed CISGD method.},
	journal = {Signal Processing},
	author = {Mukhopadhyay, Samrat},
	month = jun,
	year = {2020},
	keywords = {linear systems, gradient descent},
	pages = {107494},
}

@misc{ma_stochastic_2019,
	title = {Stochastic {Gradient} {Descent} for {Linear} {Systems} with {Missing} {Data}},
	url = {http://arxiv.org/abs/1702.07098},
	abstract = {Traditional methods for solving linear systems have quickly become impractical due to an increase in the size of available data. Utilizing massive amounts of data is further complicated when the data is incomplete or has missing entries. In this work, we address the obstacles presented when working with large data and incomplete data simultaneously. In particular, we propose to adapt the Stochastic Gradient Descent method to address missing data in linear systems. Our proposed algorithm, the Stochastic Gradient Descent for Missing Data method (mSGD), is introduced and theoretical convergence guarantees are provided. In addition, we include numerical experiments on simulated and real world data that demonstrate the usefulness of our method.},
	language = {en},
	urldate = {2023-07-05},
	publisher = {arXiv},
	author = {Ma, Anna and Needell, Deanna},
	month = jan,
	year = {2019},
	note = {arXiv:1702.07098 [math]},
	keywords = {linear systems, Mathematics - Numerical Analysis, gradient descent},
	file = {1702.07098.pdf:C\:\\Users\\isido\\Zotero\\storage\\BE762FG6\\1702.07098.pdf:application/pdf},
}

@misc{shlomi_steinberg_towards_2023,
	title = {Towards {Practical} {Physical}-{Optics} {Rendering} — {Presentation}},
	url = {https://www.youtube.com/watch?v=4Z3ohq0ZszI},
	abstract = {Physical light transport (PLT) algorithms can represent the wave nature of light globally in a scene, and are consistent with Maxwell’s theory of electromagnetism. As such, they are able to reproduce the wave-interference and diffraction effects of real physical optics. However, the recent works that have proposed PLT are too expensive to apply to real-world scenes with complex geometry and materials. To address this problem, we propose a novel framework for physical light transport based on several key ideas that actually makes PLT practical for complex scenes. First, we restrict the spatial coherence shape of light to an anisotropic Gaussian and justify this restriction with general arguments based on entropy. This restriction serves to simplify the rest of the derivations, without practical loss of generality. To describe partially-coherent light, we present new rendering primitives that generalize the radiometric radiance and irradiance, and are based on the well-known Stokes parameters. We are able to represent light of arbitrary spectral content and states of polarization, and with any coherence volume and anisotropy. We also present the wave BSDF to accurately render diffractions and wave-interference effects. Furthermore, we present an approach to importance sample this wave BSDF to facilitate bi-directional path tracing, which has been previously impossible. We show good agreement with state-of-the-art methods, but unlike them we are able to render complex scenes where all the materials are new, coherence-aware physical optics materials, and with performance approaching that of “classical” rendering methods.},
	urldate = {2023-07-15},
	author = {{Shlomi Steinberg}},
	month = jul,
	year = {2023},
	keywords = {rendering},
}

@article{steinberg_generalized_nodate,
	title = {A {Generalized} {Ray} {Formulation} {For} {Wave}-{Optics} {Rendering}},
	abstract = {In this paper we present the generalized ray: an extension of the classical ray to wave optics. The generalized ray retains the defining characteristics of the ray-optical ray: locality and linearity. These properties allow the generalized ray to serve as a “point query” of light’s behaviour—the same purpose that the classical ray fulfils in rendering. By using such generalized rays, we enable the rendering of complex scenes, like the one shown, under rigorous wave-optical light transport. Materials admitting diffractive optical phenomena are visible: (a) a Bornite ore with a layer of copper oxide causing interference; (b) a Brazilian Rainbow Boa, whose scales are biological diffraction grated surfaces; and (c) a Chrysomelidae beetle, whose colour arises due to naturally-occurring multilayered interference reflectors in its elytron. Our formalism serves as a link between path tracing techniques and wave optics, and admits a highly general validity domain. Therefore, we are able to apply sophisticated sampling techniques, and achieve performance that surpasses the state-of-the-art by orders-of-magnitude. We indicate resolution and samples-per-pixel (spp) count in all figures rendered using our method. While these figures showcase converged (high spp) results, our implementation also allows interactive rendering of all these scenes at 1 spp. Frame times (at 1 spp) for interactive rendering are indicated. Implementation, as well as additional renderings and videos are available in our supplemental material.},
	language = {en},
	author = {Steinberg, Shlomi and Ramamoorthi, Ravi and Bitterli, Benedikt and D’Eon, Eugene and Yan, Ling-Qi and Pharr, Matt},
	keywords = {rendering},
	file = {Steinberg e.a. - A Generalized Ray Formulation For Wave-Optics Rend.pdf:C\:\\Users\\isido\\Zotero\\storage\\CV99AWZL\\Steinberg e.a. - A Generalized Ray Formulation For Wave-Optics Rend.pdf:application/pdf},
}

@misc{beznea_monte_2022-1,
	title = {From {Monte} {Carlo} to neural networks approximations of boundary value problems},
	url = {http://arxiv.org/abs/2209.01432},
	abstract = {In this paper we study probabilistic and neural network approximations for solutions to Poisson equation subject to H¨older or C2 data in general bounded domains of Rd. We aim at two fundamental goals.},
	language = {en},
	urldate = {2023-07-17},
	publisher = {arXiv},
	author = {Beznea, Lucian and Cimpean, Iulian and Lupascu-Stamate, Oana and Popescu, Ionel and Zarnescu, Arghir},
	month = sep,
	year = {2022},
	note = {arXiv:2209.01432 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Mathematics - Numerical Analysis, Computer Science - Artificial Intelligence, Mathematics - Analysis of PDEs, neural networks, boundary value problems},
	file = {Beznea e.a. - 2022 - From Monte Carlo to neural networks approximations.pdf:C\:\\Users\\isido\\Zotero\\storage\\NUHRK4MF\\Beznea e.a. - 2022 - From Monte Carlo to neural networks approximations.pdf:application/pdf},
}

@article{daun_complexity_2017,
	title = {Complexity of parametric initial value problems for systems of {ODEs}},
	volume = {135},
	issn = {03784754},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378475415000713},
	doi = {10.1016/j.matcom.2015.04.008},
	abstract = {We study the approximate solution of initial value problems for parameter dependent ﬁnite or inﬁnite systems of scalar ordinary diﬀerential equations (ODEs). Both the deterministic and the randomized setting is considered, with input data from various smoothness classes. We study deterministic and Monte Carlo multilevel algorithms and derive convergence rates. Moreover, we prove their optimality by showing matching (in some limit cases up to logarithmic factors) lower bounds and settle this way the complexity. Comparisons between the deterministic and randomized setting are given, as well.},
	language = {en},
	urldate = {2023-07-17},
	journal = {Mathematics and Computers in Simulation},
	author = {Daun, Thomas and Heinrich, Stefan},
	month = may,
	year = {2017},
	keywords = {ODE, information complexity},
	pages = {72--85},
	file = {Daun en Heinrich - 2017 - Complexity of parametric initial value problems fo.pdf:C\:\\Users\\isido\\Zotero\\storage\\BJIUZ98W\\Daun en Heinrich - 2017 - Complexity of parametric initial value problems fo.pdf:application/pdf},
}

@article{gocwin_randomized_2014-1,
	title = {Randomized and quantum complexity of nonlinear two-point {BVPs}},
	volume = {245},
	issn = {0096-3003},
	url = {https://www.sciencedirect.com/science/article/pii/S009630031401073X},
	doi = {10.1016/j.amc.2014.07.106},
	abstract = {We deal with the complexity of nonlinear BVPs with nonlinear two-point boundary conditions. We consider the randomized and quantum models of computation. We assume that the right-hand side function is r times differentiable with all derivatives bounded by a constant. We show that the ε-complexity is roughly of order ε-1/(r+1/2) in the randomized setting, and ε-1/(r+1) in the quantum setting. We compare our results with known results in the deterministic setting. The speed-up of the randomized computations with respect to the deterministic computations is by 1/(r(2r+1)) in the exponent of 1/ε, and the speed-up of the quantum computations by 1/(r(r+1)) in the exponent.},
	language = {en},
	urldate = {2023-07-17},
	journal = {Applied Mathematics and Computation},
	author = {Goćwin, Maciej},
	month = oct,
	year = {2014},
	keywords = {ODE, Complexity, Optimal algorithms, Quantum computing, Randomized computing, information complexity, boundary value problems},
	pages = {357--371},
	file = {ScienceDirect Snapshot:C\:\\Users\\isido\\Zotero\\storage\\CRRV662Q\\S009630031401073X.html:text/html},
}

@article{gocwin_complexity_2010,
	title = {On the complexity of a two-point boundary value problem in different settings},
	volume = {87},
	issn = {0020-7160, 1029-0265},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00207160903401852},
	doi = {10.1080/00207160903401852},
	abstract = {We study the complexity of a two-point boundary value problem. We concentrate on the linear problem of order k with separated boundary conditions. Right-hand side functions are assumed to be r times differentiable with all derivatives bounded by a constant. We consider three models of computation: deterministic with standard and linear information, randomized and quantum. In each setting, we construct an algorithm for solving the problem, which allows us to establish upper complexity bounds. In the deterministic setting, we show that the use of linear information gives us a speed-up of at least one order of magnitude compared with the standard information. For randomized algorithms, we show that the speed-up over standard deterministic algorithms is by 1/2 in the exponent. For quantum algorithms, we can achieve a speed-up by one order of magnitude. We also provide lower complexity bounds. They match upper bounds in the deterministic setting with the standard information, and almost match upper bounds in the randomized and quantum settings. In the deterministic setting with the linear information, a gap still remains between the upper and lower complexity bounds.},
	language = {en},
	number = {15},
	urldate = {2023-07-17},
	journal = {International Journal of Computer Mathematics},
	author = {Goćwin, Maciej and Szczęsny, Marek},
	month = dec,
	year = {2010},
	keywords = {information complexity, boundary value problems},
	pages = {3370--3386},
	annote = {[TLDR] This work studies the complexity of a two-point boundary value problem, and shows that the use of linear information gives us a speed-up of at least one order of magnitude compared with the standard information.},
}

@misc{herman_survey_2022,
	title = {A {Survey} of {Quantum} {Computing} for {Finance}},
	url = {http://arxiv.org/abs/2201.02773},
	abstract = {Quantum computers are expected to surpass the computational capabilities of classical computers during this decade and have transformative impact on numerous industry sectors, particularly ﬁnance. In fact, ﬁnance is estimated to be the ﬁrst industry sector to beneﬁt from quantum computing, not only in the medium and long terms, but even in the short term. This survey paper presents a comprehensive summary of the state of the art of quantum computing for ﬁnancial applications, with particular emphasis on stochastic modeling, optimization, and machine learning, describing how these solutions, adapted to work on a quantum computer, can potentially help to solve ﬁnancial problems, such as derivative pricing, risk modeling, portfolio optimization, natural language processing, and fraud detection, more eﬃciently and accurately. We also discuss the feasibility of these algorithms on nearterm quantum computers with various hardware implementations and demonstrate how they relate to a wide range of use cases in ﬁnance. We hope this article will not only serve as a reference for academic researchers and industry practitioners but also inspire new ideas for future research.},
	language = {en},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Herman, Dylan and Googin, Cody and Liu, Xiaoyuan and Galda, Alexey and Safro, Ilya and Sun, Yue and Pistoia, Marco and Alexeev, Yuri},
	month = jun,
	year = {2022},
	note = {arXiv:2201.02773 [quant-ph, q-fin]},
	keywords = {Quantitative Finance - Computational Finance, Quantum Physics, Quantum computing},
	annote = {Comment: 60 pages, 5 figures},
	file = {2201.02773.pdf:C\:\\Users\\isido\\Zotero\\storage\\JM9ZHVD5\\2201.02773.pdf:application/pdf},
}

@misc{pistoia_quantum_2021,
	title = {Quantum {Machine} {Learning} for {Finance}},
	url = {http://arxiv.org/abs/2109.04298},
	abstract = {Quantum computers are expected to surpass the computational capabilities of classical computers during this decade, and achieve disruptive impact on numerous industry sectors, particularly ﬁnance. In fact, ﬁnance is estimated to be the ﬁrst industry sector to beneﬁt from Quantum Computing not only in the medium and long terms, but even in the short term. This review paper presents the state of the art of quantum algorithms for ﬁnancial applications, with particular focus to those use cases that can be solved via Machine Learning.},
	language = {en},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Pistoia, Marco and Ahmad, Syed Farhan and Ajagekar, Akshay and Buts, Alexander and Chakrabarti, Shouvanik and Herman, Dylan and Hu, Shaohan and Jena, Andrew and Minssen, Pierre and Niroula, Pradeep and Rattew, Arthur and Sun, Yue and Yalovetzky, Romina},
	month = sep,
	year = {2021},
	note = {arXiv:2109.04298 [quant-ph]},
	keywords = {Computer Science - Machine Learning, Quantum Physics, machine learning, Quantum computing},
	annote = {not detailed, dont like how they present the ideas
},
	file = {Pistoia e.a. - 2021 - Quantum Machine Learning for Finance.pdf:C\:\\Users\\isido\\Zotero\\storage\\RUN9KCYU\\Pistoia e.a. - 2021 - Quantum Machine Learning for Finance.pdf:application/pdf},
}

@article{orus_quantum_2019,
	title = {Quantum computing for finance: {Overview} and prospects},
	volume = {4},
	issn = {24054283},
	shorttitle = {Quantum computing for finance},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2405428318300571},
	doi = {10.1016/j.revip.2019.100028},
	abstract = {We discuss how quantum computation can be applied to financial problems, providing an overview of current approaches and potential prospects. We review quantum optimization algorithms, and expose how quantum annealers can be used to optimize portfolios, find arbitrage opportunities, and perform credit scoring. We also discuss deep-learning in finance, and suggestions to improve these methods through quantum machine learning. Finally, we consider quantum amplitude estimation, and how it can result in a quantum speed-up for Monte Carlo sampling. This has direct applications to many current financial methods, including pricing of derivatives and risk analysis. Perspectives are also discussed.},
	language = {en},
	urldate = {2023-07-20},
	journal = {Reviews in Physics},
	author = {Orús, Román and Mugel, Samuel and Lizaso, Enrique},
	month = nov,
	year = {2019},
	keywords = {finance, Quantum computing},
	pages = {100028},
	file = {Orús e.a. - 2019 - Quantum computing for finance Overview and prospe.pdf:C\:\\Users\\isido\\Zotero\\storage\\7E3B7Z3C\\Orús e.a. - 2019 - Quantum computing for finance Overview and prospe.pdf:application/pdf},
}

@misc{ryan_t_lecture_2017,
	title = {Lecture 01: {Linear} regression},
	shorttitle = {Lecture 01},
	url = {https://www.youtube.com/watch?v=Z1cSby8ZzhA},
	urldate = {2023-07-22},
	author = {{Ryan T}},
	month = jan,
	year = {2017},
	annote = {3:40 
combining computational complexity and statistical complexity is an open problem
how hard is something statistically constraint to computational complexity
},
}

@misc{cynthia_rudin_pacmap_2021,
	title = {{PaCMAP}: {An} algorithm for dimension reduction},
	shorttitle = {{PaCMAP}},
	url = {https://www.youtube.com/watch?v=sD-uDZ8zXkc},
	abstract = {This video describes the PaCMAP technique for dimension reduction, which is an alternative to t-SNE and UMAP. It was derived based on an understanding of what makes dimension algorithms work.
Paper:
Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMAP, and PaCMAP for Data Visualization},
	urldate = {2023-07-23},
	author = {{Cynthia Rudin}},
	month = aug,
	year = {2021},
}

@misc{wang_understanding_2021,
	title = {Understanding {How} {Dimension} {Reduction} {Tools} {Work}: {An} {Empirical} {Approach} to {Deciphering} t-{SNE}, {UMAP}, {TriMAP}, and {PaCMAP} for {Data} {Visualization}},
	shorttitle = {Understanding {How} {Dimension} {Reduction} {Tools} {Work}},
	url = {http://arxiv.org/abs/2012.04456},
	abstract = {Dimension reduction (DR) techniques such as t-SNE, UMAP, and TriMAP have demonstrated impressive visualization performance on many real world datasets. One tension that has always faced these methods is the trade-off between preservation of global structure and preservation of local structure: these methods can either handle one or the other, but not both. In this work, our main goal is to understand what aspects of DR methods are important for preserving both local and global structure: it is difficult to design a better method without a true understanding of the choices we make in our algorithms and their empirical impact on the lower-dimensional embeddings they produce. Towards the goal of local structure preservation, we provide several useful design principles for DR loss functions based on our new understanding of the mechanisms behind successful DR methods. Towards the goal of global structure preservation, our analysis illuminates that the choice of which components to preserve is important. We leverage these insights to design a new algorithm for DR, called Pairwise Controlled Manifold Approximation Projection (PaCMAP), which preserves both local and global structure. Our work provides several unexpected insights into what design choices both to make and avoid when constructing DR algorithms.},
	language = {en},
	urldate = {2023-07-23},
	publisher = {arXiv},
	author = {Wang, Yingfan and Huang, Haiyang and Rudin, Cynthia and Shaposhnik, Yaron},
	month = aug,
	year = {2021},
	note = {arXiv:2012.04456 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, machine learning},
	file = {2012.04456.pdf:C\:\\Users\\isido\\Zotero\\storage\\TW5ZFTXU\\2012.04456.pdf:application/pdf},
}

@misc{pydata_leland_2023,
	title = {Leland {McInnes} - {Data} {Mapping} for {Data} {Exploration} {\textbar} {PyData} {Seattle} 2023},
	url = {https://www.youtube.com/watch?v=r8dWZX8IGw8},
	abstract = {As embeddings and and vector databases become ever more popular we need to develop new tools for exploratory data analysis. One such approach is interactive data maps -- using 2D map style representations of the data, combined with rich interactivity that can link back to the source data. We'll look at the open source tools available for building interactive data maps, and work through an example use case.},
	urldate = {2023-07-23},
	author = {{PyData}},
	month = jun,
	year = {2023},
}

@misc{noauthor_tutte_nodate,
	title = {Tutte {Institute} for {Mathematics} and {Computing}},
	url = {https://github.com/TutteInstitute},
	abstract = {Tutte Institute for Mathematics and Computing. Tutte Institute for Mathematics and Computing has 8 repositories available. Follow their code on GitHub.},
	language = {en},
	urldate = {2023-07-23},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\XQFYKYGV\\tutteinstitute.html:text/html},
}

@misc{xin_exploring_2022,
	title = {Exploring the {Whole} {Rashomon} {Set} of {Sparse} {Decision} {Trees}},
	url = {http://arxiv.org/abs/2209.08040},
	abstract = {In any given machine learning problem, there might be many models that explain the data almost equally well. However, most learning algorithms return only one of these models, leaving practitioners with no practical way to explore alternative models that might have desirable properties beyond what could be expressed by a loss function. The Rashomon set is the set of these all almost-optimal models. Rashomon sets can be large in size and complicated in structure, particularly for highly nonlinear function classes that allow complex interaction terms, such as decision trees. We provide the ﬁrst technique for completely enumerating the Rashomon set for sparse decision trees; in fact, our work provides the ﬁrst complete enumeration of any Rashomon set for a non-trivial problem with a highly nonlinear discrete function class. This allows the user an unprecedented level of control over model choice among all models that are approximately equally good. We represent the Rashomon set in a specialized data structure that supports efﬁcient querying and sampling. We show three applications of the Rashomon set: 1) it can be used to study variable importance for the set of almost-optimal trees (as opposed to a single tree), 2) the Rashomon set for accuracy enables enumeration of the Rashomon sets for balanced accuracy and F1-score, and 3) the Rashomon set for a full dataset can be used to produce Rashomon sets constructed with only subsets of the data set. Thus, we are able to examine Rashomon sets across problems with a new lens, enabling users to choose models rather than be at the mercy of an algorithm that produces only a single model.},
	language = {en},
	urldate = {2023-07-23},
	publisher = {arXiv},
	author = {Xin, Rui and Zhong, Chudi and Chen, Zhi and Takagi, Takuya and Seltzer, Margo and Rudin, Cynthia},
	month = oct,
	year = {2022},
	note = {arXiv:2209.08040 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, machine learning, interpretable machine learning},
	annote = {Comment: NeurIPS 2022 (Oral)},
	file = {Xin e.a. - 2022 - Exploring the Whole Rashomon Set of Sparse Decisio.pdf:C\:\\Users\\isido\\Zotero\\storage\\D6ZQM9GH\\Xin e.a. - 2022 - Exploring the Whole Rashomon Set of Sparse Decisio.pdf:application/pdf},
}

@inproceedings{wang_timbertrek_2022,
	title = {{TimberTrek}: {Exploring} and {Curating} {Sparse} {Decision} {Trees} with {Interactive} {Visualization}},
	shorttitle = {{TimberTrek}},
	url = {http://arxiv.org/abs/2209.09227},
	doi = {10.1109/VIS54862.2022.00021},
	abstract = {Given thousands of equally accurate machine learning (ML) models, how can users choose among them? A recent ML technique enables domain experts and data scientists to generate a complete Rashomon set for sparse decision trees--a huge set of almost-optimal interpretable ML models. To help ML practitioners identify models with desirable properties from this Rashomon set, we develop TimberTrek, the first interactive visualization system that summarizes thousands of sparse decision trees at scale. Two usage scenarios highlight how TimberTrek can empower users to easily explore, compare, and curate models that align with their domain knowledge and values. Our open-source tool runs directly in users' computational notebooks and web browsers, lowering the barrier to creating more responsible ML models. TimberTrek is available at the following public demo link: https://poloclub.github.io/timbertrek.},
	language = {en},
	urldate = {2023-07-23},
	booktitle = {2022 {IEEE} {Visualization} and {Visual} {Analytics} ({VIS})},
	author = {Wang, Zijie J. and Zhong, Chudi and Xin, Rui and Takagi, Takuya and Chen, Zhi and Chau, Duen Horng and Rudin, Cynthia and Seltzer, Margo},
	month = oct,
	year = {2022},
	note = {arXiv:2209.09227 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, machine learning, Computer Science - Human-Computer Interaction, interpretable machine learning},
	pages = {60--64},
	annote = {Comment: Accepted at IEEE VIS 2022. 5 pages, 6 figures. For a demo video, see https://youtu.be/3eGqTmsStJM. For a live demo, visit https://poloclub.github.io/timbertrek},
	file = {Wang e.a. - 2022 - TimberTrek Exploring and Curating Sparse Decision.pdf:C\:\\Users\\isido\\Zotero\\storage\\8A6FQSQ2\\Wang e.a. - 2022 - TimberTrek Exploring and Curating Sparse Decision.pdf:application/pdf},
}

@misc{cynthia_rudin_nobel_2021,
	title = {Nobel {Conference} - {Rudin}},
	url = {https://www.youtube.com/watch?v=Eokq35hm3mM},
	abstract = {Cynthia Rudin's lecture for the 2021 Nobel Conference, on the topic of interpretable machine learning.},
	urldate = {2023-07-23},
	author = {{Cynthia Rudin}},
	month = oct,
	year = {2021},
	keywords = {machine learning, interpretable machine learning},
}

@misc{bonev_spherical_2023,
	title = {Spherical {Fourier} {Neural} {Operators}: {Learning} {Stable} {Dynamics} on the {Sphere}},
	shorttitle = {Spherical {Fourier} {Neural} {Operators}},
	url = {http://arxiv.org/abs/2306.03838},
	abstract = {Fourier Neural Operators (FNOs) have proven to be an efficient and effective method for resolutionindependent operator learning in a broad variety of application areas across scientific machine learning. A key reason for their success is their ability to accurately model long-range dependencies in spatio-temporal data by learning global convolutions in a computationally efficient manner. To this end, FNOs rely on the discrete Fourier transform (DFT), however, DFTs cause visual and spectral artifacts as well as pronounced dissipation when learning operators in spherical coordinates since they incorrectly assume a flat geometry. To overcome this limitation, we generalize FNOs on the sphere, introducing Spherical FNOs (SFNOs) for learning operators on spherical geometries. We apply SFNOs to forecasting atmospheric dynamics, and demonstrate stable autoregressive rollouts for a year of simulated time (1,460 steps), while retaining physically plausible dynamics. The SFNO has important implications for machine learning-based simulation of climate dynamics that could eventually help accelerate our response to climate change.},
	language = {en},
	urldate = {2023-07-25},
	publisher = {arXiv},
	author = {Bonev, Boris and Kurth, Thorsten and Hundt, Christian and Pathak, Jaideep and Baust, Maximilian and Kashinath, Karthik and Anandkumar, Anima},
	month = jun,
	year = {2023},
	note = {arXiv:2306.03838 [physics]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Physics - Computational Physics, machine learning, Physics - Atmospheric and Oceanic Physics},
	file = {Bonev e.a. - 2023 - Spherical Fourier Neural Operators Learning Stabl.pdf:C\:\\Users\\isido\\Zotero\\storage\\4GD8MR5N\\Bonev e.a. - 2023 - Spherical Fourier Neural Operators Learning Stabl.pdf:application/pdf},
}

@misc{kalinin_analog_2023,
	title = {Analog {Iterative} {Machine} ({AIM}): using light to solve quadratic optimization problems with mixed variables},
	shorttitle = {Analog {Iterative} {Machine} ({AIM})},
	url = {http://arxiv.org/abs/2304.12594},
	abstract = {Solving optimization problems is challenging for existing digital computers and even for future quantum hardware. The practical importance of diverse problems, from healthcare to financial optimization, has driven the emergence of specialised hardware over the past decade. However, their support for problems with only binary variables severely restricts the scope of practical problems that can be efficiently embedded. We build analog iterative machine (AIM), the first instance of an opto-electronic solver that natively implements a wider class of quadratic unconstrained mixed optimization (QUMO) problems and supports all-to-all connectivity of both continuous and binary variables.Beyond synthetic 7-bit problems at small-scale, AIM solves the financial transaction settlement problem entirely in analog domain with higher accuracy than quantum hardware and at room temperature. With compute-in-memory operation and spatial-division multiplexed representation of variables, the design of AIM paves the path to chip-scale architecture with 100 times speed-up per unit-power over the latest GPUs for solving problems with 10,000 variables. The robustness of the AIM algorithm at such scale is further demonstrated by comparing it with commercial production solvers across multiple benchmarks, where for several problems we report new best solutions. By combining the superior QUMO abstraction, sophisticated gradient descent methods inspired by machine learning, and commodity hardware, AIM introduces a novel platform with a step change in expressiveness, performance, and scalability, for optimization in the post-Moores law era.},
	language = {en},
	urldate = {2023-07-27},
	publisher = {arXiv},
	author = {Kalinin, Kirill P. and Mourgias-Alexandris, George and Ballani, Hitesh and Berloff, Natalia G. and Clegg, James H. and Cletheroe, Daniel and Gkantsidis, Christos and Haller, Istvan and Lyutsarev, Vassily and Parmigiani, Francesca and Pickup, Lucinda and Rowstron, Antony},
	month = jun,
	year = {2023},
	note = {arXiv:2304.12594 [physics]},
	keywords = {Mathematics - Optimization and Control, optimization, analog, Computer Science - Emerging Technologies, Physics - Applied Physics},
	annote = {Comment: Main sections plus supplementa material for a total of 41 pages. 7 figures},
	file = {Kalinin e.a. - 2023 - Analog Iterative Machine (AIM) using light to sol.pdf:C\:\\Users\\isido\\Zotero\\storage\\LIAP6PZ5\\Kalinin e.a. - 2023 - Analog Iterative Machine (AIM) using light to sol.pdf:application/pdf},
}

@book{foucart_mathematical_2022,
	edition = {1},
	title = {Mathematical {Pictures} at a {Data} {Science} {Exhibition}},
	isbn = {978-1-00-900393-3 978-1-316-51888-5 978-1-00-900185-4},
	url = {https://www.cambridge.org/core/product/identifier/9781009003933/type/book},
	abstract = {This text provides deep and comprehensive coverage of the mathematical background for data science, including machine learning, optimal recovery, compressed sensing, optimization, and neural networks. In the past few decades, heuristic methods adopted by big tech companies have complemented existing scientific disciplines to form the new field of Data Science. This text embarks the readers on an engaging itinerary through the theory supporting the field. Altogether, twenty-seven lecture-length chapters with exercises provide all the details necessary for a solid understanding of key topics in data science. While the book covers standard material on machine learning and optimization, it also includes distinctive presentations of topics such as reproducing kernel Hilbert spaces, spectral clustering, optimal recovery, compressed sensing, group testing, and applications of semidefinite programming. Students and data scientists with less mathematical background will appreciate the appendices that provide more background on some of the more abstract concepts.},
	language = {en},
	urldate = {2023-07-29},
	publisher = {Cambridge University Press},
	author = {Foucart, Simon},
	month = may,
	year = {2022},
	doi = {10.1017/9781009003933},
	keywords = {optimization, machine learning, information complexity},
	file = {Foucart - 2022 - Mathematical Pictures at a Data Science Exhibition.pdf:C\:\\Users\\isido\\Zotero\\storage\\IV37PZ8W\\Foucart - 2022 - Mathematical Pictures at a Data Science Exhibition.pdf:application/pdf},
}

@article{bakbouk_mean_2023,
	title = {Mean {Value} {Caching} for {Walk} on {Spheres}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://diglib.eg.org/handle/10.2312/sr20231120},
	doi = {10.2312/SR.20231120},
	abstract = {Walk on Spheres (WoS) is a grid-free Monte Carlo method for numerically estimating solutions for elliptical partial differential equations (PDE) such as the Laplace and Poisson PDEs. While WoS is efficient for computing a solution value at a single evaluation point, it becomes less efficient when the solution is required over a whole domain or a region of interest. WoS computes a solution for each evaluation point separately, possibly recomputing similar sub-walks multiple times over multiple evaluation points. In this paper, we introduce a novel filtering and caching strategy that leverages the volume mean value property (in contrast to the boundary mean value property that forms the core of WoS). In addition, to improve quality under sparse cache regimes, we describe a weighted mean as well as a non-uniform sampling method. Finally, we show that we can reduce the variance within the cache by recursively applying the volume mean value property on the cached elements.},
	language = {en},
	urldate = {2023-08-01},
	author = {Bakbouk, Ghada and Peers, Pieter},
	year = {2023},
	note = {Artwork Size: 10 pages
Publisher: The Eurographics Association},
	keywords = {walk on spheres, Computing methodologies, Shape analysis},
	pages = {10 pages},
	annote = {Other
Ray TracingOther
Ghada Bakbouk and Pieter Peers},
	file = {Bakbouk en Peers - 2023 - Mean Value Caching for Walk on Spheres.pdf:C\:\\Users\\isido\\Zotero\\storage\\VT2WT79A\\Bakbouk en Peers - 2023 - Mean Value Caching for Walk on Spheres.pdf:application/pdf},
}

@misc{computerphile_quantum_2023,
	title = {Quantum {Computing} in {Reality} ({Pt3}: {Beyond} the {Hype}) - {Computerphile}},
	shorttitle = {Quantum {Computing} in {Reality} ({Pt3}},
	url = {https://www.youtube.com/watch?v=gH_rF9LLzFA},
	abstract = {What's actually possible vs what's theoretically possible vs what's actually useful with quantum computing? Victor V. Albert of University of Maryland and NIST simplifies!},
	urldate = {2023-08-01},
	author = {{Computerphile}},
	month = jul,
	year = {2023},
	keywords = {Quantum computing},
}

@misc{noauthor_bsc_mathematics_2017_hollander_rmpdf_nodate,
	title = {{BSc}\_Mathematics\_2017\_Hollander\_RM.pdf},
	url = {https://fse.studenttheses.ub.rug.nl/14905/1/BSc_Mathematics_2017_Hollander_RM.pdf},
	abstract = {For a homogeneous system of linear differential equations with a constant coefficient matrix, the fundamental matrix can be computed for example using the Jordan Canonical Form. However, when the coefficient matrix depends on a single variable t, this method does not always provide a correct solution. The fundamental matrix can be computed using a numerical method, for example the Picard iterative method, but using such method, one can lose important qualitative properties. Wilhelm Magnus provided a method to approximate the fundamental matrix, such that these qualitative properties are preserved. In this thesis, we will state Magnus’ theorem and it’s proof. We will compute the Magnus Expansion for some simple examples, and compare the solutions with the fundamental matrices obtained by applying Picard iteration.},
	urldate = {2023-06-12},
	keywords = {ODE},
	file = {BSc_Mathematics_2017_Hollander_RM.pdf:C\:\\Users\\isido\\Zotero\\storage\\Y5W4DSKX\\BSc_Mathematics_2017_Hollander_RM.pdf:application/pdf},
}

@article{bochacik_randomized_2021,
	title = {Randomized {Runge}-{Kutta} method -- stability and convergence under inexact information},
	volume = {65},
	issn = {0885064X},
	url = {http://arxiv.org/abs/2006.12131},
	doi = {10.1016/j.jco.2021.101554},
	abstract = {We deal with optimal approximation of solutions of ODEs under local Lipschitz condition and inexact discrete information about the right-hand side functions. We show that the randomized two-stage Runge-Kutta scheme is the optimal method among all randomized algorithms based on standard noisy information. We perform numerical experiments that conﬁrm our theoretical ﬁndings. Moreover, for the optimal algorithm we rigorously investigate properties of regions of absolute stability.},
	language = {en},
	urldate = {2023-08-02},
	journal = {Journal of Complexity},
	author = {Bochacik, Tomasz and Goćwin, Maciej and Morkisz, Paweł M. and Przybyłowicz, Paweł},
	month = aug,
	year = {2021},
	note = {arXiv:2006.12131 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, ODE, information complexity},
	pages = {101554},
	file = {Bochacik e.a. - 2021 - Randomized Runge-Kutta method -- stability and con.pdf:C\:\\Users\\isido\\Zotero\\storage\\NYK5DYWB\\Bochacik e.a. - 2021 - Randomized Runge-Kutta method -- stability and con.pdf:application/pdf},
}

@article{eisenmann_randomized_2019,
	title = {On a {Randomized} {Backward} {Euler} {Method} for {Nonlinear} {Evolution} {Equations} with {Time}-{Irregular} {Coefficients}},
	volume = {19},
	issn = {1615-3375, 1615-3383},
	url = {http://link.springer.com/10.1007/s10208-018-09412-w},
	doi = {10.1007/s10208-018-09412-w},
	abstract = {In this paper, we introduce a randomized version of the backward Euler method that is applicable to stiff ordinary differential equations and nonlinear evolution equations with time-irregular coefﬁcients. In the ﬁnite-dimensional case, we consider Carathéodory-type functions satisfying a one-sided Lipschitz condition. After investigating the well-posedness and the stability properties of the randomized scheme, we prove the convergence to the exact solution with a rate of 0.5 in the root-mean-square norm assuming only that the coefﬁcient function is square integrable with respect to the temporal parameter. These results are then extended to the approximation of inﬁnitedimensional evolution equations under monotonicity and Lipschitz conditions. Here, we consider a combination of the randomized backward Euler scheme with a Galerkin ﬁnite element method. We obtain error estimates that correspond to the regularity of the exact solution. The practicability of the randomized scheme is also illustrated through several numerical experiments.},
	language = {en},
	number = {6},
	urldate = {2023-08-02},
	journal = {Foundations of Computational Mathematics},
	author = {Eisenmann, Monika and Kovács, Mihály and Kruse, Raphael and Larsson, Stig},
	month = dec,
	year = {2019},
	keywords = {ODE, information complexity},
	pages = {1387--1430},
	file = {Eisenmann e.a. - 2019 - On a Randomized Backward Euler Method for Nonlinea.pdf:C\:\\Users\\isido\\Zotero\\storage\\NBPU49QC\\Eisenmann e.a. - 2019 - On a Randomized Backward Euler Method for Nonlinea.pdf:application/pdf},
}

@incollection{hutchison_complexity_2007,
	address = {Berlin, Heidelberg},
	title = {Complexity of {Monte} {Carlo} {Algorithms} for a {Class} of {Integral} {Equations}},
	volume = {4487},
	isbn = {978-3-540-72583-1 978-3-540-72584-8},
	url = {http://link.springer.com/10.1007/978-3-540-72584-8_97},
	abstract = {In this work we study the computational complexity of a class of grid Monte Carlo algorithms for integral equations. The idea of the algorithms consists in an approximation of the integral equation by a system of algebraic equations. Then the Markov chain iterative Monte Carlo is used to solve the system. The assumption here is that the corresponding Neumann series for the iterative matrix does not necessarily converge or converges slowly. We use a special technique to accelerate the convergence. An estimate of the computational complexity of Monte Carlo algorithm using the considered approach is obtained. The estimate of the complexity is compared with the corresponding quantity for the complexity of the grid-free Monte Carlo algorithm. The conditions under which the class of grid Monte Carlo algorithms is more efﬁcient are given.},
	language = {en},
	urldate = {2023-08-06},
	booktitle = {Computational {Science} – {ICCS} 2007},
	publisher = {Springer Berlin Heidelberg},
	author = {Dimov, Ivan and Georgieva, Rayna},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Rangan, C. Pandu and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Shi, Yong and Van Albada, Geert Dick and Dongarra, Jack and Sloot, Peter M. A.},
	year = {2007},
	doi = {10.1007/978-3-540-72584-8_97},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {monte carlo, walk on spheres, integral equations},
	pages = {731--738},
	file = {Dimov en Georgieva - 2007 - Complexity of Monte Carlo Algorithms for a Class o.pdf:C\:\\Users\\isido\\Zotero\\storage\\IQMEVCF4\\Dimov en Georgieva - 2007 - Complexity of Monte Carlo Algorithms for a Class o.pdf:application/pdf},
}

@misc{qiskit_understanding_2023,
	title = {Understanding {Quantum} {Information} and {Computation} {\textbar} {Lesson} 05 {\textbar} {Quantum} {Query} {Algorithms}},
	url = {https://www.youtube.com/watch?v=2wticzHE1vs},
	abstract = {Lesson 5 is the first lesson of the second unit of the series, which is on the fundamentals of quantum algorithms. This lesson is on the query model of computation, and describes a progression of quantum algorithms that offer advantages over classical algorithms within this model. The quantum algorithms include Deutsch’s algorithm, the Deutsch-Josza algorithm, and Simon’s algorithm.

0:00 — Introduction
2:13 — Overview
3:45 — A standard picture of computation
5:19 — The query model of computation
8:19 — Examples of query problems
12:49 — Query gates
22:08 — Deutsch’s algorithm
22:50 — Deutsch’s problem
24:46 — Deutsch’s algorithm
31:08 — Phase kickback
34:06 — The Deutsch-Jozsa circuit
35:40 — The Deutsch-Jozsa problem
37:48 — Deutsch-Jozsa analysis
47:30 — The Bernstein-Vazirani problem
51:53 — Simon’s algorithm
52:46 — Simon’s problem
57:46 — Simon’s algorithm
59:25 — Simon’s algorithm analysis
1:09:28 — Classical post-processing
1:15:31 — Classical difficulty
1:18:14 — Conclusion

Link to textbook lesson: https://learn.qiskit.org/course/algor...

Series Overview:    • Understanding Quantum Information and...  

\#ibmquantum \#learnquantum \#qiskit},
	urldate = {2023-08-06},
	author = {{Qiskit}},
	month = jun,
	year = {2023},
	keywords = {Quantum computing, information complexity},
}

@misc{stoudenmire_grovers_2023,
	title = {Grover's {Algorithm} {Offers} {No} {Quantum} {Advantage}},
	url = {http://arxiv.org/abs/2303.11317},
	abstract = {Grover's algorithm is one of the primary algorithms offered as evidence that quantum computers can provide an advantage over classical computers. It involves an "oracle" (external quantum subroutine) which must be specified for a given application and whose internal structure is not part of the formal scaling of the quantum speedup guaranteed by the algorithm. Grover's algorithm also requires exponentially many steps to succeed, raising the question of its implementation on near-term, non-error-corrected hardware and indeed even on error-corrected quantum computers. In this work, we construct a quantum inspired algorithm, executable on a classical computer, that performs Grover's task in a linear number of call to the oracle - an exponentially smaller number than Grover's algorithm - and demonstrate this algorithm explicitly for boolean satisfiability problems (3-SAT). Our finding implies that there is no a priori theoretical quantum speedup associated with Grover's algorithm. We critically examine the possibility of a practical speedup, a possibility that depends on the nature of the quantum circuit associated with the oracle. We argue that the unfavorable scaling of the success probability of Grover's algorithm, which in the presence of noise decays as the exponential of the exponential of the number of qubits, makes a practical speedup unrealistic even under extremely optimistic assumptions on both hardware quality and availability.},
	language = {en},
	urldate = {2023-08-06},
	publisher = {arXiv},
	author = {Stoudenmire, E. M. and Waintal, Xavier},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11317 [cond-mat, physics:quant-ph]},
	keywords = {Quantum Physics, Quantum computing, Condensed Matter - Strongly Correlated Electrons},
	annote = {Comment: 16 pages, 4 figures},
	file = {Stoudenmire en Waintal - 2023 - Grover's Algorithm Offers No Quantum Advantage.pdf:C\:\\Users\\isido\\Zotero\\storage\\JVZATAKX\\Stoudenmire en Waintal - 2023 - Grover's Algorithm Offers No Quantum Advantage.pdf:application/pdf},
}

@misc{noauthor_sparse_nodate,
	title = {Sparse {Training} of {Neural} {Networks} {Using} {AC}/{DC} - {YouTube}},
	url = {https://www.youtube.com/watch?v=npcT5VO6YOA},
	urldate = {2023-08-08},
	keywords = {machine learning, neural networks, compressing neural networks},
	file = {Sparse Training of Neural Networks Using AC/DC - YouTube:C\:\\Users\\isido\\Zotero\\storage\\PGJHNNXE\\watch.html:text/html},
}

@misc{nagel_white_2021,
	title = {A {White} {Paper} on {Neural} {Network} {Quantization}},
	url = {http://arxiv.org/abs/2106.08295},
	abstract = {While neural networks have advanced the frontiers in many applications, they often come at a high computational cost. Reducing the power and latency of neural network inference is key if we want to integrate modern networks into edge devices with strict power and compute requirements. Neural network quantization is one of the most effective ways of achieving these savings but the additional noise it induces can lead to accuracy degradation.},
	language = {en},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Nagel, Markus and Fournarakis, Marios and Amjad, Rana Ali and Bondarenko, Yelysei and van Baalen, Mart and Blankevoort, Tijmen},
	month = jun,
	year = {2021},
	note = {arXiv:2106.08295 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, machine learning, neural networks, compressing neural networks},
	file = {Nagel e.a. - 2021 - A White Paper on Neural Network Quantization.pdf:C\:\\Users\\isido\\Zotero\\storage\\LUBPHW5D\\Nagel e.a. - 2021 - A White Paper on Neural Network Quantization.pdf:application/pdf},
}

@article{hetzler_continuous_1997,
	title = {A {Continuous} {Version} of {Newton}'s {Method}},
	volume = {28},
	issn = {0746-8342, 1931-1346},
	url = {https://www.tandfonline.com/doi/full/10.1080/07468342.1997.11973888},
	doi = {10.1080/07468342.1997.11973888},
	abstract = {Newton's method for finding a root of f(x) = 0, starting with an initial estimate xo, is equivalent to applying Euler's method with step size ft = 1 to approximate the solution of the initial value problem {\textasciicircum} = ?f(x)/f'(x), with initial condition x(0) = xo. We will examine this connection and prove that, under quite general conditions, the solution of this initial value problem converges to a root of / as t ?{\textgreater}? oo. Then we consider some examples to show how this continuous version of Newton's method [6], [7], which deserves to be more widely known, can be used to find roots in cases where Newton's method fails. The discussion is accessible to calculus students and provides a nice motivation for studying differential equations. It would complement a discussion, as in [4] or [8], of the chaotic behavior of Newton's method.},
	language = {en},
	number = {5},
	urldate = {2023-08-10},
	journal = {The College Mathematics Journal},
	author = {Hetzler, Steven M.},
	month = nov,
	year = {1997},
	keywords = {gradient descent},
	pages = {348--351},
	file = {Hetzler - 1997 - A Continuous Version of Newton's Method.pdf:C\:\\Users\\isido\\Zotero\\storage\\2QQAE4CJ\\Hetzler - 1997 - A Continuous Version of Newton's Method.pdf:application/pdf},
}

@misc{kunsch_high-dimensional_2017,
	title = {High-{Dimensional} {Function} {Approximation}: {Breaking} the {Curse} with {Monte} {Carlo} {Methods}},
	shorttitle = {High-{Dimensional} {Function} {Approximation}},
	url = {http://arxiv.org/abs/1704.08213},
	abstract = {In this dissertation we study the tractability of the information-based complexity \$n({\textbackslash}varepsilon,d)\$ for \$d\$-variate function approximation problems. In the deterministic setting for many unweighted problems the curse of dimensionality holds, that means, for some fixed error tolerance \${\textbackslash}varepsilon{\textgreater}0\$ the complexity \$n({\textbackslash}varepsilon,d)\$ grows exponentially in \$d\$. For integration problems one can usually break the curse with the standard Monte Carlo method. For function approximation problems, however, similar effects of randomization have been unknown so far. The thesis contains results on three more or less stand-alone topics. For an extended five page abstract, see the section "Introduction and Results". Chapter 2 is concerned with lower bounds for the Monte Carlo error for general linear problems via Bernstein numbers. This technique is applied to the \$L\_\{{\textbackslash}infty\}\$-approximation of certain classes of \$C{\textasciicircum}\{{\textbackslash}infty\}\$-functions, where it turns out that randomization does not affect the tractability classification of the problem. Chapter 3 studies the \$L\_\{{\textbackslash}infty\}\$-approximation of functions from Hilbert spaces with methods that may use arbitrary linear functionals as information. For certain classes of periodic functions from unweighted periodic tensor product spaces, in particular Korobov spaces, we observe the curse of dimensionality in the deterministic setting, while with randomized methods we achieve polynomial tractability. Chapter 4 deals with the \$L\_1\$-approximation of monotone functions via function values. It is known that this problem suffers from the curse in the deterministic setting. An improved lower bound shows that the problem is still intractable in the randomized setting. However, Monte Carlo breaks the curse, in detail, for any fixed error tolerance \${\textbackslash}varepsilon{\textgreater}0\$ the complexity \$n({\textbackslash}varepsilon,d)\$ grows exponentially in \${\textbackslash}sqrt\{d\}\$ only.},
	language = {de},
	urldate = {2023-08-11},
	publisher = {arXiv},
	author = {Kunsch, Robert J.},
	month = apr,
	year = {2017},
	note = {arXiv:1704.08213 [math]},
	keywords = {Mathematics - Numerical Analysis, information complexity},
	annote = {Comment: This is the author's submitted PhD thesis, still in the referee process},
	annote = {good intro to information complexity
},
	file = {Kunsch - 2017 - High-Dimensional Function Approximation Breaking .pdf:C\:\\Users\\isido\\Zotero\\storage\\WLUIMUHC\\Kunsch - 2017 - High-Dimensional Function Approximation Breaking .pdf:application/pdf},
}

@misc{gabriel_mongaras_retnet_2023,
	title = {{RetNet}: {A} {Successor} to {Transformer} for {Large} {Language} {Models} {Explained}},
	shorttitle = {{RetNet}},
	url = {https://www.youtube.com/watch?v=B_iGSeG04qo},
	abstract = {Paper found here: https://arxiv.org/abs/2307.08621

Code will be found here soon: https://github.com/microsoft/unilm/tr...},
	urldate = {2023-08-14},
	author = {{Gabriel Mongaras}},
	month = jul,
	year = {2023},
	keywords = {machine learning, neural networks},
}

@misc{sun_retentive_2023,
	title = {Retentive {Network}: {A} {Successor} to {Transformer} for {Large} {Language} {Models}},
	shorttitle = {Retentive {Network}},
	url = {http://arxiv.org/abs/2307.08621},
	abstract = {In this work, we propose Retentive Network (RETNET) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost O(1) inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RETNET achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RETNET a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.},
	language = {en},
	urldate = {2023-08-14},
	publisher = {arXiv},
	author = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
	month = aug,
	year = {2023},
	note = {arXiv:2307.08621 [cs]},
	keywords = {Computer Science - Machine Learning, machine learning, Computer Science - Computation and Language, neural networks},
	file = {Sun e.a. - 2023 - Retentive Network A Successor to Transformer for .pdf:C\:\\Users\\isido\\Zotero\\storage\\VBIIYVIW\\Sun e.a. - 2023 - Retentive Network A Successor to Transformer for .pdf:application/pdf},
}

@misc{noauthor_unilmretnet_nodate,
	title = {unilm/retnet at master · microsoft/unilm},
	url = {https://github.com/microsoft/unilm/tree/master/retnet},
	abstract = {Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities - microsoft/unilm},
	language = {en},
	urldate = {2023-08-14},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\isido\\Zotero\\storage\\857SUU34\\unilm.html:text/html},
}

@misc{ding_longnet_2023,
	title = {{LongNet}: {Scaling} {Transformers} to 1,000,000,000 {Tokens}},
	shorttitle = {{LongNet}},
	url = {http://arxiv.org/abs/2307.02486},
	abstract = {Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LONGNET, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LONGNET has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LONGNET yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence. Code is available at https://aka.ms/LongNet.},
	language = {en},
	urldate = {2023-08-14},
	publisher = {arXiv},
	author = {Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Zheng, Nanning and Wei, Furu},
	month = jul,
	year = {2023},
	note = {arXiv:2307.02486 [cs]},
	keywords = {Computer Science - Machine Learning, machine learning, Computer Science - Computation and Language, neural networks},
	annote = {Comment: Work in progress},
	file = {Ding e.a. - 2023 - LongNet Scaling Transformers to 1,000,000,000 Tok.pdf:C\:\\Users\\isido\\Zotero\\storage\\V4J9KZAD\\Ding e.a. - 2023 - LongNet Scaling Transformers to 1,000,000,000 Tok.pdf:application/pdf},
}

@article{heinrich_randomized_2006,
	title = {The randomized information complexity of elliptic {PDE}},
	volume = {22},
	issn = {0885064X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885064X05001135},
	doi = {10.1016/j.jco.2005.11.003},
	language = {en},
	number = {2},
	urldate = {2023-08-15},
	journal = {Journal of Complexity},
	author = {Heinrich, Stefan},
	month = apr,
	year = {2006},
	keywords = {monte carlo, PDE, integral equations, information complexity},
	pages = {220--249},
	file = {Heinrich - 2006 - The randomized information complexity of elliptic .pdf:C\:\\Users\\isido\\Zotero\\storage\\WJB2Z5H5\\Heinrich - 2006 - The randomized information complexity of elliptic .pdf:application/pdf},
}

@article{heinrich_monte_nodate,
	title = {Monte {Carlo} {Complexity} of {Global} {Solution} of {Integral} {Equations}},
	abstract = {We study the problem of global solution of Fredholm integral equations. This means that we seek to approximate the full solution function (as opposed to the local problem, where only the value of the solution in a single point or a functional of the solution is sought). We analyze the Monte Carlo complexity, i. e. the complexity of stochastic solution of this problem. The framework for this analysis is provided by informationbased complexity theory. Our investigations complement previous ones on stochastic complexity of local solution and on deterministic complexity of both local and global solution . The results show that even in the global case Monte Carlo algorithms can perform better than deterministic ones, although the difference is not as !arge as in the local case.},
	language = {en},
	author = {Heinrich, Stefan},
	keywords = {monte carlo, integral equations, information complexity},
	file = {Heinrich - l!Ionte Carlo Complexity of Global Solution of In.pdf:C\:\\Users\\isido\\Zotero\\storage\\RG5LPU73\\Heinrich - l!Ionte Carlo Complexity of Global Solution of In.pdf:application/pdf},
}

@misc{becker_learning_2023,
	title = {Learning the random variables in {Monte} {Carlo} simulations with stochastic gradient descent: {Machine} learning for parametric {PDEs} and financial derivative pricing},
	shorttitle = {Learning the random variables in {Monte} {Carlo} simulations with stochastic gradient descent},
	url = {http://arxiv.org/abs/2202.02717},
	abstract = {In financial engineering, prices of financial products are computed approximately many times each trading day with (slightly) different parameters in each calculation. In many financial models such prices can be approximated by means of Monte Carlo (MC) simulations. To obtain a good approximation the MC sample size usually needs to be considerably large resulting in a long computing time to obtain a single approximation. In this paper we introduce a new approximation strategy for parametric approximation problems including the parametric financial pricing problems described above. A central aspect of the approximation strategy proposed in this article is to combine MC algorithms with machine learning techniques to, roughly speaking, learn the random variables (LRV) in MC simulations. In other words, we employ stochastic gradient descent (SGD) optimization methods not to train parameters of standard artificial neural networks (ANNs) but to learn random variables appearing in MC approximations. We numerically test the LRV strategy on various parametric problems with convincing results when compared with standard MC simulations, Quasi-Monte Carlo simulations, SGD-trained shallow ANNs, and SGD-trained deep ANNs. Our numerical simulations strongly indicate that the LRV strategy might be capable to overcome the curse of dimensionality in the \$L{\textasciicircum}{\textbackslash}infty\$-norm in several cases where the standard deep learning approach has been proven not to be able to do so. This is not a contradiction to lower bounds established in the scientific literature because this new LRV strategy is outside of the class of algorithms for which lower bounds have been established in the scientific literature. The proposed LRV strategy is of general nature and not only restricted to the parametric financial pricing problems described above, but applicable to a large class of approximation problems.},
	language = {en},
	urldate = {2023-08-15},
	publisher = {arXiv},
	author = {Becker, Sebastian and Jentzen, Arnulf and Müller, Marvin S. and von Wurstemberger, Philippe},
	month = jun,
	year = {2023},
	note = {arXiv:2202.02717 [cs, math]},
	keywords = {monte carlo, Mathematics - Probability, PDE, Mathematics - Numerical Analysis, Mathematics - Analysis of PDEs, neural networks, information complexity, 35K15, 65C05, 65M75, 68T99, 91G20, parametric},
	annote = {Comment: 71 pages, 4 Figures, 14 Tables; to appear in Math. Finance},
	file = {Becker e.a. - 2023 - Learning the random variables in Monte Carlo simul.pdf:C\:\\Users\\isido\\Zotero\\storage\\RJSJUJED\\Becker e.a. - 2023 - Learning the random variables in Monte Carlo simul.pdf:application/pdf},
}

@incollection{dick_complexity_2018,
	address = {Cham},
	title = {On the {Complexity} of {Parametric} {ODEs} and {Related} {Problems}},
	isbn = {978-3-319-72455-3 978-3-319-72456-0},
	url = {http://link.springer.com/10.1007/978-3-319-72456-0_26},
	abstract = {We present an iterative Monte Carlo procedure to solve initial value problems for systems of ordinary diﬀerential equations depending on a parameter. It is based on a multilevel Monte Carlo algorithm for parametric indeﬁnite integration. As an application, we also obtain a respective method for solving almost linear ﬁrst order partial diﬀerential equations. We also consider deterministic algorithms.},
	language = {en},
	urldate = {2023-08-15},
	booktitle = {Contemporary {Computational} {Mathematics} - {A} {Celebration} of the 80th {Birthday} of {Ian} {Sloan}},
	publisher = {Springer International Publishing},
	author = {Heinrich, Stefan},
	editor = {Dick, Josef and Kuo, Frances Y. and Woźniakowski, Henryk},
	year = {2018},
	doi = {10.1007/978-3-319-72456-0_26},
	keywords = {monte carlo, ODE, information complexity, parametric},
	pages = {567--595},
	file = {Heinrich - 2018 - On the Complexity of Parametric ODEs and Related P.pdf:C\:\\Users\\isido\\Zotero\\storage\\UXR58HGT\\Heinrich - 2018 - On the Complexity of Parametric ODEs and Related P.pdf:application/pdf},
}

@article{vavalis_implementing_2014,
	title = {{IMPLEMENTING} {HYBRID} {PDE} {SOLVERS}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://figshare.com/articles/journal_contribution/IMPLEMENTING_HYBRID_PDE_SOLVERS/1134520},
	doi = {10.6084/M9.FIGSHARE.1134520},
	abstract = {We investigate the possibility that we effectively combine both conventional deterministic PDE solving methods and traditional probabilistic Monte Carlo approaches for solving linear Elliptic Partial Differential Equations. Our objective is to provide a robust and easy to use implementation that allows further experimentation on this new type of PDE solvers in order to elucidate their capabilities and computational characteristics. We ﬁrst present the general formulation of the algorithm, then describe its implementation in C++ for a class of model problems in two and three space dimensions, we analyze its performance and we ﬁnally discuss possible extensions.},
	language = {en},
	urldate = {2023-08-15},
	author = {Vavalis, Manolis},
	year = {2014},
	note = {Artwork Size: 204576 Bytes
Publisher: figshare},
	keywords = {PDE, walk on spheres, boundary value problems, 10301 Numerical Analysis, FOS: Mathematics},
	pages = {204576 Bytes},
	file = {Vavalis - 2014 - IMPLEMENTING HYBRID PDE SOLVERS.pdf:C\:\\Users\\isido\\Zotero\\storage\\2ISLXPWQ\\Vavalis - 2014 - IMPLEMENTING HYBRID PDE SOLVERS.pdf:application/pdf},
}

@article{heinrich_monte_2006,
	title = {Monte {Carlo} approximation of weakly singular integral operators},
	volume = {22},
	issn = {0885064X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885064X05001123},
	doi = {10.1016/j.jco.2005.11.002},
	abstract = {We study the randomized approximation of weakly singular integral operators. For a suitable class of kernels having a standard type of singularity and being otherwise of ﬁnite smoothness, we develop a Monte Carlo multilevel method, give convergence estimates and prove lower bounds which show the optimality of this method and establish the complexity. As an application we obtain optimal methods for and the complexity of randomized solution of the Poisson equation in simple domains, when the solution is sought on subdomains of arbitrary dimension.},
	language = {en},
	number = {2},
	urldate = {2023-08-15},
	journal = {Journal of Complexity},
	author = {Heinrich, Stefan},
	month = apr,
	year = {2006},
	keywords = {monte carlo, integral equations, information complexity},
	pages = {192--219},
	file = {Heinrich - 2006 - Monte Carlo approximation of weakly singular integ.pdf:C\:\\Users\\isido\\Zotero\\storage\\W79W9FBH\\Heinrich - 2006 - Monte Carlo approximation of weakly singular integ.pdf:application/pdf},
}

@article{kim_bk-sdm_nodate,
	title = {{BK}-{SDM}: {Architecturally} {Compressed} {Stable} {Diffusionfor} {Efficient} {Text}-to-{Image} {Generation}},
	abstract = {Exceptional text-to-image (T2I) generation results of Stable Diffusion models (SDMs) come with substantial computational demands. To resolve this issue, recent research on efficient SDMs has prioritized enabling fewer sampling steps and utilizing network quantization. Orthogonal to these directions, this study highlights the power of classical architectural compression for general-purpose T2I synthesis by introducing block-removed knowledge-distilled SDMs (BKSDMs). We eliminate several residual and attention blocks from the U-Net of SDMs, obtaining over a 30\% reduction in the number of parameters, MACs per sampling step, and latency. We conduct distillation-based pretraining with only 0.22M LAION pairs (fewer than 0.1\% of the full training pairs) on a single A100 GPU. Despite being trained with limited resources, our compact models can imitate the original SDM by benefiting from transferred knowledge and achieve competitive results against larger multi-billion parameter models on the zero-shot MS-COCO benchmark. Moreover, we show the applicability of our lightweight pretrained models in personalized generation with DreamBooth finetuning.},
	language = {en},
	author = {Kim, Bo-Kyeong and Song, Hyoung-Kyu and Castells, Thibault and Choi, Shinkook},
	keywords = {machine learning, neural networks, diffusion model},
	file = {Kim e.a. - BK-SDM Architecturally Compressed Stable Diffusio.pdf:C\:\\Users\\isido\\Zotero\\storage\\WSZLKADA\\Kim e.a. - BK-SDM Architecturally Compressed Stable Diffusio.pdf:application/pdf},
}

@misc{tang_any--any_2023,
	title = {Any-to-{Any} {Generation} via {Composable} {Diffusion}},
	url = {http://arxiv.org/abs/2305.11846},
	abstract = {We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io},
	language = {en},
	urldate = {2023-08-17},
	publisher = {arXiv},
	author = {Tang, Zineng and Yang, Ziyi and Zhu, Chenguang and Zeng, Michael and Bansal, Mohit},
	month = may,
	year = {2023},
	note = {arXiv:2305.11846 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, machine learning, Computer Science - Computation and Language, neural networks, diffusion model, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Project Page: https://codi-gen.github.io},
	file = {2305.11846.pdf:C\:\\Users\\isido\\Zotero\\storage\\HSBGQDVT\\2305.11846.pdf:application/pdf},
}

@misc{simons_institute_what_2023,
	title = {What {Can} {I} {Do} {With} a {Noisy} {Quantum} {Computer}?},
	url = {https://www.youtube.com/watch?v=OrSuP1oaMhE},
	abstract = {Abhinav Kandala (IBM)
https://simons.berkeley.edu/talks/abh...
Provable NISQ Quantum Advantage Workshop},
	urldate = {2023-08-18},
	author = {{Simons Institute}},
	month = aug,
	year = {2023},
	keywords = {Quantum computing},
}

@article{mathe_monte_nodate,
	title = {{THE} {MONTE} {CARLO} {COMPLEXITY} {OF} {FREDHOLM} {INTEGRAL} {EQUATIONS}},
	abstract = {A complexity study of Monte Carlo methods for Fredholm integral equations is carried out. We analyze the problem of computing a functional p.(y), where y is the solution of a Fredholm integral equation on the w-dimensional unit cube Im , where the kernel k and right-hand side / are given r times differentiable functions. We permit stochastic numerical methods which can make use of function evaluations of k and / only. All Monte Carlo methods known to the authors for solving the above problem are of the order n{\textasciitilde} '/2 , while the optimal deterministic methods yield rate n-r/(2m) ( tmls taking into account the given smoothness of the data. Here, n denotes the (average) number of function evaluations performed. The optimal algorithm we present combines deterministic and stochastic methods in an optimal way. It can be seen that both rates—the standard Monte Carlo rate for general continuous data and the deterministic rate for r-smooth data—multiply. This provides the smallest error that stochastic methods of given computational cost can achieve.},
	language = {en},
	author = {Mathé, Peter and Stefan  Heinrich},
	keywords = {monte carlo, integral equations, information complexity},
	file = {Mathé - THE MONTE CARLO COMPLEXITY OF FREDHOLM INTEGRAL EQ.pdf:C\:\\Users\\isido\\Zotero\\storage\\6RNCHEZU\\Mathé - THE MONTE CARLO COMPLEXITY OF FREDHOLM INTEGRAL EQ.pdf:application/pdf},
}

@incollection{niederreiter_wavelet_2000,
	address = {Berlin, Heidelberg},
	title = {Wavelet {Monte} {Carlo} {Methods} for the {Global} {Solution} of {Integral} {Equations}},
	isbn = {978-3-540-66176-4 978-3-642-59657-5},
	url = {http://link.springer.com/10.1007/978-3-642-59657-5_15},
	abstract = {We study the global solution of Fredholm integral equations of the second kind by the help of Monte Carlo methods. Global solution means that we seek to approximate the full solution function. This is opposed to the usual applications of Monte Carlo, were one only wants to approximate a functional of the solution. In recent years several researchers developed Monte Carlo methods also for the global problem.},
	language = {en},
	urldate = {2023-08-18},
	booktitle = {Monte-{Carlo} and {Quasi}-{Monte} {Carlo} {Methods} 1998},
	publisher = {Springer Berlin Heidelberg},
	author = {Heinrich, Stefan},
	editor = {Niederreiter, Harald and Spanier, Jerome},
	year = {2000},
	doi = {10.1007/978-3-642-59657-5_15},
	keywords = {monte carlo, integral equations, information complexity},
	pages = {227--237},
	file = {Heinrich - 2000 - Wavelet Monte Carlo Methods for the Global Solutio.pdf:C\:\\Users\\isido\\Zotero\\storage\\MR89NC5G\\Heinrich - 2000 - Wavelet Monte Carlo Methods for the Global Solutio.pdf:application/pdf},
}

@article{heinrich_monte_1999,
	title = {Monte {Carlo} {Complexity} of {Parametric} {Integration}},
	volume = {15},
	issn = {0885064X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885064X99905083},
	doi = {10.1006/jcom.1999.0508},
	language = {en},
	number = {3},
	urldate = {2023-08-18},
	journal = {Journal of Complexity},
	author = {Heinrich, Stefan and Sindambiwe, Eugène},
	month = sep,
	year = {1999},
	keywords = {monte carlo, integration, information complexity, parametric},
	pages = {317--341},
	file = {Heinrich en Sindambiwe - 1999 - Monte Carlo Complexity of Parametric Integration.pdf:C\:\\Users\\isido\\Zotero\\storage\\IU5ZFZ6K\\Heinrich en Sindambiwe - 1999 - Monte Carlo Complexity of Parametric Integration.pdf:application/pdf},
}
